--- 
title: "CalcZ Student Notes"
author: "Daniel Kaplan"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
description: "Textbook for Math 141Z/142Z for 2021-2022"
---
```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```

# Welcome to calculus {.unnumbered}

Calculus is the set of concepts and techniques that provide the main mathematical basis for dealing with motion, growth, decay, and oscillation.  The phenomena can be as simple as a ball arcing through the air to as complex as the airflow over a wing that generates lift. Calculus is used in biology and business, chemistry, physics and engineering. It is the basis for weather prediction, understanding climate change, the algorithms for heart rate and blood oxygen measurement by wristwatches. It is a major part of the language of science, of logistics. The electron orbitals of chemistry, the stresses of bones and beams, and the business cycle of recession and rebound are all understood primarily through calculus. 

Calculus has been central to science from the very beginnings. It is no coincidence that the scientific method was introduced and the language of calculus was invented by the same small group of people during the historical period known as the enlightenment. Learning calculus has always been a badge of honor and an entry ticket to professions. Millions of students career ambitions have been enhanced by passing a calculus course or thwarted by lack of access to one.

In the 1880s, a hit musical featured a man who was "the very model of a modern major general."  One of his claims for modernity was that "I'm very good at integral and differential calculus." [Watch here](https://www.youtube.com/watch?v=Rs3dPaz9nAo)

Of course, what was modern in 1880 is not modern anymore. Yet, amazingly, calculus today is every bit as central to science and technology as it ever was, and is much more important to logistics, economics and myriad other fields than every before. 
In the last 20 years, calculus has become even more important. There's a very concrete reason for this. Science, engineering, and society have now fully adopted the computer for almost all aspects of work, study, and life. The amount of data collected and used has exploded. Machine learning has become the way human decision makers interact with such data.

Think about what it means to become "computerized." To take an everyday example, consider video. Over the span of a human life we moved from a system which involved people going to theaters to watch the shadows recorded on cellulose film to the distribution over the airwaves by low-resolution television, to the introduction of high-def broadcast video, to on demand streaming from huge libraries of movies. Just about anyone can record, edit, and distribute their own video. The range of topics (including calculus) on which you can access a video tutorial or demonstration is incredibly vast. All of this recent progress is owed to computers.

The "stuff" that computers operate on, transform and transmit is always mathematical representations stored as bits. The creation of mathematical representations of objects and events in the real world is essential to every task of any sort that any computer performs. Calculus is a key component of inventing and using such representations.

You may be scratching your head. If calculus is so important, why is it that many of your friends who took calculus came away wondering what it is for? What's so important about "slopes" and "areas" and how come your high-school teacher couldn't tell you what calculus is for?

The disconnect between the enthusiasm expressed in the preceding paragraphs and the lived experience of students is very real. There are two major reasons for that disconnect, both of which we tackle head-on in this book.

First, teachers of mathematics have a deep respect for tradition. Such respect has its merits but the result is that almost all calculus is taught using methods that were appropriate for the era of paper and pencil, but not for the computer era. As you will see, in this book we express the concepts of calculus in a way that carries directly over to the uses of calculus on computers and in genuine work.

Second, the uses of calculus are enabled not by the topics of Calc I and Calc II, but the courses for which I/II are a preliminary: linear algebra and dynamics. Only a small fraction of students who start in Calc I ever reach the parts of calculus that are the most useful.

Fortunately, there is a large amount of bloat in the standard textbook topics of Calc I/II which can be removed to make room for the genuinely important topics. 

## Computing and apps {.unnumbered}

The text provides two complementary ways to access computing. The most intuitive is designed purely to exercise and visualize mathematical concepts through mouse-driven, graphical ***apps***. To illustrate, here is an app that we'll use in Block 6. You can click on the snapshot to open the app in your browser.

<a href="https://maa-statprep.shinyapps.io/142Z-Matrix-iteration/" target="_blank"><img src="www/app-matrix-iteration.png" width="70%"></a>

More fundamentally, you will be carrying out computing by composing computer commands and text and having a computer carry out the commands. One good way to do this is in a ***sandbox***, a kind of app which provides a safe place to enter the commands. You'll open the sandbox (click on the image below) in your browser. 

<a href="https://maa-statprep.shinyapps.io/141Z-Student-sandbox" target="_blank"><img src="www/app-sandbox-draft.png" width="50%"></a>

::: {.todo}
Update the image and link to the new version of the sandbox app.
:::


Once you've entered the computer commands, you press a button to have the commands carried out.

::: {.tip}
You may prefer to install the R and RStudio software on your own laptop. This usually provides a faster response to you and lowers the load on the sandbox cloud servers being used by other students. 

Experienced R users may even prefer to skip the sandbox entirely and use the standard resources of RStudio to edit and evaluate their computer commands. You'd use exactly the same R commands regardless of whether you use a cloud server or your own laptop.
:::

An important technique for teaching and learning computing is to present ***scaffolding*** for computer commands. At first, the scaffolding may be complete, correct commands that can be cut-and-paste into a ***sandbox*** where the calculation will be carried out. Other times it will be left to the student to fill in some part that's been left out of the scaffolding. For example, when we introduce drawing graphs of functions and the choice of a domain, you might see a scaffold that has blanks to be filled in:

::: {.scaffold}
```{r eval=FALSE}
slice_plot( exp(-3*t) ~ t, domain( --fill in domain-- ))
```
:::

You can hardly be expected at this point to make sense of any part of the above command, but soon you will. 

## Exercises and feedback {.unnumbered}


Learning is facilitated by rapid, formative feedback. Many of the exercises in this book are arranged to give this. 

```{r child=exercise_file("01", "exercise-intro.Rmd")}
```

## [[Taken out. Is there any use for it?]]

There's mathematics ... and then there's the real world. The term ***mathematical modeling*** is about constructing representations of situations or phenomena in the real world in terms of mathematical objects. The advantage of being able to construct models is that models are easy to study, explore, take apart, do experiments on, and deduce the implications of.

We use computers to interact with the world, whether that be record-keeping, communications, media displays, and so on. The "things" that computers work with are always mathematical representations of the real world: models. Computers carry out operations on models. For instance, sound is a real-world phenomenon that resides somewhere between the vibrations of air and the psycho-acoustics of the brain. A simple computer representation of sound is constructed using mathematical stuff: a long series of numbers. Suppose we want to *slow down* sound (say, for someone learning a foreign language) or to speed it up (to get through a slow-paced video) or to translate it into text. The technologies for doing these things are almost always implemented by doing arithmetic or more complicated mathematical procedures on the mathematical stuff.

A major theme of this course is how to make such mathematical representations of real-world phenomena and how to work with the representations to extract the results we want. In particular, we will focus on representations in the form of mathematical functions. (There are other mathematical representations, but functions have a special importance and are ubiquitous in scientific work and computing.) 


<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# CalcZ Project Development Plan {.unnumbered}

```{r include=FALSE}
dd1 <- function(days, course=c("141Z", "142Z")) {
  course <- match.arg(course)
  raw_days <- days <- as.character(days)
  days <- ifelse(nchar(days) < 2, 
                 paste0("0", days),
                 days)
  path <- if (course == "142Z") {
    "https://maa-statprep.shinyapps.io/142Z-DD-"
  } else {
     "https://maa-statprep.shinyapps.io/141Z-daily-digital-"
  }
    
  path <- paste0(path, days)
  
  links <- glue::glue("[DD-{days}]({path})")
  paste0(links, collapse=", ")
}
dd2 <- function(days) {
  dd1(days, course="142Z")
}
```


This document reflects the developing text for Math141Z/142Z in AY 2021-2022 at the US Air Force Academy. This section describes to prospective instructors and project collaborators the planned schedule for the book and links to the materials developed in AY 2020-2021.

Note that nothing in this book necessarily reflects any official policy of the US Air Force Academy or the US government more generally.

## Development schedule {.unnumbered}

- 1 June 2021: Complete usable draft of Block 1 (Functions)
- 7 June 2021: Deployment of CalcZ R sandbox
- 15 June 2021: Complete usable draft of Block 2 (Differencing)
- 30 June 2021: Complete usable draft of Block 3 (Accumulation)
- 15 July 2021: Re-organization of relevant `{mosaicCalc}` and `{CalcZ}` R packages.
- 1 August 2021: Deployment of Shiny apps for Blocks 1-3.
- 1 August 2021: Revisions to Blocks 1, 2, and 3 and deployment in a student-facing form
- 31 September 2021: Complete usable draft of Block 4 (Applications of calculus)
- 30 October 2021: Complete usable draft of Block 5 (Linear combinations)
- 31 November 2021: Complete usable draft of Block 6 (Dynamics)
- 1 December 2021: Student-facing form of entire book deployed.

## Resources from AY 2020-2021 {.unnumbered}

### Software {.unnumbered}

Note that the book itself and the multiple-choice questions with feedback included in the book, require only a browser. No software needs to be installed.

Computational exercises can be handled with a "sandbox" app that, similarly, requires only a browser. 

The [R Command Guide](https://maa-statprep.shinyapps.io/141Z-R_command_guide/) from AY 2020-2021 will be integrated into the book.

The Daily Digitals from AY 2020-2021 have integrated R consoles that are sufficient for all computations in the course. 

You can also use RStudio directly. You will need to install the following packages:
```r
install.packages(c("mosaic", "ggplot2", "ggformula", "MMAC"))
remotes::install.github("ProjectMosaic/mosaicCalc", ref="beta")
remotes::install.github("dtkaplan/math141Z", ref="master")
```

Collaborators who wish to compile the book will need to use the RStudio interface and install several additional packages. Be a volunteer to identify these packages! Start with a fresh installation of R and RStudio and work with Danny to get a comprehensive list. (It's hard for me to do this since I have the packages installed on the computers I use and don't want to start over with a brand-new installation to make sure I've got them all.)

### Exercises {.unnumbered}

**Daily digitals** containing student exercises. Some contain narrative that will be folded in to this book.

[NOTE: This is a quick pass through the links. Some will be broken or lead to a different topic]

1. Functions `r dd1(1:12)`
2. Linear Combinations `r dd1(13:25)`
3. Differencing `r dd1(26:38)`
4. Accumulation `r dd2(1:12)`
5. Dynamics `r dd2(13:25)`
6. Decision-making `r dd2(26:38)`

### Projects from AY 2020-2021 {.unnumbered}

1. [Spread of ebola](https://dtkaplan.github.io/CalcZ/project-lessons-from-ebola.html) (sigmoidal functions)
#. [Engines big and small](https://dtkaplan.github.io/CalcZ/project-engines-big-and-small.html) (dimensions, linear combinations)
#. [Walking the gradient](https://dtkaplan.github.io/CalcZ/project-gradient-ascent.html) (derivatives, optimization, modeling)
#. [Gears and grades](https://dtkaplan.github.io/CalcZ/project-gears-and-grades.html)
#. [Driving with integrals](https://maa-statprep.shinyapps.io/142Z-driving-with-integrals/) (accumulation, function composition)
#. Intervening in an epidemic (dynamics)
#. [Security and life](https://maa-statprep.shinyapps.io/142Z-Project3/) (decision making)

### Apps {.unnumbered}

Some of these were used as part of Daily Digital assignments, some were not used. There are also some apps integrated with the Daily Digitals. These will be provided as stand-alone apps.


#. [Numerics of search](https://maa-statprep.shinyapps.io/141Z-numerics-of-search/) optimization, zero finding not yet included.
#. [Average vs marginal](https://maa-statprep.shinyapps.io/141Z-average_marginal/)
#. [Interpolation explorer](https://maa-statprep.shinyapps.io/142Z-Interpolation/)
#. [Euler for DE](https://maa-statprep.shinyapps.io/142Z-DE-Euler/)
#. [Balancing functions to solve a DE](https://maa-statprep.shinyapps.io/142Z-balance-logistic/)
#. [Eigenflows](https://maa-statprep.shinyapps.io/142Z-Eigenflows/)
#. [Matrix iteration](https://maa-statprep.shinyapps.io/142Z-Matrix-iteration/)
#. [Using the SIR model](https://maa-statprep.shinyapps.io/using-the-SIR-model)

Some apps that aren't graphic:

#. [Instructor sandbox](https://maa-statprep.shinyapps.io/141Z-Instructor-sandbox/) containing blank sandboxes for use outside of the Daily Digitals. (This will be revised for AY 2021-2022.)
#. [Student sandbox]() is like the instructor sandbox. It will also be revised for AY 2021. The revised version, as it will be deployed is the main computational resource for students not using RStudio. It will be beefed up to handle hundreds of students simultaneously. In AY 2021-2022 we will be using an MAA-StatPREP server, which is open to everyone. Thereafter, we will switch servers. We're not sure how this will be set up, but there are several arrangement which can already be used by schools or students.

    a. Schools can deploy their own Shiny/RStudio servers or use a service like `shinyapps.io`.
    b. Small groups of students can be handled with the "free tier" account on `shinyapps.io`.
    c. Students can install RStudio on their laptops and run the sandbox there. Note: There is no tablet version of RStudio.
    d. Students can set up an RStudio.cloud account (which is free) and run the sandbox from there. 
    e. Students or instructors can use the RStudio console or (better) an Rmd document to run the commands needed for the exercises and projects. This will work either on RStudio desktop, an institution's RStudio server, or `rstudio.cloud`
    
The same applies for the apps, but the apps consume relatively little in the way of computer resources, so we will try to have them available on the web with no setup. *Performance* won't be optimal, but at least *access* will be easy.
#. [Basic R syntax](https://maa-statprep.shinyapps.io/141Z-Syntax/). This is just a homework assignment that fell outside the Daily Digital organization.
#. [Swimming records](https://maa-statprep.shinyapps.io/144Z-Swimming-Day-04). This is just a single exercise that wasn't integrated with the Daily Digitals.

<!--chapter:end:Development-outline.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# (PART) Block 1: Functions and quantity {.unnumbered}


<!--chapter:end:Fun-part-marker.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Outline of Block 1 {.unnumbered}

::: {.todo}
This section is for development purposes only. It is not to be included in the released text.
:::

This outline was that established during the May 17-19, 2021 working sessions at USAFA. It's copied directly from the Teams document. I've made some modifications which are noted in [[square brackets]] for deletions and **bold face** for additions..

Block 1 Functions and Mathematical Modeling

1. Intro to the course
    a. Lecture Topics
        i. Calculus Z overview and trajectory
        i. Course resources and systems
        i. Calculus is the study of change
        i. Quantities vs numbers
        i. Functions
        i. Linear Function is the model for “change”
    a. Outcomes
        i. Understand than an expression relates quantities, but a function maps inputs to outputs
        i. Understand the difference between $\equiv$ and $=$
        i. Be able to identify a function's input
    a. Reading Chapter \@ref(change)
    a. Assignments
        i. HW01
        i. Feedback01
        i. DD01

2. Basics of modeling functions
    a. Lecture Topics
        i. Basic Modeling functions
        i. 
    a. Outcomes
        i. Memorize the names and mathematical representations of the six basic modeling functions
        i. Be able to translate between traditional math notation and R expressions
        i. Be able to identify the name of a function given in either math notation or R notation
    a. Readings: Section \@ref(naked-intro) and Chapter \@ref(fun-describing)
    a. Assignments
        i. 

3. Functions as I/O and Notational Structure of functions (R<>Math)
    a. Topics
        i. **Graphics and graphs** 
        i. [[Polynomials]] moved to linear combinations
        #. Introduce Multi-input functions notation
        #. Distinguish between mathematical symbols: $=$, $\equiv$, \rightarrow$, `<-` See Secton \@ref(foursigns)
    b. Reading: Chapter \@ref(fun-notation) & \@ref(graphs-and-graphics)
    c. To do:
        - Provide glossary of notation across 6-blocks (+alternate forms)

4. Parameterized functions as models (eqn, params, data, applications)
    a. NTI
        i. Introduce idea that input has meaning, and output has meaning relative to the input
        ii. Give students a function. Then change the units of input to a different unit (e.g. feet go to Meters)
    b. Objectives:
        i. Linear
        i. Sine
        i. Power
        i. Exponential
        i. Logarithmic
    c. Reading: Chapter \@ref(params-intro)

5. [[Combining 2+ functions]] I've integrated this with the "Parameterized function" chapter.
    a. Examples:
    b. NTIs:
    c. Topics:
        i. Linear Combination (ex: Polynomial)
        ii. Composition with scaling function ($e^{kt})
            1. Gallons/$ to Liters/Euros
        iii. Composition (affine shift)
            1. Fahrenheit <> Celsius

6. **The modeling process**

6. Slope function
    a. NTI:
        i. How do broaden the idea of describing the slope of a line to a general function?
        ii. No $h$ at this point
        iii. The slope function is a function $s()$  which is built from a function $f()$
    b. Topics
        i. Average rate of change.  (Example: Quarterly return of stocks. HW of tree harvesting.) (Example 2: Sine wave with difference around 1 period. Or maybe this becomes low-pass filter example in accumulation.)
        i. $s(x) \equiv \frac{f(x + 0.1) - f(x)}{0.1}$
        ii. Except for linear function, slope **function** depends on how big $h$ is. So let's fix $h$ while we figure things out.
        ii. **Every function has a slope function.**
        iii. **It's helpful to name functions so we know where they come from. We're going to use $f'()$ to stand for the slope function of $f()$**
    c. Reading: Chapter \@ref(fun-slopes)

7. Composition Generally
    a. NTI:
        i. What temperature is it while driving up a mountain?
        ii. Problem where they are calculating the slope of a sigmoid
    b. Topics:
        i. Formula for Hump (exponential with a quadratic)
        ii. **Driving up a mountain** temperature vs altitude, altitude vs road position, road position vs time
        iii. Hump is the slope function of Sigmoid 
    c. Reading: Chapter \@ref(fun-assembling)

8. Multiplication of functions
    a. NTIs:
        i. Multiplication about Sines, exponential, hump, sigmoid
    b. Topics:
        i. Multiplying functions **of the same inputs** gives a function of those inputs.
        ii. **Multiplying functions of different inputs** gives a function whose inputs are the union of the inputs to the multiplicands.
    c. Be able to reverse engineer these products: sin*exp, sin*hump*, sin*sigmoid
    d. Reading: Chapter \@ref(fun-assembling)

9. Multivariate functions
    a. NTIs:
        i. Ask students how they compute the slope of a multi-input function as best they can imagine how (homework?) Build their imagined function (for the slope) and plot the function
    b. Outcomes:
        i. Be able to read & construct (in R) Contour Plots
        ii. Be able to read & construct (in R) Surface Plots
        iii. Be able to read a table w/ 2 inputs
    c. Reading: Chapter \@ref(fun-multiple-inputs)
    
10. Piecewise functions [jettison if needed in favor of doing this as homework]
    a. Topics
        i. Construct and evaluate piecewise functions
        ii. Demonstrate how to implement in R
    b. Outcomes:
        i. Be able to identify break(s) of a piecewise function
        i. Be able to evaluate a piecewise function at a given input
        i. Be able to implement a piecewise function in R
    c. Reading: Chapter \@ref(fun-piecewise)

11. Iterative Modeling
    a. NTI:
    b. Examples:
        i. Tides
        ii. Cooling Water
    c. Objectives:
        i. Describe differences between two models and between a model and data
        ii. Identify disagreements between the model and data
    d: Reading: NEED LINK TO CHAPTER
::: {.workedexample}
**Review, GR, Project Days**
:::

12. [[Semi-log Log-log plots]] Magnitude and log
    a. Topics:
        i. Introduce the idea that using logarithms converts a number to its order of magnitude
    b. Outcomes:
    c. Reading: Chapter \@ref(magnitudes)
::: {.workedexample}
**Flexible catchup day**
:::

13. Dimensional Analysis 
    a. Reading: Chapter \@ref(dimensions)
    
    1.5 class days

::: {.takenote}
One of our highlighting formats, called `::: {.takenote}` is being used as a clear statement of basic skills that a students should master. It might be good to tie quizzes to these and *vice versa*.
:::

<!--chapter:end:Fun-outline.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Change

Calculus is about change, and change is about relationships. A changing climate is about the relationship between, say, global average temperature and time. It's also about changing levels of CO~2~ and methane, both their production and elimination by atmospheric and geological processes. It's about how burning oil (a change in configuration of the atoms in hydrocarbons) contributes to ocean acidification (the process of change in ocean pH). It's about a whole complex and intricate network of relationships and how change in one component provokes change in others.

This book presents calculus in terms of two simple but central concepts: ***functions*** and ***quantities***. Those words have everyday meanings which are, happily, close to the specific mathematical concepts that we will be using over and over again. Close ... but not identical. So pay careful attention to the brief descriptions that follow.

## Quantity vs number

A mathematical ***quantity*** is an amount of "stuff." The real-world stuff might be mass or time or length. It equally well can be velocity or volume or momentum or corn yield per acre. We live in a world of stuff, some of which is tangible (e.g., corn, mass, force) and some of which is harder to get your hands on and your minds around (acceleration, crop yield, fuel economy). An important use of calculus is helping us conceptualize the abstract kinds of stuff as mathematical compositions of simpler stuff. For example, crop yield incorporates mass with length and time. Later, you'll see us using the more scientific-sounding term ***dimension*** instead of "stuff." In fact, Chapter \@ref{dimensions} is entirely dedicated to the topic of dimensions, but for now it's sufficient for you to understand that numbers alone are not quantities.

Most people are inclined to think "quantity" is the same as "number"; they conflate the two. This is understandable but misguided. By itself a number is meaningless. What meaning does the number 5 have without more context? It is simply an arithmetic value. Quantity, on the other hand, combines a number with the appropriate context to describe some amount stuff. So, the first thing you need to know about any quantity is the kind of stuff it describes. A "mile" is a kind of stuff: length. A meter is the same kind of stuff: length. A liter is a different kind of stuff: volume. A gallon and an acre-foot is the same kind of stuff: volume.

"Stuff," as we mean it here, is what we measure. As you know, we measure with ***units***. Which units are appropriate depends on the kind of stuff. Meters, miles, microns are all appropriate units of length, even though the actual lengths of these units differ markedly. (A mile is roughly a million microns.) 

Only after you know the dimension and units does the number have meaning. Thus, a *number* is only part of specifying a *quantity*.

Here's the salient difference between number and quantity when it comes to calculus: All sorts of arithmetic and other mathematical operations can be performed to combine numbers: addition, multiplication, square roots, etc. When performing mathematics on quantities, only multiplication and division are universally allowed. For addition, square roots, and such, the operation makes sense only if the dimensions are suitable.

The mathematics of units and dimension are the equivalent in the technical world of common sense in our everyday world. For instance (and this may not make sense at this point), if someone tells me they are taking the square root of 10 liters, I know immediately that either they are just mistaken or that they haven't told me essential elements of the situation. It's just as if someone said, "I swam across the tennis court." You know that they either used the wrong verb---walk and run would work---or that it wasn't a tennis court, or that something important was unstated, perhaps, "During the flood, I swam across the tennis court."

## Functions

The other central concept in the book is ***functions*** in their mathematical and computing sense; this is the primary topic for Block 1. A function is something that takes one or more inputs and returns an output. In calculus, we'll deal mainly with functions that take one or more quantities as inputs and returns another quantity as output. But sometimes we'll work with functions that take functions as input and return a quantity as output. And there will even be functions that take a function as inputs and return a function as an output.

You've almost certainly seen functions expressed in the general form $f(x)$. The function is $f()$, the input is $x$. Perhaps it's obvious at this point that $x$ is a quantity. $f(x)$ is the operation performed on that quantity. In computing, there is a definite, widely used, notation^[Really, a set of notations.] to identify the output of a function. Strangely, in mathematics there is not. This is a major source of confusion both to students learning calculus and professionals using computers to do the work of calculus.

It's possible to present calculus without functions. For instance, Isaac Newton, the inventor of calculus, spoke of "flowing quantities."^[In Newton's language, a "flowing quantity" was a *fluent* and the change in a flowing quantity was a *fluxion*.]

It's practically impossible (and generally unwise) to do computing without functions. They are a basic building block of every mainstream modern computer language. Since the operations of calculus in actual practice are performed on the computer, common sense suggests that we should describe calculus in terms of functions. That's what we will do in this book.

For you, this may take a bit of getting used to. The reason is that the notation used in high-school algebra and in almost all calculus texts is not the notation of functions. For example, almost all students have seen a mathematical ***expression*** in this form: $$y = m x + b$$
Using the language of math classes, we can say that the expression "describes a relationship between two variables." And from your experience, you know that the ***variables*** are $x$ and $y$. The other letters, $m$ and $b$, are something else. Many students will reflexively call them the "slope" and "intercept" of the "line." 

You are so used to this that you probably don't see the huge ambiguity involved. For instance, what kind of mathematical thing is being referred to in $y = m x + b$? It's an ***equation***, for sure. It's a straight line. But a straight line is not the same thing as an equation. And vice versa. So we might say, "It is an equation describing a straight line." 

If you've studied calculus before, you may have seen an equation like $y' = m$ or perhaps $dy/dx = m$ which is another way of describing a straight line.

The notation of equations is a poor substitute for the notation of functions. For instance, what is the input and what is the output? It's not explicitly stated and left to the reader to wonder. If we re-arrange $$y = m x + b \ \ \text{into}\ \ \ 0 = m x + b - y$$ using the allowed manipulations of algebra do we still have a function? If so, is it the same function as $m x + b$? Would $x$ be the input or would both $x$ and $y$ be inputs? And if $mx + b - y$ is a function, then is the output always zero?

The engineers and mathematicians who invented computer languages realized over about a decade that they had to be more explicit in identifying the input, the output, and the function itself. Why? Because computers need to be given unambiguous instructions.^[Actually, it's common to give computers ambiguous instructions. The computer will carry out the instruction in the way it does, which may not be anything like what the programmer expected or intended. So, eliminating ambiguous instructions has become a best practice in computer programming.] Sorting this out was a difficult process even for those mathematically talented and skilled pioneers of notation. So, you can be forgiven for the occasional confusion you have when dealing with notation that pre-dates computing. 

In this book we'll be explicit and consistent in the ways we denote functions so that you can always figure out what are the inputs and how they are being translated into the output. A good start in learning to read the function notation is to see the equivalent of $y=m x + b$ in that notation: $$g(x) \equiv m x + b$$ 
Notice that we're using $\equiv$ (with three parallel bars) rather than $=$. And we don't need any $y$. $x$ appears on both sides of $\equiv$ And there is a place to state the ***name*** of the function, in this case $g()$, so that we can refer to the function unambiguously when we are doing operations on it. 

## The naked modeling functions {#naked-intro}

Experience allows us to make a short list of mathematical functions that provide a large majority of the tools for representing the real world as a mathematical object. Think of this list as different actors, each of whom is skilled in portraying an archetypical character: hero, outlaw, lover, fool, comic. A play brings together different characters, costumes them, builds on dialog. 

A mathematical modeler is a kind of playwright. She combines mathematical character types to tell a story about relationships. But there is only a handful of archetypical mathematical functions, the analog of the character actors in drama and comedy. We are calling these the ***naked modeling functions***. In writing a mathematical model, you will clothe the actors to suit the era and location and assemble them together in harmony or discord. 

Here is a list of our basic, unadorned functions, the naked model function, 

* Exponential $e^x$
* Logarithm $\ln(x)$
* Power-law $x^p$
* Sinusoid $\sin(x)$
* Straight-line $m x + b$
* Hump $\text{dnorm}(x)$
* Sigmoid $\text{pnorm}(x)$

We've written these here in a traditional notation so that you can see the connections to the math you've already studied. We've used $x$ to stand for the single input to these functions just because that's traditional. 

It's good to refer to the functions by their word name---exponential, logarithm, power-law, sinusoid, straight-line, etc. This helps to avoid the source of common confusion. For example, the mathematical expressions $e^x$ and $x^e$ are easily confused, but they are notation for ***utterly different patterns***. You won't so easily mistake "exponential" and "power-law." 

::: {.takenote}
One important point to make here is that this is a very short list. You should memorize the names and be able easily to associate each name with the traditional notation. 

Over the next several chapters, we will introduce several features of functions. Some of our basic modeling functions have these features, some don't. These features include:

- monotonicity up or down
- concavity up or down
- horizontal asymptotes
- vertical asymptotes
- periodicity

By the end of Block 1, you should be able to list all seven basic modeling functions and say which of these features are relevant to each.

:::

You will also use and see the computer names for these functions. The names can differ somewhat from one computer language to another, but the names in the language we will use, R, are easily recognized by programmers who use any other language.

For ease of reference, here's a table

Name | Traditional notation | R expression
-----|----------------------|---------------
Exponential | $e^x$ | `exp(x)`
Logarithm | $\ln(x)$ | `log(x)`
Power law | $x^p$ | `x^p`
Sinusoid | $\sin(x)$ | `sin(x)`
Straight-line | $m x + b$ | `mx+b`
Hump |   | `dnorm(x)`
Sigmoid | | `pnorm(x)`

We've left out the traditional notation for the hump and sigmoid because there isn't a standard one. Surprisingly, there is no specific name in R for the Power law or Straight-Line functions, but it's easy to implement them when needed using the code above.

You noticed that the section heading is "The naked modeling functions." We mean "naked" in a metaphorical sense, and chose the metaphor to make it easy to remember. Think of this list of seven functions as the celebrities of the world of calculus. Unlike human celebrities who appear and wane over the years, and marry and divorce each other frequently, these celebrities have been with us for generations and maintain intimate connections with one another that reflect the nature of mathematics rather than the fads and fancies of celebrities. (Mastering calculus is largely a matter of becoming familiar with the mathematical connections. You'll see these in due time.)

These (basic) celebrity functions appear in many mathematical settings, just as a human celebrity strives to maintain a public image. The human celebrity is a human organism and that organism is naturally naked. In public appearance, however, the celebrity always is clothed in one way or another. (OK ... Rarely some of them appear unclothed and the same is true in mathematics.) In other words, in order to interact with the world at large, the celebrities need attire. 

Similarly, the mathematical functions that appear in real-world applications---as opposed to most math textbooks---always wear clothes, they are adorned with what we call ***parameters***. Parameters help them deal with the units and dimension of quantities. And just as there are standard elements of clothing: shirt, skirt, trousers, ... there are standard ways of clothing the naked modeling function. The process of decorating basic modeling functions is called the ***parameterization*** of the function, and there are often multiple ways of paremeterizing the same function.

Once we dress the naked functions---that is, *parameterize* them---they will become the superheroes of calculus. We'll call this league of superheroes the ***basic modeling functions***.

# Structure of a function

::: {.objectives data-latex=""}
```{r echo=FALSE, results="asis"}
state_objective("F-10", 
text="Objective Original: Understand the relationship between inputs and outputs for functions of one or two input variables, to include functions defined piecewise.
**Objective Revised**: Recognize that functions are a way of representing (storing) what we know and be able to use properly the basic nomenclature of functions).")
```
:::


You're used to mathematical functions being stated as ***formulas***, expressions composed of addition, multiplication, square roots, and so on. The expression $m x + b$ uses a multiplication and an addition. $\sqrt{\strut 1 - x^2}$ uses exponentiation ($x^2$), subtraction and square root.

But there's nothing in the mathematical concept of "function" that requires a formula. And computer functions in general are not based on a formula. (The word used to describe the internals of a computer function is ***algorithm***, which is a generalization of "formula" that includes many non-arithmetic operations such as looping and branching).

We will be using formulas extensively, but best if you can visualize functions generally as something that's not necessarily a formula. This section gives another perspective on how to describe and think about a function. But remember, functions ***take inputs*** and ***return the corresponding output***. Any arrangement that accomplishes this is a function, even if arithmetic is nowhere in sight.

People have many ways of organizing what they know. Often we rely on our intuitive abilities of memory, but we also have constructed frameworks to make the storage more accessible and reliable. One of these frameworks is the ***table***, generally set up as an array of rows and columns. For instance, here is a table about a range of internal combustion engines of various sizes:

```{r}
DT::datatable(Engines)
```

Each row of the table reports on one, specific engine. Each column is one attribute of the of an engine. Using such tables can be easy. For example, if asked to report how fast the engine named "Enya 60-4C" spins, you would go down to the Enya 60-4C row and over to the "RPM" column and read off the answer: 11,800 revolutions per minute (RPM).

A table like this can be said to describe as raw numbers the general relationships between various engine attributes. For instance, we might want to understand the relationship (if any) between RPM and engine mass, or relate the number and diameter (that is, "bore") and depth (that is, "stroke") of the cylinders to the power generated by the engine. Any single entry in the table doesn't tell us about such general relationships; we need to consider the rows and columns as a whole.

If you examined the relationship between engine power and cylinder number, diameter, and depth, you will find that (as a rule) the larger the number, diameter, and depth, the more powerful the engine. That's a ***qualitative*** description of the relationship. Most educated people are able to understand such a quantitative description. Even if they don't know exactly what "power" means, they have some rough conception of it. And almost everyone knows what diameter, depth, and number mean.

Often, we're interested in having a ***quantitative*** description of a relationship such as the one (number, diameter, depth) $\rightarrow$ power. Remarkably, the many of educated people are uncomfortable with the idea of using quantitative descriptions of a relationship: what sort of language the description should be written with; how to perform the calculations to use the description; how to translate between data (such as in the table) and a quantitative description; how to translate the quantitative description to address a particular question or make a decision.

This course is about constructing and using such quantitative descriptions: that is, ***mathematical modeling***. Skills for modeling are essential for work in engineering and science, and highly valued in many other fields in commerce, management, and government. Often, the work of applying such quantitative skills is called ***calculation***. The name ***calculus*** is used to describe the methods that are widely used for undertaking calculations.

***Functions*** are a fundamental way of organizing mathematical models and calculations. You have undoubtedly seen them in your previous mathematics education, but it's worth reviewing them from the basics so that we can share a vocabulary for communicating about them. 

- A function is a transformation from one or more ***inputs*** to an ***output***. 
- To keep things simple for now we'll focus on inputs and outputs that are ***numeric***, but later we'll need a more nuanced view of "numeric" that takes into account the different kinds of things that are represented by numbers, e.g. length, power, RPM.

::: {.workedexample}
***Functions as bureaucracy***

To see how functions work, imagine a long corridor with a sequence of offices, each identified by a room number. The input to the function is the room number. To ***evaluate*** the function for that input, you knock on the appropriate door and, in response, you'll receive a piece of paper with a number to take away with you. That number is the output of the function.

This will sound at first too simple to be true, but ... In a mathematical function each office gives out exactly the same number every time someone knocks on the door. Obviously, being a worker in such an office is highly tedious and requires no special skill. Every time someone knocks on the worker's door, he or she writes down the *same* number on a piece of paper and hands it to the person knocking. What that person will do with the number is of absolutely no concern to the office worker.

The utility of such functions depends on the artistry and insight of the person who creates them: the ***modeler***. An important point of this course is to teach you some of that artistry. Hopefully you will learn through that artistry to translate your insight to the creation of functions that are useful in your own work. But even if you just use functions created by others, knowing how functions are built will be helpful in using them properly.

In the sort of function just described, all the offices were along a single corridor. Such functions are said to have ***one input***, or, equivalently, to be ***functions of one variable***. To operate the function, you just need one number: the address of the office from which you'll collect the output.

Many functions have more than one input: two, three, four, ... tens, hundreds, thousands, millions, .... In this course, we'll work mainly with functions of two inputs, but the skills you develop will be applicable to functions of more than two inputs.

What does a function of two inputs look like in our office metaphor? Imagine that the office building has many parallel corridors, each with a numeric ID. To evaluate the function, you need two numeric inputs: the number of the corridor and the number of the door along that corridor. With those two numbers in hand, you locate the appropriate door, knock on it and receive the output number in return.

Three inputs? Think of a building with many floors, each floor having many parallel corridors, each corridor having many offices in sequence. Now you need three numbers to identify a particular office: floor, corridor, and door. 

Four inputs? A street with many three-input functions along it. Five inputs? A city with many parallel four-input streets. And on and on.

Applying inputs to a function in order to receive an output is only a small part of most calculations. Calculations are usually organized as ***algorithms***, which is just to say that algorithms are descriptions of a calculation. The calculation itself is ... a function! 

How does the calculation work? Think of it as a business. People come to your business with one or more inputs. You take the inputs and, following a carefully designed protocol, hand them out to your staff, perhaps duplicating some or doing some simple arithmetic with them to create a new number. Thus equipped with the relevant numbers, each member of staff goes off to evaluate a particular function with those numbers. (That is, the staff member goes to the appropriate street, building, floor, corridor, and door, returning with the number provided at that office.) The staff re-assembles at your roadside stand, you do some sorting out of the numbers they have returned with, again following a strict protocol. Perhaps you combine the new numbers with the ones you were originally given as inputs. In any event, you send your staff out with their new instructions---each person's instructions consist simply of a set of inputs which they head out to evaluate and return to you. At some point, perhaps after many such cycles, perhaps after just one, you are able to combine the numbers that you've assembled into a single result: a number that you return to the person who came to your business in the first place. 

A calculation might involve just one function evaluation, or involve a chain of them that sends workers buzzing around the city and visiting other businesses that in turn activate their own staff who add to the urban tumult.
:::

::: {.caution}
The reader familiar with floors and corridors and office doors may note that the addresses are ***discrete***. That is, office 321 has offices 320 and 322 as neighbors. Calculus is about continuous functions, so we need a way to accept, say, 321.487... as an input. There is no such office. 

A slight modification to the procedure will produce a continuous function. It works like this: for an input of 321.487... the messenger goes to both office 321 and 322 and collects their respective outputs. Let's imagine that they are -14.3 and 12.5 respectively. All that's needed is a small calculation, which in this case will look like $$-14.3 \times (1 - 0.487...)   + 12.5 \times 0.487...$$ This is called ***linear interpolation*** and lets us construct continuous functions out of discrete data. 

In Blocks 2 and 5 we'll discuss other widely used ways to do this that produce not just continuous functions but ***smooth*** functions. Understanding the difference between continuous and smooth will have to wait until we introduce a couple more calculus concepts: derivatives and limits.
:::

## Domain: input space



As you know, there is a powerful way of thinking about numbers in terms of ***space*** and ***geometry***. For instance, a single number corresponds to a point on a line: the so-called ***number line***. A pair of inputs, say, (x, y) corresponds to a point in a plane, often called the ***Cartesian coordinate plane***. Three numbers corresponds to a point in space, perhaps organized into (x, y, z) of a Cartesian space. There are higher-dimensional spaces, but usually special training is needed to become comfortable with them. If you are having this discomfort, you might prefer to work with the office metaphor. Just for fun, here's how you can think of a 10-dimensional space: 10 numbers, one telling you which planet, the next specifying the continent on that planet, and so on for country, state, city, street, building, floor, corridor, door.

The set of inputs with which the function can be evaluated is called the ***domain*** of the function. Sometimes we describe the domain as a space, e.g. the number line, the plane, and so on. Sometimes domains including more restrictions. For instance, a particular input might only meaningfully be positive, with no offices corresponding to negative values for that input. Or, an input might be restricted to be in the interval 0 to 1. Sometimes in calculus, the domain excludes an isolated point. For instance, there may be no office at the door marked 0 but the neighboring doors open into working offices.

The ***range*** of a function is the set of all the *outputs* that can be produced. Since at this stage we're working only with functions that return a single number as output, it's common to describe the range as all or part of the number line. For instance, some functions only have positive outputs. Other functions' outputs are always in the interval 0 to 1. (This is the case, for instance, when the function returns a probability as the output.)

::: {.worked-example data-latex=""}
**Weather forecasting by numerical process**

*[Weather forecasting by numerical process](https://archive.org/details/weatherpredictio00richrich/page/184/mode/2up?view=theater)* is a highly influential book, from 1922, by [Lewis Fry Richardson](https://en.wikipedia.org/wiki/Lewis_Fry_Richardson). He envisioned a calculation for a weather forecast as a kind of function. The domain for the forecast is the latitude and longitude of a point on the globe, rather than the rectilinear organization of corridor. 

One fantastic illustration of the idea shows a building constructed in the form of an inside-out globe. At each of many points on the globe, there is a business. (You can see this most clearly in the foreground, which shows several boxes of workers.)

```{r echo=FALSE, out.width="70%", fig.align="center", fig.cap="An artist's depiction of the organization of calculations for weather forecasting by Richardson's system. [Source](https://www.cabinetmagazine.org/issues/27/foer.php)"}
knitr::include_graphics(normalizePath("www/Richardson-globe.jpg"))
```

Each business might work this way: In each business there is a person who will report the current air pressure at that point on the globe, another person who reports the temperature, another reporting humidity, and so on. To compute the predicted weather for the next day, the business has a staff assigned to visit the neighboring businesses to find out the pressure, temperature, humidity, etc. Still other staffers take the collected output from the neighbors and carry out the arithmetic to translate those outputs into the forecast for tomorrow. For instance, knowing the pressure at neighboring points enables the direction of wind to be calculated, thus the humidity and temperature of air coming in to and out of the region the business handles. In today's numerical weather prediction models, the globe is divided very finely by latitude, longitude, and altitude, and software handles both the storage of present conditions and the calculation from that of the future a few minutes later. Repeating the process using the forecast enables a prediction to be made for a few minutes after that, and so on.

Some of the most important concepts in calculus relate to the process of collecting outputs from neighboring points and combining them: for instance finding the difference or the sum. To illustrate, here is the first set of equations from Richardson's *Weather forecasting ...* written in the notation of calculus:

```{r echo=FALSE, out.width="70%", fig.align="center"}
knitr::include_graphics(normalizePath("www/Richardson-equations.png"))
```

You can hardly be expected at this point to understand the calculations described by these equations, which involve the physics of air flow, the coriolis force, etc. but it's worth pointing out some of the notation:

- The equations are about the momentum of a column of air at a particular latitude ($\phi$) and longitude.
- $M_E$ and $M_N$ are east-west and north-south components of that momentum. 
- $\partial M_E /\partial t$ is the amount the east-west momentum will change in the next small interval of time ($\partial t$).
- $p_G$ is the air pressure at ground level from that column of air. 
- $\partial p_G / \partial n$ is about the difference between air pressure in the column of air and the columns to the north and south.

Calculus provides both the notation for describing the physics of climate and the means to translate this physics into arithmetic calculation.
:::

::: {.todo}
In the section on linear combinations, have a sidebar that explains that the so-called "straight-line" function is really just a linear combination of the more fundamental functions $\text{identity}(x) \equiv x$ (which is exactly the same thing as the power-law function $x^p$ with $p=1$) and and $\text{constant}(x) \equiv 1$ which is pretty much the same as the power-law function $x^0$. 
:::

::: {.todo}
In the limits section or an exercise, ask about $x^0$? Is that the same as 1? What about when $x=0$. Zero raised to any positive power is 0. But any positive number raised to 0 is 1. So two ways of thinking about it give different answers.

The computer handles the question with ease:

```{r}
0^0
```
But computer results don't always agree with mathematical results. 

In this case, mathematicians looking at the question through the lens of limits, agree with the computer when looking at the function $g(x) \equiv x^0$. But they disagree if the function is $h(x) \equiv 0^x$ or $f(x) \equiv x^x$.  These three functions are not the same when it comes to limits. 

Remember that $\lim_{x\rightarrow 0}$ is not about replacing $x$ with zero, it is about a process of examining the result for non-zero $x$ as $x$ approaches zero. In "zero raised to any positive power" we're talking about $0^{\lim_{x\rightarrow 0}}$, which is not the same as $\lim_{x\rightarrow 0} \left[x^x\right]$.  Similarly, $\left[\lim_{x\rightarrow 0}\right]^0$ is different from 
:::


<!--chapter:end:Fun-change.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Describing functions {#fun-describing}

```{r include=FALSE}
library(etude2)
library(mosaic)
library(mosaicCalc)
```

We will need to communicate about functions to your fellow humans and to computers. Important modes of communication include:

* Graphs of functions that show their "shape"
* Tables of inputs and outputs
* Word descriptions
* Computer function name
* Mathematical notation

In this chapter, we'll illustrate these five different modes using the naked modeling functions. It's important to become proficient at all five.

## Computer commands

We'll start with simple computer commands that enable us to draw graphs.

::: {.scaffolding}
To make a graph of a function with one input, use the `slice_plot()` command, like this:
```{r results="hide"}
slice_plot(exp(x) ~ x, domain(x=c(-2,2)))
```
:::

You can simply copy-and-paste the command into a sandbox to create the graph. But let's take apart the command into it's components to see how R commands are structured.

1. `slice_plot( ... )` Commands usually start with the name of the operation to perform. The name is *always* followed by a pair of parentheses. Those parentheses will contain the ***arguments*** of the operation, which you can think of as specifying the details of what is to be done. The `slice_plot()` operator draws graphs for functions of a single input.
2. `exp(x) ~ x` is being given as the first argument to the `slice_plot()` operator. We'll defer a detailed explanation, just pointing out that we are specifying to the computer that we want a plot of the ***exponential*** function and that we are going to use `x` as the name of the input.
3. `domain(x = c(-3,3))` Functions have mathematical domains: the set of valid inputs to the function. The exponential function has the entire number line as the domain. To draw a function graph you need to specify the ***graphical domain***, that is, which part of the function domain to show in the graph. 

## Tables of inputs and outputs

Another way of describing a function is to give a table of inputs and outputs. Like graphics, a table can only show some of the possible inputs.

```{r echo=FALSE}
tibble::tibble(
  input = seq(-2, 2, by = 0.25),
  output = exp(input)
) %>%
  knitr::kable() %>%
  kableExtra::kable_styling()
```
Before modern computing, tables were one of the primary means to describe functions. People working with calculus needed a reference collection of books containing tables for the functions they used. 

With computers, we have better and faster ways to get the output of a function from the input. Still, modelers often use recorded data to construct functions. Tables are perhaps the most widely used method for storing and accessing data, although electronic spreadsheets are used today instead of printed tables.

Even today, a table can be a nice way to describe a function when we are interested in the output from only a handful of the possible inputs. 

Conceptually, it's helpful to keep in mind that every naked modeling function is just a way of organizing information that could have been stored in a table. 

::: {.todo}
Exercise: give seven tables of the naked modeling functions. Use a sandbox to determine which table is which function.



:::

## Word descriptions

Knowing and correctly using a handful of phrases goes a long way in being able to communicate with other people about functions with a single input. Often, the words make sense in everyday speech ("steep", "growing", "decaying", "goes up", "goes down", "flat). 

Sometimes the words are used in everyday speech but the casual person isn't sure exactly what they mean. For instance, you will often hear the phrase "growing exponentially." The graph of the exponential function illustrates exactly this sort of growth: flat for small $x$ and growing steadily steeper and steeper as $x$ increases. 

Still other words are best understood by those who learn calculus. "Concave up," "concave down", "approaching 0 asymptotically," "continuous", "discontinuous", "smooth", "having a minimum **at** ...," "having a minimum **of** ...", "approaching $\infty$ asymptotically," "having a vertical asymptote."

::: {.todo}
Exercise: Show graphs of the naked modeling functions. Ask which of the phrases apply.
:::

::: {.todo}
Exercise: Show a graph and ask questions about asymptotes. Use the phrases "as the input gets very small" (meaning $x \rightarrow -\infty$), "as $x$ goes to zero" (meaning $x \rightarrow 0)"
:::

## Computer function names

As you might expect, computer programmers and language developers have written software implementing several of the ***naked modeling functions***: `exp()`, `log()`, `sin()`, `dnorm()`, and `pnorm()`. For these functions, the name tells everying, so far as the computer is concerned, that is needed to calculate the output from any given input.

Computer notation for the power-law and straight-line functions is different. It will be much easier to understand once you have seen how to create and name your own functions. The reason for this difference is that the power-law and straight-line functions are not quite naked. Both of them have parameters: the exponent in the power-law function and the slope and intercept in the straight-line function.

::: {.why}
**Why do you include the power-law and straight-line functions in the list of naked modeling functions when they are not naked?**

We're using the "naked modeling function" list to accomplish two things at the same time:

i. To emphasize that in modeling real-world situations you should always expect that your functions will have parameters.
ii. To point out that a large fraction modeling situations can be handled by just a few function "shapes."  

To make the list of shapes in (ii) comprehensive, we've had to stretch the "naked" metaphor a bit.
:::

## Mathematical notation

You have grown up with traditional mathematical notation and have are likely familiar with the notation for several of the naked modeling functions: $\ln x$, $\sin x$, $e^x$, $x^p$ (as in $x^2$ where $p=2$ or $\sqrt{x}$ where $p=1/2$).

Traditional notation mixes up several things that computer notation sensibly keeps separate. From the computer programmer's point of view, traditional notation is idiosyncratic rather than systematic. 

If you are interested in these things, three concepts from computing may help you appreciate the the differences:

- ***Functional notation*** In computing, the notation in which a function **name** is followed by parentheses^[Or sometimes square braces or curly braces or another token, depending on the language.] with the inputs **inside** the parentheses is called "functional notation." functional notation is part of traditional mathematical notation, although often the parentheses are left out.
- ***Infix notation*** For functions with two inputs, computing languages often support a different arrangement of the function name and the inputs where the name comes between the arguments. You've seen this in parts of traditional arithmetic notation, for instance, $3 + 5$ or $8/2$. Infix notation is sometimes used in place of functional notation as in `3^2` or `x^3`. 
- ***Markup notation*** You are undoubtedly familar with word processing and, particularly, a style of word processing called What-you-see-is-what-you-get (WYSIWYG). In WYSIWYG, you can enter plain text just using the keyboard, but if you want to make something **boldface** or **italics**, you use the mouse to select the text involved and select a style from a menu. Typically, WYSIWYG mathematical content involves a similar mouse-based process. In contrast, in mainstream computer languages, the mouse is not needed at all. The computer commands are constructed from plain, linear sequences of letters and other characters. 

    Computer programmers helpfully observed that the word-processing process can be constructed as simple plain text input to computer program that interprets the input in well-defined ways and carries out ***typesetting***. To illustrate, consider the next couple of lines. The first shows an ordinary looking word-processor formatted sentence. The second shows the way this was encoded so that a typesetter can produce the formatted content.
    
    > $e^x$, $\sqrt{x^2}$ and $\int_0^\infty \frac{1}{x^2} dx$ are examples of *traditional* notation.^[A footnote]
    
    > `$e^x$, $\sqrt{x^2}$ and $\int_0^\infty \frac{1}{x^2} dx$ are examples of *traditional* notation.^[A footnote]`

Traditional mathematical notation includes features such as superscripts and special symbols that are easily written out with pencil in hand. Often this notation is beautiful and aesthetically cherished by mathematicians. But computer commands in most computer languages are straight sequences of characters using function or infix notation.^[Some computer languages use notation where the name of the function is contained inside the parentheses as in `(+ 2 3)`.]




## Exercises




<!--chapter:end:Fun-describing.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Notation for functions {#fun-notation}

Part of the difficulty of mathematics for many people is making sense of the nomenclature and notation. What you were taught in high school is a highly idiomatic system that can be mastered only with experience. Mathematicians are undoubtedly skilled in logic, but mathematics itself has an ancient history which has littered the language with synonyms, near synonyms, inconsistencies, diacritical marks, letters in unfamiliar alphabets. An Oxford mathematician wrote a poem that has become famous as nonsense:

*’Twas brillig, and the slithy toves  
       Did gyre and gimble in the wabe:  
All mimsy were the borogoves,  
       And the mome raths outgrabe.*  
    
Here are some words commonly encountered in traditional mathematics notation. 

**equation**, **formula**, **function**, **variable**, **unknown**, **root** 

And here are a few mathematical sentences.

i. $y = x$
ii. $y = \sqrt{x}$
iii. $y^2 = x$
iv. $x^2 = x$
v. $x = \sqrt{x}$

All five sentences are *equations*. That's easy, because they each have an equal sign between the two sides. Which are *formulas*? Which are *functions*? You're used to calling $x$ and $y$ *variables*. When do they become *unknowns*? Sentence (iv) might be interpreted as describing *roots*. But sentence (v) says the same thing as (iv) but would not be related to roots.

## A notation for computing

The traditional notation is practically useless for computing. A programmer has to have a deep understanding of what the notation is intended to mean in any given circumstance before she can construct a computer expression that will carry that same meaning in the computer's work.


::: {.scaffolding}
To illustrate, here are some similar-looking sentences. In math notation, each of them can mean something. In R, one of them is valid and the others invalid. Read each one and try to guess which one is valid and why the others are not.
```{r eval=FALSE}
y = x
y = sqrt(x)
y^2 = 3
y = sqrt(3)
y - x = 0
0 = (x+3)(x-2)
```
Once you've made your guesses, open a sandbox to see if you got it right.
:::

::: {.todo}
Produce an MC exercise that addresses the why's of the above.
:::

The student who knows how to make sense of math notation will find this not of much help in writing computer notation. It's like a well educated foreigner trying to make sense of how some of these sentences are meaningful and others not.

"Chair a meeting," but not "seat a meeting." 
"Seat a guest," but not "chair a guest."  
"Bush yourself out," but not "tree yourself out."  
"Tree a cat," but not "bush a cat."  
"Table a motion," but not "desk a motion."  
"Bench a player," but not "couch a player."   
"Couch a meaning" but not "bench a meaning."  


In this book, we're going to use a mathematical notation that corresponds to a usable computer notation. The first step is to stop using $=$ to mean so many different things. 

## Objects and actions

::: {.objectives data-latex=""}
```{r echo=FALSE, results="asis"}
state_objective("F-30", "Identify the structure of function notation when the function is defined by a formula.")
```
:::


We're going to give names to mathematical "objects" and actions. In this introduction, we'll use **boldface for objects** and *italics for actions.*

* A **function** is a mathematical object that gives instructions about how to transform one or more **inputs** into an **output**. Typically, but not always, we will give names to functions so that we can refer to them. You can think of a function like a recipe: it tells you what to do with the inputs to create the output. You're used to functions where the recipe is written as a **formula** using arithmetic and other operators. A more general term for the recipe in a function is **algorithm**. (Sectin \@ref(sec:algorithms) gives a definition of "algorithm" and shows examples that are not formulas.)


* An **argument** is a symbol that stands for an **input** in an **algorithm**.

* *Applying a function to inputs* means to carry out the steps of the function's algorithm, inserting the appropriate quantity in place of the symbol. Recipes will use symbols like "rice" to describe the algorithm. In cooking, you apply a recipe to the incredients. Where the recipe says, "Add rice to the boiling water" you use the physical rice and the physical boiling water, instead of the symbols. Applying a function to inputs is so fundamental to computing, that you'll often hear other words for it than "apply": *run the function* or *evaluate the function* or *execute the function*.
    
To *define a function* in mathematical notation we will write something like this: $$g(x) \equiv 4 x^2 - 7$$ 

* $g()$ is the name of the function. 
* The function takes just one argument, which is being written with the symbol $x$ in the definition. To make double sure that the human reader sees the symbol being used for the argument, we are putting the list of symbols in the parentheses following the function name. The definition of a function with two inputs  looks like $h(x, y) \equiv 3y - 5 x^2 + x$ and functions with more than two inputs follow the same pattern.
* The algorithm of $g()$  is presented as a formula. The formula in the example, $4 x^2 - 7$ says, "Take the input quantity. Multiply 4 by the input and then multiply by the input again. Subtract $7$ from that to produce the output of the function. 
* We use $\equiv$ as the punctuation to distinguish the function name (and argument list) from the algorithm. The algorithm is always on the right of $\equiv$, the name of the function and argument list are to the left.

The command `g(3)`, applies the function named $g()$ to the value 3. Do this by replacing the $x$ in the algorithm by the $3$ and carry out the calculations in the algorithm. For $g(3)$, the output will be 29. 

In R, a function is defined using a very similar expression. For the $g()$ we defined above, the R definition would be
```{r}
g <- makeFun(4*x^2 - 7 ~ x)
```

i. The name of the function, without parentheses, is on the left side of the ***assignment operator*** `<-`. 
ii. We could use the name `g` to refer to anything, function or not. Here, you can see that `g` is the name of a function by looking at the object on the right side of `<-`. The R/mosaic operator `makeFun()` is what constructs functions. 
iii. The text in the parentheses of `makeFun` is also an R expression. We call this sort of expression a ***tilde expression***. (The symbol `~` is called "tilde.") The tilde character is simply punctuation to separate the expression on the left side, `4*x^2 - 7`, from the expression on the right side. The right side of the tilde expression identifies which symbols are the arguments. (In this case, `x` is the only argument.) The left side of the tilde expression describes the algorithm for the function, using the argument names specified on the right side.

To *apply the function* $g()$ to an argument, we write an R expression with the value for the argument given in the parentheses. For instance, to apply $g()$ to the value 3, we write:
```{r}
g(3)
```




## Formulas and algorithms {#algorithms}

The function algorithms in the previous section involves only arithmetic. More generally, a formula will involve the invocation of another function. For example: $\sqrt{\strut m x + b}$ or $\sin(m x) + \ln(b)$.
    
The idea of "algorithm" generalizes that of formulas. A starter definition is:

> An ***algorithm*** is a set of instructions for performing a computation. 

High-school math typically involves presenting algorithms as formula. You learned, maybe in middle school, how to follow the arithmetic steps involved in algorithms described as formulas. You also have likely used a calculator to perform some of the arithmetic or to evaluate functions such as $\sin()$ and $\ln()$ and $\sqrt{\strut}$ for which you don't know how to evaluate using simple arithmetic. The calculator is implementing an algorithm with which its been programmed to enable the calculation of $\sin()$ and $\ln()$ and such.

A slightly more detailed definition of "algorithm" highlights that algorithms are written in terms of other, simpler algorithms.

> An ***algorithm*** is a set of instructions for performing a computation written in terms of other algorithms that we already know how to perform.

For our purposes, the "algorithms that we already know how to perform" will be taken to be arithmetic---addition, subtraction, multiplication, division---as well as the evaluation of the naked modeling functions. Admittedly, people cannot compute logarithms as fluently as they can add numbers, but you already have the R implementations of the basic modeling functions: `exp()`, `log()`, `sin()`, power-law, `dnorm()`, and `cnorm()`. 
The vast majority of functions you will see in this book (and in mathematical modeling in general) can be constructed out of basic arithmetic and the application of the naked modeling functions.

## Algorithms without formulas

Many functions are described by algorithms that use concepts common in computer programming but unknown to traditional mathematical notation. Some of these have names like ***iteration*** or ***branching*** and many refer to stored lists of fixed numbers (like the office workers in the street, building, floor, corridor, door image of a function). We'll deal with some of these things later, but for now ...

> We are going to use the word ***algorithm*** to name the kind of expression to the right of $\equiv$ in a function definition. A ***formula*** is a specific kind of algorithm generally written in traditional math notation.

Algorithms, including the ones that are formulas, are written in terms of a set of ***symbols*** that stand for inputs. This is a high-fallutin' way of saying something simple: in $mx + b$, the $x$, $m$, and $b$ are the names we give to the quantities being used in the calculation.

The notation we are using for function definition lists some of these names in two places, and others in only one place. Again, look at $$g(x) \equiv m x + b$$
The $x$ appears both in the algorithm and the list of input name $(x)$ to the left of $\equiv$.

The $m$ and $b$ are different; they appear **only** in the algorithm. The word for such quantities in mathematics is ***parameter***. Eventually, when the algorithm is followed, we're going to have to put in specific numerical values in place of each parameter. 

**Where will these parameter values come from?** This is a subject on which mathematical notation is silent. You have to figure it out from context and experience. This is potentially very confusing, especially when a human is not around to sort things out.

## Computer notation

The notation used in computer programming lets us be explicit about which symbols refer to function inputs and which to parameters of the function. 
Depending on the computer language things can be handled in one way or another. For experienced computer programmers: This is the issue of ***scope*** and can be complex in its own right. 

In the software used in CalcZ (R, with the `mosaic` package of extensions, which we'll refer to as R/mosaic), we will take a simple-to-use approach. It works like this: 

> All modeling functions we construct with R/`mosaic` will list parameters formally as ***arguments*** to the function.

It is as if we wrote in traditional notation $$g(x, m, b) \equiv m x + b$$

There is also a way to give default numerical values to parameters so that you can write $g(3.5)$ and the computer will know where to find the parameter values. In writing about formulas using math notation, we'll extend the traditional notation to write, for instance, $g(x, m=2, b=3) \equiv m x + b$.

In R/`mosaic`, we would construct a mathematical function like $g()$ using the `makeFun()` function:

```r
g <- makeFun(m*x + b ~ x, m=2, b=3)
```

You could also write 

```r
g <- makeFun(m*x + b ~ x)
```
but this notation means that you will have to give specific numerical values for the `m` and `b` inputs whenever you evaluate `g()`. There won't be any default values for the "parameters-as-inputs" `m` and `b`.

<!-- Relevant Blog post: CalculusBlog/post-formal-arguments.Rmd -->


::: {.objectives data-latex=""}
```{r echo=FALSE, results="asis"}
state_objective("F-31-R", "Identify and create R versions of mathematical functions using `makeFun()` and tilde expressions.")


```
:::

::: {.objectives data-latex=""}
```{r echo=FALSE, results="asis"}
state_objective("F-33", "Master the conventions used in CalcZ for giving names to functions and for referring to functions with \"pronouns.\"")
```
:::

## CalcZ naming conventions

We're going to be using a lot of functions in CalcZ. Some of these functions have ***proper names***, usually written using short sequences of letters: for instance the naked modeling functions $\sin()$, $\ln()$, $\exp()$. 

Other functions will needed just for a sentence or a paragraph or a section, perhaps being used in an example or to lay out the steps of an algorithm. In ***natural languages*** such as English, we often use ***pronouns*** for such purposes: she, he, it, they, we, I, .... We also will use pronouns for identifying functions. Our policy is this:

> A pronoun for a function will be named $f()$ or $g()$ or $h()$ or the corresponding upper-case letters $F()$, $G()$, or $H()$. **The particular choice of letter f, g, or h has no significance whatsoever.** It is just a way to give an unambiguous handle for a function that we are going to be using for a little while. 

On occasion, we will use subscripts or superscripts on these pronoun letters, for instance $f_1()$ or $g^\star()$. This is a way to give us many more possible pronouns when we need them. Perhaps this is analogous to words like "sister," "parent," "husband," "cousin," etc. that allow us to refer, without a proper name, to a specific person.

On other occasions, where a function relates to a specific quantity such as position or velocity, we will use the names $x()$, $y()$, $z()$, $u()$, $v()$, $w()$, and such. These letters are, of course, the same ones we frequently use to name the inputs to functions. How do you know whether the letter is meant to refer to an input rather than a function? The parentheses provide the clue: $x()$ is a function name, $x$ is an input name. 

It can become tedious to give a name to every function, even if it's not being used again. Consider this sentence as an example: 

> "The functions $g(x) \equiv \sqrt{\strut x}$ and $h(x) \equiv x^3$ are examples of power-law functions." 

Long and awkward. So we'll feel free to write instead, 

> "$\sqrt{\strut x}$ and $x^3$ are examples of power-law functions."

Without the formality provided by "$g(x) \equiv$" it can be hard to know whether $\sqrt{\strut x}$ means "the square-root function" or "take the square root of a specific number $x$. The convention we will use is based on the name used in the expression. When standard argument names from the end of the alphabet are being used without any subscript (e.g. $x_0$) or with a special symbol as a subscript (e.g. $t^\star$), we intend the expression to be a function. However, when we want to ***apply a function*** to specific values for inputs we will write in any of the following styles:

$$f(x^\star) \ \ \mbox{or}\ \ \sqrt{\strut x^\star}\ \ \mbox{or}\ \ \sqrt{\strut x=3}\ \ \mbox{or}\ \ \left.\sqrt{\strut x}\right|_{x=3}\ \ \mbox{or}\ \ \left.g(x)\strut \right|_{x=3}$$

One of the important techniques of calculus is to take something we often think of as a number and turn it into a function whose output is a number. It will take you time to get used to the calculus notation and to be able to tell at a glance whether something is a function or a number.

Here's an illustration that will only make sense to those who have already studied some calculus:

$$\int_0^3\!\! f(x) dx\ \ \text{is a number, but }\int_0^x\!\! f(x) dx \ \ \text{is a function.}\ \ $$

In high-school math notation, it can be hard to tell if an expression is intended to be a function or a number. Careful attention to the CalcZ conventions will make it much easier to figure whether an expression resolves to a function or a number. When you first read a mathematical expression, a good first question to ask is, "What kind of thing is this? A function? A number?"

## Functions in R

Any name in the R language can refer to any sort of object. We will use the CalcZ naming conventions in our use of R, so that the ability you develop to read math notation should help reading R **and vice versa**.

R/mosaic provides additional clues to distinguish between numbers and functions.

Almost all the time we will create a function using `makeFun()`. So when you see an R expression starting as `name <- makeFun(`tilde expression`)` you know for sure the name refers to a function.

As we get deeper into calculus, you will meet additional R operators that generate functions. We'll introduce these in good time, but for someone reviewing the course, these include `D()`, `antiD()`, `compose()` and `iterate()`.

::: {.why}
Some readers may have encountered R previously in a statistics or data science course. Those readers will be wondering what this is with `makeFun()`. Experienced programmers know that the way you make functions in R is by using the `function` keyword. For instance:
```{r}
f <- function(x) { 3 + 2*x }
```

That's a completely correct and legitimate way to define a function in R and in most settings is the universal practice.

We developed `makeFun()` to handle a situation where computers, in their insistence on avoiding ambiguity, will do something that is not what the person familiar with math notation is likely to suspect. The problem comes up in something as simple as
```{r}
g <- function(x) { m*x + b }
```
In traditional math notation, $g(x) \equiv m x + b$, we are usually silent on where parameters like $m$ and $b$ are coming from. And, to be honest, you don't have to worry about this **until you try to evaluate the function**.

If you evaluate, say, the command $g(3)$, the R system knows how to find the right values for $m$ and $b$. If there are no such objects in the appropriate places in the R system, an error message will be generated. The rules that computer languages follow in tracking down symbols that aren't in the argument list are called ***scoping rules***. Scoping is an advanced programming concept and different languages use different rules. That's a recipe for trouble and confusion for newbies. (And even for experts!)

`makeFun()` is arranged to impose its own scoping system, one that is dead easy and essentially the same in most every computer language. All that `makeFun()` does is to add any parameters in the function algorithm to the argument list. The `makeFun()` command creates a function with 3 arguments.
```{r}
makeFun(m*x + b ~ x)
```
The point of the `~ x` part of the tilde expression is simply to name which arguments should come first. To evaluate the function, you'll have to provide values for `m` and `b`. But some operations on a function---differentiation and anti-differentiation, in particular---can be done without having to specify parameter values. For those of you who know what differentiation or anti-differentiation are, here is an example:
```{r}
D(m*x + b ~ x)
antiD(m*x + b ~ x)
```

Sometimes you have particular numerical values in mind for the parameters. For instance, if you are modeling the trajectory of a ball, you will undoubtedly need to make use of gravitational acceleration at the Earth's surface, which is $9.8 \text{m}/\text{s}^2$. You might prefer not to include the specific number 9.8 in your function definitions so that you can use the same functions to model a ball's trajectory on Mars. But since most balls are thrown on Earth, maybe it's not worthwhile to insist that the value 9.8 be specified every time the function is used. You can have it both ways by using `g` as the parameter name and instruct R to set `g` to 9.8 *unless otherwise specified*. The function will look like this:

```{r}
ball_velocity <- makeFun(g*t + t0 ~ t, g = 9.8, t0 = 0)
# For Earth: falling 3 seconds from a standstill
ball_velocity(3)
# For Earth when the ball has an initial upward velocity of 10 m/s
ball_velocity(3, t0 = -10)
# For Mars ...
ball_velocity(3, t0 = -10, g = 3.711)
```
:::

## E unibus plurum: $=$, $\equiv$, $\rightarrow$, `<-` {#foursigns}

The $=$ sign carries a lot of weight in high-school notation. Too much weight. It is used for several different meanings. That leads to confusion and error.

**Meaning 1**: "Is defined to be ..."  

We use $\equiv$ in mathematical notation and `<-` in R. The notation in R is a bit simpler than the mathematical notation: it is a way of giving something a name. 
```r
name <- something
```
If the "something" is a function, you will see that on the left side of `<-`, for instance by use of the `makeFun()` operator qw in `h <- makeFun(x^2 ~ x)`. The left side is simply a name.

In math notation, the equivalent would be written $h(x) \equiv x^2$. The left side isn't exactly a name. It's a name followed by parentheses in which are the names being used in the algorithm.

Keep in mind that in writing about functions, we will generally provide a hint that the name refers to the function, writing $h()$ or `h()`. The parentheses aren't part of the name; the name here is $h$. But the parentheses remind us that $h$ is a function.

**Meaning 2**: "Happens to be ..."  

The acceleration due to gravity is often given the name $g$. On Earth's surface, it happens to be $9.8 \text{m}/\text{s}^2$. In our math notation, we will use the equal sign for this narrow meaning, as in $g=9.8\text{m}/\text{s}^2$. In R we will use `=`. 

**Meaning 3**: "Gets closer and closer to ..."

Calculus is about relationships: the connection between two (or more) things. So you will hear phrases like, "As $x$ increases, $f(x)$ decreases." Or, in everyday experience, "As it gets more humid, the weather becomes more uncomfortable." Or, "slower is safer," or "the spicier the better" or "the heavier the blanket, the warmer I'll be." (One of the important uses of ***derivatives*** in calculus is to represent such statements quantitatively. But that's a subject for the next Block.)

In calculus, sometimes you have to distinguish between "$x$ is zero" and "$x$ gets closer and closer to zero." We'll need this when we want to say, "It gets smaller and smaller, but doesn't disappear entirely." The symbol for "gets closer and closer to" is $\rightarrow$, as in $x \rightarrow 0$

In reading math, take care to notice which of $=$, $\rightarrow$, or $\equiv$ is being used. The sign has something important to say and is intended to help you make sense of what you read.
:::

```{exercise, fnotation}
```
`r knitr::knit_child(exercise_file("02", "function-notation.Rmd"))`

<!--chapter:end:Fun-notation.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Graphics & function graphs {#graphs-and-graphics}

::: {.objectives data-latex=""}
```{r echo=FALSE, results="asis"}
state_objective("F-20", "Understand that a mathematical graph consists of points in a composite space: the input $\\times$ output.")
```
:::


***Scientific and statistical graphics*** are visual depictions of information and data. For displaying a function with a single input, mathematicians and other favor a particular style of graphics. This favored style is called a ***function graph***, which has a specific technical meaning in mathematics. Notice that we're writing "function graph" rather than "function graphic." A function graph is a particular sort of graphic: there are many other types of graphics some of which we'll use to display data or features of functions.

## The graphics frame

Technical graphs are usually drawn in a ***graphics frame*** 
that has several components:

1. The ***frame*** is a region on the piece of paper or computer screen that is marked off by a horizontal and a vertical axis.
2. The horizontal axis stands for one quantity.
3. The vertical axis stands for another quantity.
4. Both the horizontal and vertical axes are drawn with a ***scale*** that enables you to translate between a numerical value and position.

```{r empty-frame, echo=FALSE, fig.cap="An empty graphics frame with scales for both the horizontal and vertical axes."}
gf_blank(mpg ~ wt, data=mtcars) %>%
  gf_labs(y = "Vertical", x = "Horizontal") %>%
  gf_lims(y = c(-20,20), x=c(-5, 5))
```

## Data graphics: the point plot

One of the most common uses of a graphics frame is to display visually two columns from a table containing data. For instance, here is a small part of a table about the size of penguins in the Palmer Archipelago in Antarctica.

```{r}
library(palmerpenguins)
set.seed(101)
knitr::kable(
  penguins %>% sample_n(10) %>%
    select(species, body_mass_g, flipper_length_mm, 
           bill_length_mm) %>%
    rename(body_mass = "body_mass_g", 
           flipper_length = "flipper_length_mm",
           bill_length = "bill_length_mm") %>%
    mutate(body_mass = body_mass/1000,
           flipper_length=flipper_length/10,
           bill_length = bill_length/10)
) %>% 
    kableExtra::kable_styling()
```

A point plot displays two columns from the table. Each row in the table is represented by one point in the graphic. For instance:

```{r echo=FALSE, warning=FALSE}
gf_point(flipper_length_mm/10 ~ body_mass_g/1000, 
         data=penguins) %>%
  gf_labs(y = "Flipper length (cm)", x = "Body mass (kg)") 
```

Each penguin has a specific mass and flipper length. To look at the penguins with a mass of around 4 kg, you can see that the different penguins have a variety of flipper lengths. That's typical natural variability.

A function graph can be constructed in the same way. Start with a table reporting the output of the function for a variety of inputs, like this:

```{r echo=FALSE}
Sigmoid1 <- tibble::tibble(input = seq(-3, 3, length=11), output = pnorm(input)) 
Sigmoid1 %>%
  knitr::kable() %>% 
  kableExtra::kable_styling()
```
Then make a point plot of the two columns in the table:

```{r echo=FALSE, fig.cap="A point plot of the table."}
gf_point(output ~ input, data = Sigmoid1)
```
The graph shows the shape of the sigmoid function, one of our naked modeling functions. Except ... the domain of the sigmoid is the entire number line from $-\infty$ to $\infty$. 




::: {.todo}
** Working with data**

Instructions for accessing data in R here. 


i. data frames: `names()`, `head()`, `gf_point()`
ii. graphical layers and piping: Combining gf_point() and slice_plot() 


:::

### Asymptotes of functions

There's no need to plot out the entire domain. We know that all the "action" is happening near zero and that for inputs further to the left the function value heads toward zero. Similarly, for inputs far to the right, the function value heads toward zero. We have a particular mathematical notation for this sort of "far to the left" idea. In mathematical notation, we write the situation for the sigmoid this way:

$$\lim_{x \rightarrow -\infty} \text{dnorm}(x) = 0\ \ \ \mbox{and}\ \ \ \lim_{x \rightarrow -\infty}\text{dnorm}(x) = 1$$
This information about the function output is called the ***horizontal asymptotes*** of the function. It's expected of a mathematically literate person that they know by heart the asymptotes of the basic modeling functions, just as a geographically literate person knows Africa is a continent and Ghana is a country in Africa. We'll say more about horizontal asymptotes later. For now, note that such expected knowledge enables us to convey the shape of the function by plotting just a limited, but well chosen, part of the domain. 

The graph drawn from the data table is incomplete. The domain of the sigmoid function is the entire number line---all numbers---but the graphic shows only a handful of input values. 

It's perhaps common sense that you can find the function output for an input that isn't shown on the point plot by ***interpolating*** between the points that are shown. For instance, put a straight-line betwen consecutive points:

```{r echo=FALSE, fig.cap="Interpolating between the data points"}
gf_point(output ~ input, data = Sigmoid1) %>%
  gf_line()
```

With the gaps between points filled in by interpolation, you can use the graphic to read off the output corresponding to every input in the graphic domain. (And *vice versa*: you can find an input that corresponds to any given output in the range of the function.)

Typically a function's graph is drawn using many more points, so closely spaced that you can't see a gap. Since you can't see the gaps, you won't be misled by the sharp angles between consecutive line segments connecting the points. Like this:

```{r sigmoid-graph, echo=FALSE}
slice_plot(pnorm(x) ~ x, domain(x=c(-3, 3)))
```

A more traditional, mathematical definition of a function graph is the set of coordinate pairs which are a legitimate input/output pairs of the function. Placing ink at all such legitimate pairs displays the graph.

::: {.takenote}
In the data point plot of the penguin flipper length vs body mass, there are generally multiple penguins with the same body mass but different flipper lengths. In mathematics, part of the ***definition*** of the term "function" is that a function has ***only one output*** for any given input. Thus, the graph of a function with one input will always consist of a single ***curve***, as opposed to the ***cloud of points*** often seen in data.
:::

## Graphs of the basic functions {#function-shapes}

One helpful way to get to know the naked modeling functions individually is to graph them. People have a natural talent to remember shapes and distinguish one from another; your job is to associate each shape with it's name.

```{r echo=FALSE, out.width="50%", fig.show="hold", warning=FALSE}
slice_plot(exp(x) ~ x, domain(x=c(-5, 5))) %>%
  gf_labs(title = expression(paste("Naked exponential:   f(x) =",, e^x)))
slice_plot(sin(x) ~ x, domain(x=c(-5, 5))) %>%
  gf_labs(title = "Naked sinusoid:    f(x) = sin(x)")
slice_plot(log(x) ~ x, domain(x=c(-5, 5))) %>%
  gf_labs(title = "logarithm:   f(x) = ln(x)")
slice_plot(dnorm(x) ~ x, domain(x=c(-5,5))) %>%
  gf_labs(title = "Naked hump function:   f(x) = dnorm(x)")
slice_plot(pnorm(x) ~ x, domain(x=c(-5,5))) %>%
  gf_labs(title = "Naked sigmoid function:  f(x) = pnorm(x)")
slice_plot(x ~ x, domain(x=c(-5,5))) %>%
  gf_labs(title = "Identity function:    f(x) = x")
```
The shape of a power-law function $x^p$ depends on the exponent $p$. 

* If $p$ is not an integer, that is $p \neq \ldots,-2, -1, 0, 1, 2, 3, ...$ then the domain is $0 leq x$, the positive side of the number line.
    - For positive $p$, the function is *increasing*.
    - For negative $p$, the function is *decreasing*.
* If $p$ is an integer, then the domain is the entire number line, $-\infty < x < \inft$
    - For p even, that is $p = 2, 4, 6, ...$ the output is positive.
    - For p odd, the output is negative.

```{r power-law-graphs, echo=FALSE, out.width="50%"}
slice_plot(x^2.5 ~ x, domain(x=c(0,3))) %>%
  slice_plot(abs(x)^2.5 ~ x, domain(x=c(-3, 0)), size=2,
             color="blue", alpha =0.2) %>%
  gf_labs(title = "Power law with p > 0") %>%
  gf_text(5 ~ -2, label="When p = 2, 4, 6, 8, ...", color="blue") 
  
slice_plot(abs(x)^2.5 ~ x, domain(x=c(0,3))) %>%
  slice_plot(-abs(x)^2.5 ~ x, domain(x=c(-3, 0)), size=2,
             color="blue", alpha =0.2) %>%
  gf_labs(title = "Power law with p > 0") %>%
  gf_text(5 ~ -2, label="When p = 1, 3, 5, 7, ...", color="blue") 

slice_plot(x^-1 ~ x, domain(x=c(0.1,3))) %>%
  slice_plot(1/abs(x) ~ x, domain(x=c(-3, -0.1)), size=2, alpha=0.2, color="blue") %>%
  gf_text(5 ~ -2, label="When p = 2, 4, 6, 8, ...", color="blue") %>%
  gf_labs(title = "Power law with p < 0")

slice_plot(x^-1 ~ x, domain(x=c(0.1,3))) %>%
  slice_plot(-1/abs(x) ~ x, domain(x=c(-3, -0.1)), size=2, alpha=0.2, color="blue") %>%
  gf_text(-4.5 ~ -2, label="When p = -1, -3, -5, -7, ...", color="blue", alpha = 0.3) %>%
  gf_labs(title = "Power law with p < 0")
```

::: {.notethis}
With some practice, you should be able to sketch roughly any of the naked modeling functions from memory. When you can do this, you'll have an easier time envisioning which function might best suit a situation.

In addition to the shape, here are some facts about the naked modeling functions that will let you sketch accurately.

::: {.todo}
[OR, think of each function as being ***anchored*** at one input. ]
:::

- Exponential
    - $x=0 \implies e^x=1$
    - $x=3 \implies e^x\approx 20$
- Logarithm
    - $x=1 \implies log(x)=0$
    - $x=3 \implies e^x\approx 20$
- Hump
    - $x=0 \implies$ peak of hump: dnorm(0) $\approx 0.4$
    - $x=\pm 1 \implies$ output is $\approx 0.25$
- Sigmoid
    - $x=0 \implies$ mid-way up the hill: pnorm(0) $= 0.5$
    - $x=2 \implies$ output is $\approx 0.975$
    - $x = -2 \implies$ output is $\approx 0.025$
- Sinusoid
    - Peaks are always at 1, valleys at $-1$
    - $x=0, \pm \pi, \pm 2\pi, \pm 3\pi, \ldots$ gives output 0.
:::


## EXERCISES in draft

Show some graphs, ask which ones are mathematical functions, which ones have a unique inverse, what is the output corresponding to a given input, what is the input for a given output, what is the range of the function, .... 


## Inputs to output 

You can easily evaluate a function for a given input from its graph. As you know, just put your finger at the horizontal coordinate for the input. Then move your finger upward to reach the point on the curve directly above that horizontal coordinate. You read off the value of the function at that input by reference to the scale on the vertical axis.

It's not possible to show with a graph the whole of a function whose domain is $-\infty$ to $\infty$. Consequently, when drawing a graph we choose to show only that part of the domain that we expect will be relevant to our needs.

Sometimes, the graphic's domain includes parts that are not in the domain of the function being drawn. In such cases, the function's graph does not extend into the invalid part of the graphic domain, as in this plot of a function whose domain is only the positive numbers.

```{r echo=FALSE, warning=FALSE, message=FALSE}
slice_plot(log(x) ~ x, domain(x=c(-5, 5)), npts=500) %>%
  gf_labs(y = "Output", x = "Input") %>%
  gf_lims(y = c(-3,3), x=c(-5, 5))
```

This function has a range that runs from $-\infty$ to $\infty$, but the limits of paper and display mean that we can show only part of this range. With experience, you'll learn to read the hints in a graph that the underlying function might have a range larger than the one shown in the graphic.

## Outputs to inputs

Graphs are relatively modern, coming into mainstream use only in the 1700s. Much of mathematics was developed before graphs were invented. One consequence of this is that function tasks that are easy using a graph might be very hard with the previous ways of implementing functions. This is analogous to the way that arithmetic is pretty easy with Arabic numerals, but really hard with Roman numerals.

A function graph makes it easy to evaluate the ***function inverse***. For all the basic modeling functions we have a way to calculate numerically the output for any given input (in the function's domain). 

Often, working with a function goes another way: you know the output and you want to find a corresponding input. It's easy to do this with a graph. Pick the position on the vertical axis that represents the given input. Then trace horizontally to where the ink is. From there, trace vertically to read off the value of an input that would produce the given output.

Mathematicians are careful to distinguish between functions where there is a ***unique*** input that generates each given output, and functions where there can be more than one input that generates the same output. Functions with a one-to-one relationship between output and input are called "invertible."

Invertible or not, it is a common procedure for working with functions to find *an* input corresponding to a specific, given output. In high-school algebra, this was called "solving for $x$." A special case of solving is finding the ***roots of a polynomial***. The name we give to the procedure is ***zero finding***, which correctly points out that we are trying to **find** an input.

## Graphs of functions with two inputs

We can draw graphs of functions with two inputs. Now the points need to be marked in a 3-dimensional space: one axis for each of the two inputs and another axis for the output. Like this:

```{r echo=FALSE, fig.cap="(Figure Fun-1b-1.2)"}
f2 <- rfun( ~ input_1 + input_2, seed=932)
interactive_plot(f2(input_1, input_2) ~ input_1 + input_2,
                 domain(input_1=c(-5, 5), input_2 = c(0, 4)))
```

It is very hard to read a graph of a function with two inputs. Think of the graph as a kind of tent suspended over a domain of ground. The graph itself is a surface. To show the graph on a display, some tricks of the trade are used: color to give an additional scale for the output; computer graphics to let us rotate the surface to look at it from various perspectives, added grid lines and marks on the surface to help us read out the numerical value.

## Contour plots

::: {.objectives latex-data=""}

```{r echo=FALSE, results="asis"}
state_objective("F-21", "Interpret a contour plot of a function of two input variables, to include estimating function values and locations of peaks and valleys.")
```
:::

A mathematical graph is just one way to draw a picture of a function with two inputs. There are other ways. One helpful mode of picture is called a ***contour plot***, familiar to many non-mathematicians in the form of topgraphical maps showing landscape elevation as a function of latitude and longitude. Here's a contour plot of the same function shown in the previous graph:

```{r echo=FALSE, fig.cap="(Fig-1b-2.1)"}
f2 <- rfun( ~ input_1 + input_2, seed=932)
contour_plot(f2(input_1, input_2) ~ input_1 + input_2,
                 domain(input_1=c(-5, 5), input_2 = c(0, 4)),
             skip=0)
```

This contour plot is a topographical map of the mathematical graph in Figure Figure (Fun-1b-1.2)

It may take some practice to learn to read contour plots fluently but it is a skill that's worthwhile to have. Note that the graphics frame is the Cartesian space of the two inputs. The output is presented as ***contour lines***. Each contour line has a label giving the numerical value of the function output. Each of the input value pairs on a given contour line corresponds to an output at the level labeling that contour line. To find the output for an input pair that is *not* on a contour line, you ***interpolate*** between the contours on either side of that point.

For example, the input pair (0, 0)---which is at the bottom of the frame, midway from left to right---falls between the contours labeled "20" and "22." This means that the output corresponding to input (0, 0) is somewhere between 20 and 22. The point is much closer to the contour labeled "20", so it's reasonable to see the output value as 20.5. This is, of course, an approximation, but that's the nature of reading numbers off of graphs.

Often, the specific numerical value at a point is not of primary interest. Instead, we may be interested in how steep the function is at a point, which is indicated by the spacing between contours. When contours are closely spaced, the hillside is steep. Where contours are far apart, the hillside is not steep, perhaps even flat.

Another common task for interpreting contour plots is to locate the input pair that's at a local high point or low point: the top of a hill or the bottom of a hollow. Such points are called ***local argmax*** or ***local argmin*** respectively. The *output* of the function at a local argmax is called the ***local maximum***; similarly for a local argmin, where the output is called a ***local minimum***. (The word "argmax" is a contraction of "argument of the maximum." We will tend to use the word "input" instead of "argument", but it means exactly the same thing to say "the inputs to a function" as to says "the arguments of a function.")

Still other common tasks for reading contour plots are to start at a given input pair and figure out 1) the direction to move which is most steeply uphill, or 2) the direction to move which will keep the function output the same.

It can be helpful to look at a contour map and interpret the contours as representing geographical features: hills, valleys, crests, coves, hollows, and so on.

Then, for (Fun-1c) translate between a contour plot and a graph of a function with one input. (We can think about this as a function of two inputs, where we hold one of the inputs constant, that is, always the same. )

## Slice plots

::: {.objectives data-latex=""}
```{r echo=FALSE, results="asis"}
state_objective("F-23", "Know what is meant by a slice of function of two variables.  Be able to determine if a function is increasing, decreasing, or constant when moving horizontally or vertically on the contour plot.")
```
:::

As mentioned before, ***mathematical modeling*** is the process of constructing mathematical representations of situations or phenomena of interest. In CalcZ, we are primarily interested in using ***functions*** as such representations.

Almost always, when mathematically modeling a real-world situation or phenomenon, we do not try to capture every nuance of every relationship that might exist in the real world. We leave some things out. Such simplifications make modeling problems tractable and encourage us to identify the most important features of the most important relationships.

On the other hand, it's easy to go wrong and leave out something that's important. To mitigate this risk, many modeling projects involve a ***modeling cycle*** where we propose a candidate model, examine the consequence of that model to see if it corresponds well to the parts of reality that are important to us in our task, and modify the model as needed to produce a new and better candidate.

In this spirit, it's useful always to assume that our models are leaving something out and that a more complete model involves a function with more inputs than the present candidate. The present candidate model should be considered as a ***slice*** of a more complete model. Our slice leaves out one or more of the variables in a more complete model.

To illustrate this, suppose that we have a "more complete model" in the form of a function of two inputs, as shown in the contour plot below.

```{r (Fig-1c.1), echo=FALSE}
f2 <- rfun( ~ input_1 + input_2, seed=96)
contour_plot(f2(input_1, input_2) ~ input_1 + input_2,
                 domain(input_1=c(-5, 5), input_2 = c(0, 4)),
             skip=0)
```

As you become practiced reading contour plots, you might prefer to read this one as a hilltop (shaded yellow) side-by-side with a hollow or bowl (shaded purple), with green, almost level flanks at the left and right edges of the frame.

The most common forms of ***slice*** involve constructing a simpler function that has one input but not the other. For example, our simpler function might ignore input #2. There are different ways of collapsing the function of two inputs into a function of one input. An especially useful way in calculus is to take the two-input function and set one of the inputs to a ***constant value***.

For instance, suppose we set input #2 to the constant value 1.5. This means that we can consider any value of input #1, but input #2 has been replaced by 1.5. In Figure Fig-1c.2, we've marked in red the points in the contour plot that give the output of the simplified function.

```{r (Fig-1c.2), echo=FALSE, out.width="50%", fig.show="hold", warning=FALSE}
f2 <- rfun( ~ input_1 + input_2, seed=96)
contour_plot(f2(input_1, input_2) ~ input_1 + input_2,
                 domain(input_1=c(-5, 5), input_2 = c(0, 4)),
             skip=0) %>%
  gf_hline(yintercept=1.5, color="red", size=2) %>%
  gf_labs(title="Contour plot of function of two inputs")
slice_plot(f2(input_1, input_2=1.5) ~ input_1, domain(input_1=c(-5, 5))) %>%
  gf_labs(title="Graph of function output versus input #1")
```

Each point along the red line corresponds to a specific value of input #1. From the contours, we can read the output corresponding to each of those values of input #1. This relationship, output versus input #1 can be drawn as a mathematical graph (to the right of the contour plot). Study that graph until you can see how the rising and falling parts of the graph correspond to the contours being crossed by the red line.

Slices can be taken in any direction or even along a curved path! The blue line below the slice constructed by letting input #2 vary and holding input #1 and the constant value 0.

```{r (Fig-1c.3), echo=FALSE, out.width="50%", fig.show="hold", warning=FALSE}
contour_plot(f2(input_1, input_2) ~ input_1 + input_2,
                 domain(input_1=c(-5, 5), input_2 = c(0, 4)),
             skip=0) %>%
  gf_vline(xintercept=0, color="blue", size=2) %>%
  gf_labs(title="Contour plot of function of two inputs")
slice_plot(f2(input_1 = 0 , input_2) ~ input_2, domain(input_2=c(0, 5))) %>%
  gf_labs(title="Graph of function output versus input 2")
```

## Creating graphics

::: {.objectives}
```{r echo=FALSE, results="markup"}
state_objective("F-32-R", "Use the `slice_plot()`, `contour_plot()`, and `domain()` functions in R/`mosaic`.")
```
:::


::: {.todo}
NO CONTENT YET
:::


```{exercise}
```
`r knitr::knit_child(exercise_file("03", "drawing.Rmd"))`


<!--chapter:end:Fun-graphics.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Parameters for functions {#params-intro}

```{r include=FALSE}
library(mosaic)
library(mosaicCalc)
library(math141Z)
library(lubridate)
```

The naked modeling functions provide the modeler with a collection of shapes. They are not yet fully suited to represent real-world phenomena. To illustrate, consider Figure \@ref(fig:covid-exp) which shows the number of officially confirmed cases in March 2020. 

The outbreak was widely described as "exponential," so alongside the data Figure \@ref(fig:covid-exp) shows the function $e^x$.

```{r covid-exp, echo=FALSE, fig.cap="Cumulative confirmed COVID-19 cases during the month of March, 2020. The red curve is $e^x$", warning=FALSE}
March <- Covid_US %>% filter(month(date)==3) %>%
  mutate(day = mday(date))
gf_point(confirmed ~ day, 
         data = March) %>%
  gf_labs(y = "Cumulative confirmed cases", x = "Day in March, 2020") %>%
  gf_lims(y=c(0,200000)) %>%
  slice_plot(exp(x) ~ x, domain(x=c(0,15)), color="red", label_text = "exp(x)")
```

There's an obvious mismatch between the data and the function $e^x$. Does this mean the COVID pattern is not exponential?

A hint comes from the formula $e^x$. What is $x$? Plotted as it is in the graph, $x$ is the calender day in March. But why shouldn't $x$ be the given in hours or minutes or weeks? 

If we want the input to $\exp()$ to be in hours, we can multiply $x$ by 24. If the input is to be in weeks, the multiplier should be $\frac{1}{7} = 0.1429$. In both cases, the function will be $e^{kx}$, where $k$ would be 24 for hours or 0.1429 for weeks. Exploring a bit, we found that $0.3 \leq k < 0.5$ will produce functions graphs that match the data much better than naked $e^x$. 

```{r covid-exp2, echo=FALSE, fig.cap="COVID-19 data compared to the exponential functions $e^{kt}$. A variety of possible numerical values for $k$ is shown.", warning=FALSE}
gf_point(confirmed ~ day, data = March) %>%
  gf_labs(y = "Cumulative confirmed cases", x = "Day in March, 2020") %>%
  gf_lims(y=c(0,200000)) %>%
  slice_plot(exp(x/2) ~ x, domain(x=c(0,30)), color="green", label_text = "exp(0.5*x)", label_x=.75) %>%
  slice_plot(exp(0.4*x) ~ x, domain(x=c(0,30)), color="blue", label_text = "exp(0.4*x)", label_x=0.9) %>%
  slice_plot(exp(0.45*x) ~ x, domain(x=c(0,30)), color="orange", label_text = "exp(0.45*x)", label_x=0.9) %>%
    slice_plot(exp(0.30*x) ~ x, domain(x=c(0,35)), color="tomato", label_text = "exp(0.30*x)", label_x=0.95) %>%
  slice_plot(exp(0.20*x) ~ x, domain(x=c(0,35)), color="black", label_text = "exp(0.20*x)", label_x=0.95)
```

The multiplier $k$ in $e^{kx}$ is called a ***parameter*** of the function: a number that we can use to set the ***scale*** of the input. To use the exponential function to model COVID, we've had to stretch out the red curve in Figure \@ref(fig:covid-exp2) by clothing naked $x$ as $k x$. Perhaps it's a matter of personal choice which size of $k$ will be best suited to model the data.

My personal choice is $k=0.30$. My reasoning? The orange curve parallels the COVID data. The flaw with  $k=0.30$ is that the curve lags several the data by several days. But we can fix this by pulling the $k=0.30$ curve to the left. Mathematically this can be accomplished by subtracting a few days from $x$ before multiplying by $k$, that is, using the function $$f(x) \equiv e^{k(x-s)}$$ where $s$ stands for the shift.

```{r echo=FALSE}
mod1 <- fitModel(confirmed ~ exp(0.3*(day-s)),
                 data = March,
         start = list(s=-32))
f1 <- makeFun(mod1)
mod2 <- fitModel(confirmed ~ exp(k*(day-s)),
         data = March,
         start = list(k=0.3, s=-32) )
f2 <- makeFun(mod2)
```

Figure \@ref(fig:covid-exp3) shows the orange curve after pulling it 10 days to the left. It's now a pretty good match to the data. 


```{r covid-exp3, echo=FALSE, fig.cap="COVID-19 data compared to the exponential functions $e^{k(t-s)}$.", warning=FALSE}
gf_point(confirmed ~ day, data = March) %>%
  gf_labs(y = "Cumulative confirmed cases", x = "Day in March, 2020") %>%
  gf_lims(y=c(0,200000)) %>%
  slice_plot(f2(x) ~ x, domain(x=c(0,31)), color="green", label_text = "exp(0.19*(x+32))", label_x=.6) %>%
  slice_plot(f1(x) ~ x, domain(x=c(0,30)), color="orange", label_text = "exp(0.30*(x+10)", label_x=0.9) 
```

Note that once we've aligned the orange curve horizontally, it seems to curve too much. The green curve does much better. It has a much gentler curve, $k=0.19$ and is pulled about a little more than a month to the left.

By parameterizing the exponential function as $e^{k(x-s)}$ and finding suitable values for $k$ and $s$, we get a good match to the March data. But models can sometimes tell us more. For the green curve in Figure \@ref(fig:covid-exp3) the value of $s$ is -32 days. 32 days before March 1 is in late January. And even though we didn't have any January or February data to base the green curve on, late January 2020 is regarded as the very beginning of the outbreak.

## Parallel scales

The graphical format we have been using to display functions of one variable places the input on the horizontal axis and the output on the vertical axis. This is not the only way to draw a function. Consider these everyday objects: a thermometer and a ruler.

![](www/thermometer.png)     ![](www/ruler.jpeg)

Each object presents a read-out of what's being measured---temperature or length---on two different scales. At the same time, the objects provide a way to convert one scale to another.

A function gives the output for any given input. We represent the input value as a position on a number line---which we call an "axis"---and the output as a position on another output line, almost always drawn perpendicular to one another. But the two number lines can just as well be parallel to one another. To evaluate the function, find the input value on the input scale and read off the corresponding output. The ***inverse function*** can be evaluated just as easily: switch the roles of the input and output scales.

Taking the traditional unit scale as the input and the metric scale as the output, the two functions implemented on the objects are:
$$\underbrace{C(F) = \frac{5}{9}(F-32)}_\mbox{Fahrenheit to Celcius}\ \ \ \text{and}\ \ \ \ \underbrace{\text{cm(inches)} = 2.54 \times (\text{inches}-0}_\mbox{inches to cm})$$
These are very simple, straight-line functions, but they play an important role in modeling. 

Each conversion function can be written in the form $h(x) \equiv m (x - x_0)$. Of course, if you multiply the $m$ through both terms in parentheses, you get $h(x) = m x - m x_0$ which can be written even more simply as $mx + b$ by setting $b\equiv m x_0$. So the conversion function is simply the straight-line function.

$m$ and $x_0$ are the ***parameters*** of the straight-line function. In terms of the graph of a straight-line function, they are the slope and x-intercept respectively.

Often, functions can be parameterized in other ways. For instance, you likely learned the parameterization $m x + b$, in which $m$ is (still) the slope of the graph but $b$ is now the y-intercept.

::: {.takenote}
We can call $m(x - x_0)$ the "x-intercept parameterization" and $m x + b$ the "y-intercept parameterization. They are equivalent and equally good ways of parameterizing the straight line. There are still other ways of parameterizing, each style reflecting its own format for specifying the two points that make up a line. 
:::

To turn a naked modeling function into a ***basic modeling function*** all we do is use $h()$ to convert the input *before* applying the naked function.

Basic modeling function^[We're leaving the log function out of the list, simply because it's rarely used with $h()$.] | formula
------------------------|----------------------
Exponential             | $e^{h(x)}$
Power-law               | $\left[h(x)\right]^p$
Sinusoid                | $\sin(h(x))$
Hump                    | $\text{dnorm}(h(x))$
Sigmoid                 | $\text{pnorm}(h(x))$
Straight-line           | $h(x())$

As you can see, the straight-line function is a fundamental part of modeling. 

To illustrate the link between basic modeling functions and their naked progenitors, Figure \@ref(fig:covid-scale) shows the model we fit to the COVID-19 data:

```{r covid-scale, echo=FALSE, fig.cap="A graph of the naked modeling exponential with an additional scale displayed (red) to match it to the COVID-19 data"}
add_scale(exp(day) ~ day, domain(day=c(1,25)), 0.19, -32)
```

The function being drawn is simply $e^x$: naked. The black horizontal scale shows $x$.  To match the function to the data, that is, to show the basic modeling function, we simply add a new scale that translates $x$ to "day in March." That's the red scale. So, on March 22, there were about 25,000 COVID cases to date. 

The naked modeling function does not give a good model of the COVID case numbers. But if we scale the input before applying the naked function, we are effectively laying a new axis, stretched and shifted from the original, that let's us read off the number of cases. 

Input scaling empowers the naked modeling functions to model a huge variety of phenomena. There's just one exponential function and it always looks exactly the same. But there is a huge variety of ways to scale the input. With input scaling, the naked modeling function puts on clothes and becomes one of our basic modeling functions.
$$\underbrace{e^x}_\mbox{naked modeling function}\ \ \ \underbrace{e^{k(x-x_0)}}_\mbox{basic modeling function}$$
The straight-line function $h()$ is being written here as $k(x-x_0)$ rather than $m(x-x_0)$. It's traditional to to write some of the basic modeling functions 
You may have noticed that the above uses $k$ instead of $m$ as the multiplier in the straight-line function in the exponent. Of course, you can use whatever name you wish for a parameter. The idiom of mathematical notation has several conventions. Knowing these will help you read mathematics more fluently.

The table shows a few of them. Often, there are multiple parameterizations.

Function    | Written form | Parameter 1 | Parameter 2
------------|--------------|-------------|-------------
Exponential | $e^{kt}$     | $k$ "exponential constant"^[] | Not used
Exponential | $e^{t/\tau}$     | $\tau$ "time constant"^[] | Not used
Exponential | $2^{t/\tau_2}$     | $\tau_2$ "doubling time"^[$-\tau_2$ is sometimes called the "half life."] | Not used    
Power-law   | $[x - x_0]^p$    | $x_0$ "center" | Not used
Sinusoid    | $\sin\left(\frac{2 \pi}{P} (t-t_0)\right)$ | $P$ "period" | $t_0$ "time shift" 
Sinusoid | $\sin(\omega t + \phi)$ | $\omega$ "angular frequency" | $\phi$ "phase shift"
Sinusoid | $\sin(2 \pi \omega t + \phi)$ | $\omega$ "frequency" | $\phi$ "phase shift"
Hump     | dnorm(x, mn, sd) | mn "mean" | sd "standard deviation"
Sigmoid  | pnorm(x, mn, sd) | mn "mean" | sd "standard deviation"
Straight-line | $mx + b$ | $m$ "slope" | $b$ "y-intercept"
Straight-line | $m (x-x_0)$ | $m$ "slope" | $x_0$ "center"

## Scale the output

Just as the natural input usually needs to be scaled before it reaches the naked modeling function, so the output from the naked function may need to be scaled before it presents a result suited for interpreting in the real world. 

```{r scaling-nature, echo=FALSE, out.width="100%", fig.cap="Natural **quantities** must be scaled to pure numbers before being suited to the naked modeling functions. The output is a pure number which is scaled to the natural **quantity** of interest."}
knitr::include_graphics("www/scaling-nature.png")
```

The overall result of input and output scaling is a smartly dressed modeling function ready to engage the real world.


Name        |  Naked form | Dressed for action
------------|-------------|------------------
exponential |  $e^x$      | $A e^{kx} + C$
sinusoid    | $\sin(x)$   | $A \sin(\frac{2 \pi}{P} x) + C$
straight-line | $a x + b$ | $a x + b$

::: {.takenote}
The parameter $C$ is often called the ***baseline*** or the ***offset***. Statisticians call it the "intercept," because it plays the same role as $b$ in the straight-line function.

When working with sinusoids, parameter $A$ is called the ***amplitude***.

Of course, you're already familiar with $a$ and $b$: the slope and intercept of a straight line.

```{r show-sin-params, echo=FALSE, warning=FALSE, fig.cap="Baseline (blue), amplitude (red), and period (green) for the sinusoid."}
# making waveforms for the figure
slice_plot(sin(x) ~ x, domain(x=c(-1, 12))) %>%
  gf_hline(yintercept=0, color="blue") %>%
  gf_refine(theme_void()) %>%
  gf_errorbar(0 + 1 ~ pi/2, color="red") %>%
  gf_errorbar(-1 + 0 ~ 3*pi/2, color="red") +
  geom_errorbarh(aes(y=1/2, xmin=0, xmax=2*pi), color="green", size=.5)
```

```{r show-exp-baseline, echo=FALSE, warning=FALSE, fig.cap="The baseline for the exponential is the horizontal asymptote."}
slice_plot(exp(x) ~ x, domain(x=c(-2, 2))) %>%
  gf_hline(yintercept=0, color = "blue") %>%
  gf_refine(theme_void())
```
:::

::: {.why}
The straight-line function is like a penguin: even when it's naked, it's still fully dressed!

Now we can let you in on a little secret. All along, the straight-line function has been dressed, not naked. The actual naked form is so simple that it can be confusing:

$$\text{identity}(x) \equiv x$$
Clothing the identity function by scaling looks like this:

$$ x \underbrace{\longrightarrow}_\mbox{input scaling} k x + c \underbrace{\longrightarrow}_\mbox{output scaling} A(kx + c) + C$$
A little bit of algebra transforms the scaled function into a more concise form:

$$A(kx + c) + C \longrightarrow \underbrace{Ak}_ax + \underbrace{Ac + C}_b \longrightarrow ax + b$$
Most students have thoroughly explored the straight-line function in their high-school studies, so we thought it ill-advised to start out with the identity function.
:::



## Exercises {.unnumbered}

Exercise 1: Each of the graphs shows two horizontal scales and one of the basic modeling functions. Which horizontal scale (black or red) corresponds to the naked modeling function? 

```{r}
add_scale(dnorm(x) ~ x, domain(x=c(-3,3)), 0.25, 10) %>%
  gf_labs(y="output", x="input", title="Graph (A)")
  
```

```{r}
add_scale(pnorm(x, -3, .5) ~ x, domain(x=c(-6,0)), .5, -3) %>%
  gf_labs(y="output", x="input", title="Graph (B)")
  
```

```{r}
add_scale(sin(2*pi*x/3+1) ~ x, domain(x=c(-2,10)), (2*pi)/3, 16/(2*pi)) %>%
  gf_labs(y="output", x="input", title="Graph (C)")
  
```

Exercise 2: Find the straight-line function that will give the value on the red scale for each point on the black scale.

```{r}
add_scale(exp(x) ~ x, domain(x=c(-2,2)), 3, 0)
add_scale(pnorm(x) ~ x, domain(x=c(-3, 3)), -1.5, 0)
add_scale(dnorm(x) ~ x, domain(x=c(-3, 3)), 2, -1)
add_scale(pnorm(x) ~ x, domain(x=c(-3, 3)), 1.5, 2)
```

Exercise 3

Each graph shows a basic modeling function written in this style: $\sin(r(t-t_0))$. Your job is to estimate $t_0$ and $r$. 

Exercise 4

Find the scaling function that translates

<!--chapter:end:Fun-parameterization.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Process of modeling

Seen very abstractly, a mathematical model, as we are using the term, is a set of ***functions*** that represent the relationships between inputs and outputs.

At the most simple level, building a model can be a short process:

1. Develop an understanding of the relationship you want to model. Often, part of this "understanding" is the pattern seen in data.
2. Choose a function type---e.g. exponential, sinusoidal, sigmoid---that you think would be a good match to the relationship.
3. Find parameters that scales your function to be able to accept real-world inputs and generate real-world outputs.

It's important to distinguish between two basic types of model:

1. ***Empirical models*** which are rooted in ***observation*** and ***data***.
2. ***Mechanistic models*** such as those created by applying fundamental laws of physics, chemistry, and such.

We are going to put off mechanistic models for a while, for two reasons. First, the "fundamental laws of physics, chemistry, and such" are often expressed with the concepts and methods of calculus. We are heading there, but at this point you don't yet know the core concepts and methods of calculus. Second, most students don't make a careful study of the "fundamental laws of physics, chemistry, and such" until *after* they have studied calculus. So examples of mechanistic models will be a bit hollow at this point.

The process of constructing a model that is a good match for data is called ***curve fitting***, or, more generally, ***fitting a model***.

## Variations from scaling

A good place to start building a model is to pick one of the basic modeling functions. This works surprisingly often. To remind you, here are our seven basic modeling functions: 

- straight-line
- exponential
- power-law
- logarithm
- sinusoid
- hump
- sigmoid

It helps in making the selection to have ready to mind the basic shape of each of these function families. To review, revisit Section \@ref(function-shapes).

Remember also that, in general, we scale the inputs and scale the output. This means that in choose a model family, we don't have to worry at first about the numbers on the axes. (Of course, those numbers will be critically important later on in the process!) The scaling does, however, allow us to consider some variations on the shapes of the modeling functions. In particular, the ***input scaling*** lets us flip the shape right-for-left. And the ***output scaling*** lets us flip the shape top-for-bottom.

- $f(t)$, basic shape
- $f(-t)$, flipped right-for-left
- $-f(t)$, flipped top-for-bottom
- $-f(-t)$, flipped both top-for-bottom and right-for-left

For functions such as the sinusoid, flipping is not much use, since the flipped sinusoid curve is still a sinusoid. Similarly, a right-for-left flipped hump function has the same shape as the original. For the straight-line function, flipping of either sort accomplishes the same thing: changing the sign of the slope. 

For the exponential function, the two possible types of flipping---right-for-left and top-for-bottom---produce four different curves, all of which are exponential, shown in Figure \@ref(fig:four-variations).

```{r four-variations, echo=FALSE, fig.cap="Four variations of the exponential functions."}
slice_plot(exp(x) ~ x, domain(x=c(-2,2))) %>%
  gf_refine(theme_void()) %>%
  gf_labs(title="A. The exponential function, no flipping")
slice_plot(exp(-x) ~ x, domain(x=c(-2,2))) %>%
  gf_refine(theme_void()) %>%
  gf_labs(title="B. Flipped right-for-left")
slice_plot(-exp(x) ~ x, domain(x=c(-2,2))) %>%
  gf_refine(theme_void()) %>%
  gf_labs(title="C. Flipped top-for-bottom")
slice_plot(-exp(-x) ~ x, domain(x=c(-2,2))) %>%
  gf_refine(theme_void()) %>%
  gf_labs(title="D. Flipped both right-for-left and top-for-bottom")

```

## Curve fitting an exponential function

Figure \@ref(fig:Fun-4-intro-1) shows some data collected by Prof. Stan Wagon to support his making a detailed mechanistic model of an everyday phenomenon: [The cooling of a mug of hot beverage to room temperature](https://www.researchgate.net/profile/Gianluca_Argentini/post/Is_analogy_reasoning_between_heat_transfert_and_electriocity_allows_to_apply_the_electricity_laws_about_resistance_to_thermal_resistances/attachment/59d6573379197b80779ada64/AS%3A533325451403264%401504166104259/download/Stan+WAGON+How+quickly+does+water+cool.pdf). The mug started at room temperature, which was measured at 26 degrees C. At time 0 he poured in boiling water from a kettle and measured the temperature of the water over the next few hours.

```{r Fun-4-intro-1, echo=FALSE, fig.cap="Stan's data"}
set.seed(101)
gf_point(temp ~ time, data=CoolingWater %>% sample_n(20)) %>%
  gf_labs(x = "Time (minutes)", y="Temperature (deg. C)")
```

Our task is to translate this data into the form of a function that takes time as input and returns temperature as output. Such a model would be useful for, say, filling in the gaps of the data. For instance, we might want to find the temperature of the water immediately after being poured from the kettle into the mug.



Looking at the data, one sees that the temperature decreases along a curve: cooling fast at first and then more slowly. This is the pattern of the flipped right-for-left exponential. (Figure \@ref(fig:four-variations)(B)) We can imagine then that an exponential, $A e^{kt} + C$ will be a suitable model form for the cooling water.

What remains is to find the parameters $A$, $k$, and $C$.  Here is a general process for curve-fitting an exponential. Later, we'll apply this process specifically to the water-cooling situation. 

**General process for curve-fitting an exponential**

**Step 0**: Check that the data show an exponential pattern in one of its variations, namely a smooth increase or decrease and leveling out beyond some value of $t$. If this isn't true, reconsider whether you should be using an exponential function in the first place.

**Step 1** Do the data show exponential growth or exponential decay? If it's exponential growth, then the flat region in Step 0 will be to the left and $k$ will be positive. If exponential decay, the flat region will be to the right and $k$ will be negative.

Notice that the question of "growth or decay" depends only on the sign of the parameter $k$. You can have an exponentially decaying process that's increasing. Consider, for instance, the speed of a car as it merges onto a highway from a slow speed on the entrance ramp.  The car's velocity is increasing, but as you approach highway speed the rate of increase gets smaller. That's exponential decay.

**Step 2** Where is the baseline? We're going to put aside $k$ for the moment and find the value of the output that is being approached asymptotically, that is, the height of the level zone of the data. This height is the coefficient $A$ in the linear combination.

**Step 3** Once you know the baseline, you're set to find a numerical value for the parameter $k$. 

i. Pick a point in the data far from the baseline. Call the input $t_1$. 
ii. Scan forward or backward in time to find a point in the data that's vertically half way from the original point toward the baseline. Call the input at that point $t_{1/2}$. The difference between these, $t_1 - t_{1/2}$ is called the ***half-life*** or ***halving-time*** if it's negative and the ***doubling time*** if it's positive. 
iii. The parameter $k$ is $0.693 / (t_1 - t_{1/2})$. Double check the sign of $k$ to make sure it's consistent with what you saw in Step 1. (Incidently, $0.693 = \ln(2)$. The 2 is the same as the 2 in doubling or halving.)

**Step 4** Now that you have numerical values for the baseline $A$ and the parameter $k$, calculating the value of $B$ is straightforward.  
    i. Pick a $t_0$ that's reasonably well represented in your data. Find the vertical coordinate represented by the data near that $t_0$. Call that ${\cal D}$. 
    ii. Solve with respect to $B$ the equation $A + B e^{k t_0} = {\cal D}$. Things are particularly easy if you can use $t_0 = 0$. Then you just straight off calculate $B = {\cal D} - A$.

**Step 5** Plot the function $A + B e^{k t}$ using the values for $A$, $B$, and $t$ that you just found. If you are not satisfied, tweak the parameters a bit until you are.

**Exponential curve fitting applied to the water-cooling data**

Let's illustrate the general process on the water-cooling data, redrawn in Figure \@ref(fig:Fun-4-a-2-1).

```{r Fun-4-a-2-1, echo=FALSE, fig.cap="The cooling-water data, repeated here for convenience."}
set.seed(101)
gf_point(temp ~ time, data=CoolingWater %>% sample_n(20)) %>%
  gf_lims(y = c(20, NA))
```

Step 0: The data indicate a smooth curve. As $t$ gets large, the curve approaches a constant. So an exponential model is reasonable.

Step 1: The flat zone of the data is to the right. So we've got exponential decay and $k < 0$. 

Step 2: The curve looks like it's leveling out at a temperature of about 25 degrees C for large $t$. So $A \approx 25^{\circ} \text{C}$.

Step 3: 

i. The point furthest from the baseline is located at $t_1 = 15 \text{sec}$ with a value ${\cal D} \approx 80^\circ\text{C}$. 
ii. This if $55^\circ\text{C}$ from the baseline. We want to find the value of $t_1$ where the temperature will be half way from 80 to the baseline. That's a temperature of about $80 - 55/2 = 53.5$. Scanning over to the right, the function that I can imagine going through the data crossed $53^\circ$ at about $t_{1/2} = 40$. Thus, the half-life is estimated at 25s. 
iii. The parameter $k$ is therefore $k\approx $0.693 / \mbox{half-life}) = - 0.63 / 25 = -0.0277$. Since we identified in Step 1 that exponential decay is involved, we expect $k$ to be negative. It is.

Step 4. 

i. We know $A \approx 25$ and $k \approx -0.0277$. We also now that for $t=15$ the function output is about ${\cal D} = 80^\circ$. 
ii. This means $25^\circ + B e^{- 0.0277 \times 15} \approx 80^\circ = 25 + 0.66 B$. Solving for $B$ gives $B = (80 - 25)/0.66 = 83.3$

Step 5. Graph the function over the data.

```{r Fun-4-a-2-2, echo=FALSE, fig.cap="An exponential function that roughly aligns with the data."}
set.seed(101)
gf_point(temp ~ time, data=CoolingWater %>% sample_n(20)) %>%
  gf_lims(y = c(20, NA)) %>%
  slice_plot(25 + 83.3*exp(-.0277*time) ~ time, color="blue")
```

## Curve fitting a periodic function

## Modeling periodicity

::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Fun-7a", "Know how the parameter values in a sinusoidal function affect the amplitude, period, phase shift, and vertical shift (table page 185)")
state_objective("Fun-7b", "Given a sinusoidal graph, be able to estimate the amplitude, period, phase shift, and vertical shift (table page 185)")
state_objective("Fun-7c", "Recognize when a sinusoidal model is appropriate.")
```
:::

Figure \@ref(fig:Fun-4-a-3) shows the tide level in [Providence, Rhode Island](https://www.google.com/maps/place/Providence,+RI/@41.8071,-71.4012,14z/data=!4m5!3m4!1s0x89e444e0437e735d:0x69df7c4d48b3b627!8m2!3d41.8071!4d-71.4012), starting at midnight on April 1, 2020 and recorded every minute for four and a half days. (These data were collected by the US National Oceanic and Atmospheric Administration. [Link](https://tidesandcurrents.noaa.gov/api/datagetter?begin_date=20200401%2010:00&end_date=20200405%201:59&station=8454000&product=one_minute_water_level&datum=mllw&units=metric&time_zone=gmt&application=web_services&format=csv))

```{r Fun-4-a-3-1, echo=FALSE, fig.cap="About four days of tide-level data from Providence, Rhode Island"}
#gf_line(level ~ hour, data = math141Z::Anchorage_tide[1:1000,])
gf_line(level ~ hour, data = math141Z::RI_tide) %>%
  gf_labs(y="Water level (meters)", x="Time (hours)", title="Tide level in Providence, Rhode Island") %>%
  gf_lims(y =c(0, 2))
```

It's not too hard to see what's going on. The tide rises and falls about every 12 hours. The difference between high tide and low tide is a little more than one meter. The tide gauge is calibrated so that a typical reading is 1 meter, although we don't know what that is respect to. (Certainly not sea level, since then the typical reading would be around zero.)

This simple description tells almost everything needed to construct an A/B model of the tide level using a sinusoid, that is, a function of the form $$A + B \sin(2\pi t/P)$$ The procedure is straightforward:

**Step 0**: Determine whether a sinusoid model is appropriate. As you know, sinusoids oscillate up and down repeatedly with a steady ***period***. That certainly seems the case here. But sinusoids are also steady in the ***peak*** and ***trough*** values for each cycle. That's only approximately true in the Providence data. Models inevitably involve approximation. We'll have to keep an eye on whether the ***fixed amplitude*** feature of sinusoids 

**Step 1**: Choose a sensible value to represent the low point repeatedly reached. 0.5 m seems appropriate here, but obviously the exact position of the trough of each cycle varies over the 4.5 day duration of the data.  Similarly, the peak is near 1.6 m. Parameter $A$ is the mean of the peak and trough values: $\frac{1.6 + 0.5}{2} = 1.05$ m here. Parameter $B$ is half the difference between the peak and trough values: $\frac{1.6 - 0.5}{2} = 0.55$. Parameter $A$ is called the ***baseline*** of the sinusoid. Paramter $B$ is the ***amplitude***. (Note that by convention, the amplitude is always *half* the high-to-low range of the sinusoid.)

**Step 2**: Estimate the period $P$ of the sinusoid. This can be done with a ruler: the horizontal duration of a complete cycle. I like to use the time between peaks, but the time between troughs would work just as well. Another good choice is the time between positive sloping crossings of the baseline. (But be careful. The time between *successive* baseline crossings, one positive sloping and the other negative, give just *half* the period.) 

On the scale of the above plot, it's hard to read off the time of the first peak. So, zoom in until it becomes more obvious.

```{r Fun-4-a-3-2, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="Zooming in on the start of the data (left) and on the last part of the data (right)."}
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour < 25)) %>%
  gf_labs(y="Water level (meters)", x="Time (hours)",
          title="Start of record") %>%
  gf_lims(y =c(0, 2))
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour > 90)) %>%
  gf_labs(y="Water level (meters)", x="Time (hours)",
          title = "End of record") %>%
  gf_lims(y =c(0, 2))
```

The left panel in Figure \@ref(fig:Fun-4-a-3-2) shows about 24 hours at the start of the record. The first peak is at about 7 hours, the second at about 18 hours. That indicates that the period is 18 - 7 = 11 hours. 

**Step 3** Plot out the model over the data. Replacing the symbols $A$, $B$, and $P$ with our estimates, the model is 

$$\require{color}
{\color{green}\mbox{tide}(t) \equiv 1.05 + 0.55 \sin(2\pi t/11)}$$

Figure \@ref(fig:Fun-4-a-3-3) shows this model in $\color{green}\mbox{green}$.

```{r Fun-4-a-3-3, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="The sinusoid fails to align with the timing of peaks and troughs."}
mod1 <- makeFun(1.05 + 0.55*sin(2*pi*hour/11) ~ hour)
mod2 <- makeFun(1.05 + 0.55*sin(2*pi*(hour-17)/11) ~ hour)
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour < 25)) %>%
  slice_plot(mod1(hour) ~ hour, color="green") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)") %>%
  gf_lims(y =c(0, 2))
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour > 90)) %>%
  slice_plot(mod1(hour) ~ hour, color="green") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)") %>%
  gf_lims(y =c(0, 2))
```


## The modeling cycle

Need intro explaining cycle


### Example: Cooling water

It looks like our estimate of the half-life is a bit too small; the data doesn't seem to decay at the rate implied by $k = -0.0277$. Perhaps we should try $k = -0.2$ and go on from there.

EXERCISE: Have them try $k=0.02$ and iterate until they get something they like.

Later in CalcZ, we'll study ***optimization***. There are optimization techniques for directing the computer to refine the parameters to best match the data. Just to illustrate, here's what we get:

```{r echo=FALSE}
set.seed(101)
Stans_data <- CoolingWater %>% sample_n(20)
```

```{r Fun-4-a-2-3, fig.cap="Polishing the fit using the rough model as a starting point."}
refined_params <- 
  fitModel(temp ~ A + B*exp(k*time), data = Stans_data, 
           start = list(A = 25, B = 83.3, k = -0.0277))
coef(refined_params)
new_f <- makeFun(refined_params)
gf_point(temp ~ time, data = Stans_data) %>%
  slice_plot(new_f(time) ~ time, color="blue")
```

The refined parameters give a much better fit to the data than our original rough estimates by eye. 

We had two rounds of the ***modeling cycle***. First, choice of an A/B expontential model and a rough estimate of the parameters A, B, and $k$. Second, refinement of those parameters using the computer.

Looking at the results of the second round, the experienced modeler can see some disturbing discrepancies. First, the estimated baseline appears to be too high. Related, the initial decay of the model function doesn't seem to be fast enough and the decay of the model function for large $t$ appears to be too slow. Prof. Stan Wagon noticed this. He used additional data to fill in the gaps for small $t$ and refined his model further by changing the basis functions in the linear combination. He hypothesized that there are at least two different cooling processes. First, the newly poured water raises the temperature of the mug itself. Since the water and mug are in direct contact, this is a fast process. Then, the complete water/mug unit comes slowly into equilibrium with the room temperature.

The newly refined model was a even better match to the data. But nothing's perfect and Prof. Wagon saw an opportunity for additional refinement based on the idea that there is a third physical mechanism of cooling: evaporation from the surface of the hot water. Prof. Wagon's additional circuits of the modeling cycle were appropriate to his purpose, which was to develop a detailed understanding of the process of cooling. For other purposes, such as demonstrating the appropriateness of an exponential process or interpolating between the data points, earlier cycles might have sufficed.

Here's a graph of the model Prof. Wagon constructed to match the data.

```{r Fun-4-a-2-4, echo=FALSE, out.width="70%", fig.align="center", fig.cap="A model that combines three exponentials provides an excellent fit."}
knitr::include_graphics(normalizePath("www/Wagon-water-fit.png"))
```

This is an excellent match to the data. But ... matching the data isn't always the only goal of modeling. Prof. Wagon wanted to make sure the model was physically plausible. And looking at the refined parameters, which include two exponential processes with parameters $k_1$ and $k_2$, he saw something wrong:

> *But what can we make of $k_1$, whose [positive value] violates the laws of thermodynamics by suggesting that the water gets hotter by virtue of its presence in the cool air? The most likely problem is that our simple model (the proportionality assumption) is not adequate near the boiling point. There are many complicated factors that affect heat transportation, such as air movement, boundary layer dissipation, and diffusion, and our use of a single linear relationship appears to be inadequate. In the next section [of our paper] we suggest some further experiments, but we also hope that our experiments might inspire readers to come up with a better mathematical model.*

The modeling cycle can go round and round!

### Example: The tides

**Step 4**: Evaluate and refine. The green model would make poor predictions. The model says "high tide" when the data say otherwise. What's missing is the ***phase*** of the sinusoid. A model that incorporates the phase is 

$${\color{blue}\mbox{tide}(t) \equiv 1.05 + 0.55 \sin(2\pi (t - t_0)/11)}$$

The new parameter, $t_0$, should be set to be the time of a positive-going crossing of the baseline. There's such a crossing at about time = 17. Happily, changing the phase does not itself necessitate re-estimating the other parameters: baseline, amplitude, period. This model, incorporating the phase, has been graphed in $\color{blue}\mbox{blue}$.

```{r Fun-4-a-3-4, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="Shifting the **phase** of the sinusoid gives the flexibility needed to align the peaks and troughs of the model with the data. Performing this alignment for one peak makes it clear that the period is wrong."}
mod1 <- makeFun(1.05 + 0.55*sin(2*pi*hour/11) ~ hour)
mod2 <- makeFun(1.05 + 0.55*sin(2*pi*(hour-17)/11) ~ hour)
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour < 25)) %>%
  slice_plot(mod2(hour) ~ hour, color="blue") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)") %>%
  gf_lims(y =c(0, 2))
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour > 90)) %>%
  slice_plot(mod2(hour) ~ hour, color="blue") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)")%>%
  gf_lims(y =c(0, 2))
```

For some modeling purposes, such as prediction of future tides, the phase information is impossible. For others, say, description of the amplitude of the tides, not. But getting the phase roughly right can help point out other problems. For instance, having the blue sinusoid for comparison makes it clear that the estimated period of 11 hours is too short. Maybe 13 hours would be better. Better still, since at $t=t_0 = 17$ the model is a close match to the data, let's use that as the estimate of the start of a cycle. But then, let's move much further along in the data to find another baseline crossing. To judge from the right panel, there is a baseline crossing at $t=103$. The difference between these two times is $103 - 17 = 86$ hours. Of course, the period is not 86 hours. Looking back at the whole set of data we can see 7 complete cycles between $t=17$ and $t=103$. So our new estimate of the period is $86/7 = 12.3$ hours.

With this refinement the model is
$${\color{violet}\mbox{tide}(t) \equiv 1.05 + 0.55 \sin(2\pi (t - 17)/12.3)}$$

```{r Fun-4-a-3-5, echo=FALSE, fig.cap="With the phase about right, a better estimate can be made of the period: 12.3 hours."}
mod3 <- makeFun(1.05 + 0.55*sin(2*pi*(hour-17)/12.3) ~ hour)
gf_line(level ~ hour, data = math141Z::RI_tide) %>%
  slice_plot(mod3(hour) ~ hour, color="violet") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)", title="Period 12.3 hours") %>%
  gf_lims(y =c(0, 2))
```
That's a pretty good match to the data! We might call it quits at that. First, let's polish up the parameter estimates, letting the computer do the tedious work of trying little tweaks to see if it can improve the fit.

```{r}
tide_mod <- 
  fitModel(level ~ A + B*sin(2*pi*(hour-hzero)/P),
  data = RI_tide,
  start=list(A=1.05, B=0.55, hzero=17, P=12.3))
coef(tide_mod)
```
```{r Fun-4-a-3-6, echo=FALSE, fig.cap="Polishing the parameters of the sinusoid"}
mod4 <- makeFun(tide_mod)
gf_line(level ~ hour, data = math141Z::RI_tide) %>%
  slice_plot(mod4(hour) ~ hour, color="red", npts=200) %>%
  gf_labs(y="Water level (meters)", x="Time (hours)", title="After polishing by the computer") %>%
  gf_lims(y =c(0, 2))
```
This last model seems capable of making reasonable predictions, so if we collected up-to-date data we might be able to fit a new model to predict the tide level pretty accurately a few days ahead of time. Also, the excellent alignment of the model peaks with the data tell us that the cyclic tide has a period that constant, at least so far as well can tell. 

With the period estimate $P=12.56$ hours, we can go looking for other phenomena that might account for the tides. The period of the day-night cycle is, of course 24 hours. So the tides in Providence come in and out twice a day. But not exactly. Something else must be going on. 

Isaac Newton was the first to propose that the tides were caused by the gravitational attraction of the Moon. A complete cycle of the Moon---moon rise to moon rise---takes about 50 minutes longer than a full day: the Earth revolves once every 24 hours, but in that time the Moon has moved a bit further on in its orbit of the Earth. So the Moon's period, seen from a fixed place on Earth is about 24.8 hours. Half of this, 12.4 hours, is awfully close to our estimate of the tidal period: 12.56 hours. The difference in periods, 8 minutes a day, might be hard to observe over only 4 days. Maybe with more data we'd get a better match between the tides and the moon. 

This is the modeling cycle at work: Propose a model form (A/B model with sinusoid), adjust parameters to match what we know (the Providence tide record), compare the model to the data, observe discrepancies, propose a refined model. You can stop the model when it is giving you what you need. The period 12.56 hour model seems good enough to make a prediction of the tide level a few days ahead, and is certainly better than the "two tides a day" model. But our model is not yet able to implicate precisely the Moon's orbit in in tidal oscillations.

Discrepancies between a model and data play two roles: they help us decide if the model is fit for the purpose we have in mind and they can point the way to improved models. That the tidal data deviates from the steady amplitude of our model can be a clue for where to look next. It's not always obvious where this will lead. 

Historically, careful analysis of tides led to a highly detailed, highly accurate model: a linear combination of sinusoids with diurnal periods 12.42, 12.00, 12.66, and 11.97 hours as well components with period 23.93, 25.82, 24.07, 26.87, and 24.00 hours. A tide-prediction model is constructed by finding the coefficients of the linear combination; these differ from locale to locale.


<!--chapter:end:Fun-modeling.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Slope function {#fun-slopes}

For our purposes, the ***definition of calculus*** is

> *The use of functions to model and explore continuous change*

The agenda of this chapter is to give specific mathematical meaning to the word "change."

## Change and slope

You have an solid, intuitive sense of what "change" means. In mathematics, and especially the mathematics of functions, change has a very simple meaning that you have already touched on in your previous math education.

The word that encapsulates "change" in high-school math is ***slope***. For instance, you've undoubtedly had to calculate the slope of a straight line in a graph. You learned about "rise" and "run" and how to read them from a graph or from a formula. The slope is the ratio: rise over run. 

Slope is a lovely ***metaphor*** for change, since everyone has a intuitive sense of the slope of a road or of a hillside. You learned to apply this intuition to reading graphs and the slope of a line. We'll exploit the intuitive ability to read a landscape in order to introduce abstract mathematical ideas in a down-to-earth setting. It's a very effective pedagogical strategy.

But not everything that changes has a "slope." For instance, the population of a country can change, as can the number of new cases of an epidemic disease, the temperature of a cup of coffee, or the distance from Earth of a spacecraft. A major part of learning calculus is generalizing and abstracting the mathematical concept of which "slope" is an example and becoming proficient with mathematical procedures for working with change. 

## Continuous change

Most people are comfortable with the ideas of daily changes in temperature or monthly changes in credit-card debt or quarterly changes in the unemployment rate or annual changes in the height of a child. Such things are easy to record in, say, a spreadsheet. For example, as I write, the weather forecast for the next several days (in southeastern Colorado in mid-May) is

Day | High | Low | Description
----|------|-----|------------
Thursday | 73 | 43 | sunny
Friday   | 72 | 48 | windy
Saturday | 66 | 48 | thunderstorms
Sunday   | 68 | 43 | windy
Monday   | 70 | 39 | sunny
Tuesday  | 70 | 43 | sunny
Wednesday| 66 | 45 | partly cloudy

Such data is said to be ***discrete***. The day is listed, but not the time of day. The high temperature is forecast, but not the time of that high. The "description" is also discrete, one of the several words that are used to summarize the quality of the weather, as opposed to the quantity of rain. 

Calculus is about ***continuous change***. For instance, if the weather bureau provide a web interface that let me enter the date and time to the nearest fraction of a second, they would be giving a way to track the change ***continuously***. Many physical processes are intrinsically continuous, for instance the motion (change in position) of a spacecraft or the height of the tide or the stress on a tree as a function of wind velocity.

Finding a language to describe continuous change---famously, the position of the moon or planets in their orbit, or the speed of a ball rolling down a ramp---was central to the emergence of what historians call the "Age of Enlightenment" or "modern scientific method." The first complete presentation of that language was published by Isaac Newton based on his work in the 1660s. As you might guess, the name of the language is "calculus."

## Slope

You already know pretty much everything there is to know about the straight-line function,

- **Formula**: $f(x) \equiv a x + b$. The parameters $a$ and $b$ are the "slope" and "intercept" respectively. (More precisely, $b$ is the "y-intercept." But in statistics and modeling, it's just the "intercept.")
- **Reading parameters from a graph**: You learned several ways to do this which are all equivalent. Maybe the easiest is the read the y-intercept off the graph. That's $b$. Then choose some non-zero $x_1$ and read off from the graph the value of $f(x_1)$. The slope is simply $$\frac{f(x_1) - b}{x_0}$$

    The ***y-intercept method*** is a special case of a more general method, the ***two-point method***, that you can use even if the y-intercept isn't shown on the graph. Pick two specific values of $x$, which we'll call $x_0$ and $x_1$. Evalate the function at these input values and compute the rise over run: $$\mbox{rise over run} \equiv \frac{f(x_1) - f(x_0)}{x_1 - x_0}$$
The rise over run is the slope of the straight line.

    The y-intercept method is exactly the same as the two-point method with $x_0 = 0$.
    
- **Matching a straight-line function to data**: You might not have been taught this formally, but the basic process is easy to imitate. The process is called ***line fitting*** or, in statistics and other fields, ***linear regression***.

## The fitted line

::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Fun-4-a-1", "Construct a straight-line model that fits data.")
```
:::



To illustrate line fitting, let's return to the cooling mug of water. Figure \@ref(fig:Fun-4-a) shows the data along with a dozen candidate straight line functions, each one drawn in its own color.

```{r Fun-4-a, echo=FALSE, warning=FALSE, fig.cap="Some candidate straight-line function models plotted on top of the cooling water data. Which one(s) would you pick as good matches to the data?"}
set.seed(101)
gf_point(temp ~ time, data=CoolingWater %>% sample_n(20)) %>%
  gf_labs(x = "Time (minutes)", y="Temperature (deg. C)") %>%
  gf_lm(temp ~ time, data = CoolingWater %>% sample_n(20), color="blue") %>%
  gf_lm(temp ~ time, data = CoolingWater %>% sample_n(20), color="aquamarine") %>%
  gf_lm(temp ~ time, data = CoolingWater %>% sample_n(20), color="cadetblue") %>%
  gf_lm(temp ~ time, data = CoolingWater %>% sample_n(20), color="green") %>%
  gf_lm(temp ~ time, data = CoolingWater %>% sample_n(20), color="cadetblue") %>%
  gf_lm(temp ~ time, data = CoolingWater %>% sample_n(20), color="royalblue") %>%
  gf_lm(temp ~ time, data = CoolingWater %>% sample_n(20),color="seagreen") %>%
  gf_abline(intercept = 100, slope=-0.21, color = "plum") %>%
  gf_abline(intercept = 90, slope=-0.19, color = "plum3") %>%
  gf_abline(intercept = 63, slope=-0.05, color = "purple") %>%
  gf_abline(intercept = 120, slope=-0.51, color = "purple3") %>%
  gf_abline(intercept = 7, slope=.21, color = "orchid")
```

Some of the straight-line models are a much better match to the data than others. The blue-shaded functions are pretty good fits, the greenish functions are maybe OK but a little sketchy, and the purple-shaded functions are just horrible.

Now that you know what a reasonable straight-line model looks like, you will find it pretty easy to draw one on data graphics that even remotely show a straight-line pattern. 

Step 1: Draw a reasonable straight-line through the data points. 

Step 2: Find the parameters that correspond to the line you drew. 

## Average rate of change

Since the slope is our standard way of representing a relationship of change, we will often use it as a way of summarizing a function. To illustrate, consider the exponential model we constructed to match the cooling-water data:

```{r}
water <- makeFun(60.7*exp(-0.019*t) + 25.93 ~ t)
```



```{r water-average, echo=FALSE, fig.cap="The exponential function that was previously matched to the cooling-water data. The slope of the straight line connecting two points on the function graph is the average rate of change during the interval."}

Pts <- tibble::tibble(
  x = c(10,125),
  y = water(x),
  baselabel = c("t0", "t1"),
  flabel = c("f(t0)", "f(t1)")
)
slice_plot(water(t) ~ t, domain(t = c(0,200))) %>%
  gf_point(y ~ x, data = Pts, color="red", alpha=0.25, size=4) %>%
  gf_line(y ~ x, data = Pts, color="red") %>%
  gf_point(0 ~ x, data = Pts, color="blue", alpha=0.25, size=4) %>%
  gf_lims(y=c(0,90)) %>%
  gf_segment(0 + y ~ x + x, data=Pts, color="blue", linetype=2) %>%
  gf_text(0 ~ x, data = Pts, label = ~baselabel, color="blue", nudge_x=7) %>%
  gf_text(y ~ x, data = Pts, label = ~flabel, color="red", nudge_y=7, nudge_x=2)
```

During the interval $[t_0, t_1]$ the rate at which the water cools is higher at first and lower at the end. The ***average*** rate of change is a single number that summarizes the whole interval.

For all except straight-line models, the average rate of change depends on the interval chosen.

EXERCISE: From the graph, compute the average rate of change over the interval $10 \leq t \leq 200$. How does it compare to the average rate of change over the interval $10 \leq t \leq 125$?

EXERCISE: Same, but for a sinusoid. Give a couple of intervals, including one where there is no net change.



## Instantaneous rate of change

The water is cooling continuously. Sometimes we'll be interested in the rate of change at a given instant $t_0$. If we choose a very small interval, say $[t_0, t_0 + 0.1]$, we'll get a good approximation to this ***instantaneous rate of change***.
$$\text{instantaneous rate of change} = \frac{f(t_0 + 0.1) - f(t_0)}{0.1}$$

On a graph, the instantaneous rate of change is the slope of the tangent line to a point, as shown in Figure \@ref(fig:tangent-line-exp).

```{r tangent-line-exp, echo=FALSE, fig.cap="Instantaneous rate of change is the slope of the line tangent to a the curve at a single point."}
Pt <- tibble::tibble(
  x = 25,
  y = water(x),
  baselabel="x0",
  flabel = "f(x0)"
)
tangent <- makeFun(-0.71722*(t-25) + 63.7 ~ t)
slice_plot(water(t) ~ t, domain(t=c(0, 200))) %>%
  gf_point(y ~ x, data=Pt, 
           color="sienna", alpha = 0.25, size=4) %>%
  gf_point(25 ~ x, data=Pt, 
           color="tomato", alpha = 0.25, size=4) %>%
  slice_plot(tangent(t) ~ t, color = "sienna", 
             domain(t=c(0,80))) %>%
  gf_text(y ~ x, data = Pt, label = ~ flabel, 
          nudge_x = 10, color="sienna") %>%
  gf_text(25 ~ x, data = Pt, label = ~ baselabel, 
          nudge_x = 10, color="tomato") %>%
  gf_segment(25 + y ~ x + x, data=Pt, color="tomato", linetype=2)
```

In Block 2, we'll see that a good way to define an instantaneous rate of change at $t_0$ is as the average rate of change over the interval $t_0 \leq t \leq t_1$ with the proviso that the interval length $t_1 - t_0$ goes as closely as it can to zero.

Figure \@ref(fig:three-rates) shows the fitted rate of change and average rate of change in the interval $10 \leq t \leq 125$ as well as the instantaneous rate of change at $t_0 = 50$. As you can see, they are different from one another.

```{r three-rates, echo=FALSE, warning=FALSE, fig.cap="Comparing the average rate of change (red) over the interval $10 \\leq t \\leq 125$, the instantaneous rate of change at $x_0 =25$, and the fitted linear model (orange). The exponential model is shown in gray."}
gf_point(temp ~ time, data = Stans_data, color="orange") %>%
  gf_lm(color="orange") %>%
  slice_plot(water(time) ~ time, domain(time=c(0, 200)), color="gray", size=2, alpha = 0.5) %>%
  slice_plot(tangent(time) ~ time, color = "sienna", 
             domain(time=c(0,80))) %>%
  gf_point(y ~ x, data = Pts, color="red", alpha=0.25, size=4) %>%
  gf_line(y ~ x, data = Pts, color="red") %>%
  gf_point(25 ~ x, data = Pts, color="blue", alpha=0.25, size=4) %>%
  gf_point(y ~ x, data=Pt, 
           color="sienna", alpha = 0.25, size=4) %>%
  gf_point(25 ~ x, data = Pt, color="tomato", alpha=0.25, size=4) %>%
  gf_segment(25 + y ~ x + x, data=Pt, color="tomato", linetype=2) %>%
  gf_segment(25 + y ~ x + x, data=Pts, color="blue", linetype=2) %>%
  gf_lims(y=c(25, NA))
```

## The slope function

We have a way to estimate the instantaneous rate of change of a function at a given input $t_0$:
$$\text{instantaneous_rate_of_change} \equiv \frac{f(t_0 + 0.1) - f(t_0)}{0.1}$$
Now we'll let notation do some work for us and re-write the above as the instantaneous-rate-of-change function, which we'll call the ***slope*** function.
$$\text{slope}(t) \equiv \frac{f(t + 0.1) - f(t)}{0.1}$$
We've merely made it clear that $\text{slope}(t)$ is a function and written the formula in terms of the input $t$ rather than a fixed number $t_0$. Of course, the "instantaneous rate of change at $t_0$" is simply $\text{slope}(t)$.

Let's look at the slope function that corresponds to some of our naked modeling functions: $e^x$, $\sin(x)$, $x^{-1}$ and $\ln(x)$. Recall that the input to the naked functions is always a number. The output is a number as well. Thus we can display the function and the corresponding slope function on the same coordinate frame. (This will not be true when the input is a quantity and the output is another quantity.)


```{r compare-naked-slopes, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="Comparing the naked modeling function (blue) to it's slope function (red)"}
slice_plot(sin(x) ~ x, domain(x=c(-10,10)), color="blue") %>%
  slice_plot(cos(x) ~ x, color="red", alpha=0.25, size=2) %>%
  gf_labs(title="Sinusoid")
slice_plot(exp(x) ~ x, domain(x=c(-2,2)), color="blue") %>%
  slice_plot(exp(x) ~ x, color="red", alpha=0.25, size=2) %>%
  gf_labs(title="Exponential function")
slice_plot(log(x) ~ x, domain(x=c(0.1,5)), color="blue") %>%
  slice_plot(1/x ~ x, color="red", alpha=0.25, size=2) %>%
  gf_labs(title="Logarithm function")
slice_plot(1/x ~ x, domain(x=c(0.1,5)), color="blue") %>%
  slice_plot(-1/x^2 ~ x, color="red", alpha=0.25, size=2) %>%
  gf_labs(title="Power law function (with p=-1)")
```

EXERCISE: There is a web of connections between the naked modeling functions and their slopes. 

```{r slope-sin, echo=FALSE, results="markup"}
etude2::etudeQ(
  "1. Which naked modeling function has a slope function that is simply a input-shifted version of itself?",
  "exponential",
  "+sinusoid+",
  "logarithm",
  "power-law $x^{-1}$",
  random_answer_order = FALSE
)
```

```{r slope-exp, echo=FALSE, results="markup"}
etude2::etudeQ(
  "2. Which naked modeling function has a slope function that is identical to itself?",
  "+exponential+",
  "sinusoid",
  "logarithm",
  "power-law $x^{-1}$",
  random_answer_order = FALSE
)
```

```{r slope-log, echo=FALSE, results="markup"}
etude2::etudeQ(
  "3. Which naked modeling function has a slope function that is another naked modeling function? (Hint: The other function is also shown in the set of graphs.)",
  "exponential",
  "sinusoid",
  "+logarithm+",
  "power-law $x^{-1}$+",
  random_answer_order = FALSE
)
```


::: {.why}
We defined the "slope function" as
$$\text{slope}(t) \equiv \frac{f(t + 0.1) - f(t)}{0.1}$$

The 0.1 in the definition of the slope function is definitely a kluge.  0.1 minutes may be small when it comes to cooling water, but it might not be small in other contexts. We'll fix that in Block 2 when we define the ***derivative operator*** which replaces 0.1 with something that small in every context.
:::



## To be turned into exercises

"Slope" is a natural metaphor when thinking of a function as a graph. But a more general way to describe the concept is the ***rate of change*** of the output with respect to the input. The change in the output from one end of the interval  is $f(x_1) - f(x_0)$, the change in the input is $x_1 - x_0$. If the input is time (in hours), and the output is the position of a car (in miles), then the rate of change is *miles-per-hour*: the car's velocity.

For a straight-line function---think of a car driving at constant speed on a highway---it doesn't matter what you choose for $x_1$ and $x_0$ (so long as they are not identical). But for other functions, the choice does matter.

Imagine a graph of the position of a car along a road as in Figure \@ref{fig:stop-and-go}. Over the course of an hour, the car travelled about 25 miles. In other words, the ***average*** speed is 25 miles/hour: the *slope* of the red line segment. Given the traffic, sometimes the car was stopped (time C), sometimes crawling (time D) and sometimes much faster than average (time B).  

```{r stop-and-go, echo=FALSE}
f <- rfun(~ t, seed=105, n=5)
raw <- function(t) 
        f(t) - t - 30*dnorm(t, 0, 3) + 60*dnorm(t,7,1)
speed <- function(t) {
    pmax(4*raw(20*(t-.5)), 0)
}
position <- antiD(speed(t) ~ t)
Pts <- tibble::tibble(
    t = c(0, 0.19, 0.4, 0.54, 0.65, 1),
    y = position(t) + 2,
    label=c("", "A", "B", "C", "D", "")
)
Intervals <- tibble::tribble(
    ~t0, ~ t1, ~color,
    0, 1, "red",
    .54, .65, "orange",
    .19, .4, "green",
    .4, .54, "brown",
) %>%
    mutate(y0=position(t0), y1=position(t1))
slice_plot(position(t) ~ t, domain(t = c(0, 1)), size=2) %>%
    gf_labs(y = "x(t): Position from start of trip (miles)",
            x = "Time since start (hours)") %>%
    gf_text(0 ~ t, data = Pts, label=~label, color="blue") %>%
    gf_segment(2 + y ~ t + t, data = Pts[-6,], color="blue") %>%
    gf_segment(y0 + y1 ~ t0 + t1, data = Intervals, color=~color, alpha=0.5, size=3) %>%
    gf_refine(scale_color_identity())
```

During the interval from B to C, the car was travelling relatively fast. The slope of the brown segment connecting the position at times B and C is the ***average*** rate of change between times B and C. It's easy to see that the average rate of change from B to C is larger than the overall average from $t=0$ to $t=1$. Calculating that slope is a matter of evaluating the position at the endpoints and dividing by the length of the interval.

::: {.workedexample latex-data=""}
What is the average rate of change in the car's position during the interval $t_B = 0.40$ to $t_C=0.54$?

The length of the interval is $t_C - t_B = 0.54-0.40=0.14$.

Evaluating the function gives $x(t_C) = 18$ and $x(t_B) = 12.6$. 

Rise is $x(t_C) - x(t_B) = 18 - 12.6 = 5.4$.

Run is $t_C - t_B = 0.54-0.40=0.14$.

The average rate of change during the interval is $5.4/0.14 = 38.6 $ miles/hour.
:::

::: {.workedexample latex-data=""}
The graph shows a simplified model of the amount of usable wood that can be harvested from a typical tree in a managed forest of Ponderosa Pine. (You can see some actual forestry research models [here](https://www.fs.fed.us/rm/pubs/rmrs_gtr292/1992_milner.pdf).)

```{r aver-tree, echo=FALSE}
tree_volume <- makeFun(1000*pnorm(year, mean=15, sd=10)^3 ~ year)
# do the deriv numerically to avoid bug in derivative 
# of normal function.
tree_growth <- numD(tree_volume(year) ~ year)
slice_plot(tree_volume(year) ~ year, domain(year = c(0, 50))) %>%
  gf_labs(y = "Usable wood product (board-feet)",
          x = "Year after planting")
```

You are writing a business plan for a proposed pine forest. Among other things, you have to forecast the revenue that will be generated and when you will have salable product.

They say that "time is money." Every year you wait before harvest is another year that you don't have the money. On the other hand, every year that you wait means more wood at the end. How to decide when to harvest? 

The tree continues to grow until year 50, when it seems to have reached an equilibrium: perhaps growth goes to zero, or rot balances what growth there is. There's no point waiting until after year 50. 

At year 25, the tree is growing as fast as it ever will. You'll get about 600 board-feet of lumber. Should you harvest at year 25? No! That the tree is growing so fast means that you will have a lot more wood at year 26, 27, and so on. The time to harvest is when the growth is getting smaller, so that it's not worth waiting an extra year.

The quantity of interest is the average rate of growth from seedling to harvest. Harvesting at year 25 will give a total change of 600 board feet over 25 years, giving an average rate of change of $600 \div 25 = 24 \mbox{board-feet-per-year}$. But if you wait until year 35, you'll have about 900 board feet, giving an average rate of change of $900 \div 35 = 25.7 \mbox{board-feet-per-year}$. 
:::

We've been presenting the average rate of change as a **number**: $$\frac{f(t_B) - f(t_A)}{t_B - t_A}$$

But it's helpful to think of it as a **function** that takes two inputs, which we can call $t_A$ and $t_B$:
$$\text{ave_rate_of_change}(t_A, t_B) \equiv \frac{f(t_B) - f(t_A)}{t_B - t_A}$$
This is a very subtle maneuver. The formula is exactly the same, but by writing the quantities $t_A$ and $t_B$ as inputs (by putting them in parentheses on the left side), we turn the formula into a function.

Sometimes the average-rate-of-change function presents the information in a clear way.

::: {.workedexample latex-data=""}
Back to the forest ... Here's a graph of the average-rate-of change function. The function has two inputs, but botany dictates that we can only be growing word from the time the seedling is planted. So we'll set $t_A = 0$ and look at the corresponding *slice* of the function:

```{r echo=FALSE}
slice_plot(tree_volume(year)/year ~ year, domain(year = c(1, 50))) %>%
  gf_labs(y = "Annual growth (board-feet)",
          x = "Year after planting")
```
The graph makes it clear that the maximum average growth from planting to harvest will occur at about year 32.
:::





<!--chapter:end:Fun-slopes.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Assembling functions {#fun-assembling}

When we need a new function for some purpose, we practically always build it out of existing functions. For instance, a  parameterized function like $$A \sin\left(\frac{2 \pi}{P}x\right) + C$$ is built by assempling together a straight-line input scaling, a naked $\sin()$ function, and another straight-line function for scaling the output from $\sin()$. 

In this chapter, we'll introduce three general frameworks for combining functions: linear combination, composition, and multiplication.

## Linear combination 

One of the most widely used sorts of combination is called a ***linear combination***. The mathematics of linear combination is, it happens, at the core of the use of math in applications, whether that be constructing a Google-like search engine or analyzing medical data to see if a treatment has a positive effect.

You've worked for many years with one kind of linear combination: polynomials. No doubt you've seen functions^[It's likely that you saw polynomials as things to be factored, rather than as functions taking an input and producing an output. So they were written as *equations*: $e x^2 + 5x - 2 = 0$] like $$f(x) \equiv 3 x^2 + 5 x - 2$$

There are three modeling functions in this polynomial. In this case, as in polynomials generally, they are all power-law functions: $g_0(x) \equiv 1$, $g_1(x) \equiv x$, and $g_2(x) \equiv x^2$. With these functions defined, we can write the polynomial $f(x)$ as $$f(x) \equiv 3 g_2(x) + 5 g_1(x) - 2 g_0(x)$$
Each of the functions is being scaled by a quantity---3, 5, and -2 in this example---and the scaled functions are added up. That's a linear combination; scale and add. (Later, we'll see that the ***scalars*** generally come with units. So we might well have a metric polynomial and an equivalent traditional-unit polynomial. Just wait.)


::: {.why}
Notice that we said $g_0(x) \equiv 1$ is a power-law function. Why?  Since $x^0=1$ for all $x$, we might equally well have written $g_0(x) \equiv x^0$ which is visibly a power-law function. 

We're just trying to emphasize that a polynomial is a linear combination of power-law functions, in particular those power-law functions with whole-number, positive exponents. The individual functions, say, $x^2$ or $x^5$ are called ***monomials***. A polynomial is a combination of monomials, just like a chemical polymer is a combination of monomers.
:::

There are other places where you have seen linear combinations: 

- The parameterized ***sinusoid***  $$A \sin\left(\frac{2 \pi}{P}t\right) + C$$ is a linear combination of the functions $h_1(t) \equiv \sin\left(\frac{2 \pi}{P} t\right)$ and $g_0(t) \equiv 1$. The scalars are $A$ and $C$.
- The parameterized ***exponential*** $$A e^{kt} + C$$ The functions being combined are $e^{kt}$ and $g_0(t) \equiv 1$. The scalars are, again, $A$ and $C$.
- The straight line function $a x + b$. The functions being combined are $x$ and $1$, the scalars are $a$ and $b$.

Note that neither the parameterized exponential or the parameterized sinusoid is a polynomial.

There are a few reasons for us to be introducing linear combinations here.

1. You will see linear combinations everywhere once you know to look for them.
2. There is a highly refined mathematical theory of linear combinations that gives us powerful ways to think about them as well as computer software that can quickly find the best scalars to use to match input-output data.
3. The concept of linear combination generalizes the simple idea that we have been calling "scaling the output." From now on, we'll use the linear-combination terminology and avoid the narrower idea of "scaling the output."
4. Many physical systems are described by linear combinations. For instance, the motion of a vibrating molecule or a helicopter in flight or a building shaken by an earthquake are described in terms of simple "modes" which are linearly combined to make up the entire motion. More down to Earth, the timbre of a musical instrument is set by the scalars in a linear combination of pure tones.
5. Many modeling tasks can be put into the framework of choosing an appropriate set of simple functions to combine and then (letting the computer) figure out the best scalars to use in the combination.

Even better, there is an automatic, reliable, and fast algorithm for finding the scalars for a set of functions that are to be combined to match data as closely as possible. So the modeler just has to select the functions to be used, the computer can find the coefficients. (We'll explore the mathematics and methods of linear combinations, usually called ***linear algebra***, in Block 5.)



::: {.todo}
Example: The tides as a linear combination of sinusoids.
:::

::: {.todo}
Example: Models of inequality

Lorenz curve: Connects (0,0) and (1,1) and is never concave down.  

- Any power-law with an exponent of 1 or greater.
- Any linear combination of such power-laws with the scalars adding to 1.
- Product of two Lorenz curves is a Lorenz curve. 

$(1 - (1-p)^\alpha)$

$p^\beta\left(1 - (1-p)^\alpha\right)$

:::

## Function composition {#function-composition}

To ***compose*** two functions, $f(x)$ and $g(x)$, means to apply one of the functions to the output of the other. "$f()$ composed with $g()$" means $f(g(x))$. This is generally very different from "$g()$ composed with $f()$" which means $g(f(x))$.

For instance, suppose you have recorded the outdoor temperature over the course of a day and packaged this into a function $\mbox{AirTemp}(t)$: temperature as a function of time $t$. Your digital thermometer uses degrees Celsius, but you want the output units to be degrees Kelvin. The conversion function is $$\mbox{CtoK}(C) \equiv C + 273.15$$
Notice that CtoK() takes temperature as input. With this, we can write the "Kelvin as a function of time" as $$\mbox{CtoK}\left(\mbox{AirTemp}(t)\right)$$
It's important to distinguish the above time $\rightarrow$ Kelvin function from something that looks very much the same but is utterly different: $\mbox{AirTemp}\left(\mbox{CtoK}(t)\right)$. As a matter of arithmetic, you can put time as an input to CtoK(). But it has a completely different meaning in terms of the real world. If time were measured in hours, then CtoK$(t)$ would be looking at the temperature eleven and a half days ago, instead of the temperature at time $t$.

::: {.why}
We used $C$ as the name of the input to CtoK(). Shouldn't it be something like $x$ or $y$?

Keep in mind that the names of the inputs to a function can be anything whatsoever, so long as they are used consistently in the function algorithm. If we wanted, we could call the first input to any function $x$, or for that matter $y$ or giraffe.

The point of the notation $\mbox{CtoK}(C) \equiv C + 273.15$ is to make it utterly clear that $C$ is the name we're using for the input by listing it in the parentheses that follow the function name CtoK(). This, unlike high-school notation, allows us great freedom in the choice of names. 

We can use that freedom to make it easier to communicate with other people. (And remember, one of those other people is "future you.") We used the name $C$ to reinforce the message that CtoK() converts *celsius* to *kelvin*, and not vice versa.
:::


Here is a simple, approximate formula for the length of day (in hours) as a function of latitude $L$ and the declination angle $\delta$ of the sun. 

$$\text{day_length}(L, \delta) \equiv \frac{2}{15} \arccos\left(-\tan(L)*\tan(δ)\right)$$
The declination angle is the latitude of the point on the earth's surface pierced by an imagined line connecting the centers of the earth and the sun. On the summer solstice, it is $23.44^\circ$, the longest day of the year. 

A computer implement must look different, since $L$ and $\delta$ are typically provided in degrees while the `tan()` and other trigonometric functions in most computer languages expect units of radians. The conversion is easy: $\text{deg2rad}(d) \equiv \frac{\pi}{180} d$. The conversion the other way is $\text{rad2deg}(r) \equiv \frac{180}{\pi} r$.

In order to get the day-length formula to work in a computer, we have to compose the $\tan()$ function with `deg2rad()`. The output of `acos()` is in radians, so we have to convert it back to degrees. Like this:

```{r}
day_length <- makeFun(
  (2/15)*rad2deg(
    acos(
      -tan(deg2rad(L))*tan(deg2rad(d))
    )
  ) ~ L+d)
```

```{r day-length-cos, echo=FALSE, results="markup"}
etude2::etudeQ(
  "Using an R sandbox, calculate the length of the day at latitude $39^\\deg$ on the longest day of the year, which is when the declination of the sun is $23.44^\\circ$. How long is it?",
  "13.9 hours",
  "+14.7 hours+",
  "14.9 hours",
  "15.1 hours",
  random_answer_order = FALSE
)
```

Now to make a plot of day length as a function of day of the year. Of course, `day_length(L, d)` does not take day of the year into account. What's missing is to know the declination of the sun as a function of calendar day.

We'll represent calendar day as a number $n$ that runs from 0 at the start of January 1st to 365 at the end of December 31. Given this convention, the declination of the sun is
```{r}
delta_sun <- makeFun(-23.44*cos((2*pi/365)*(n+10) ) ~ n)
```

Composing `day_length()` with `delta_sun()` (on the `d` argument only) we get a function of day of year `n`:
```{r}
slice_plot(
  day_length(39, delta_sun(n)) ~ n, 
  domain(n=c(0,365))
  )
```


```{r april-fools, echo=FALSE, results="markup"}
etude2::etudeQ(
  "How long is the day on April Fools ($n=90$) at Latitude 39N? (Use a sandbox for the calculation.)",
  "+12.38 hours+",
  "12.59 hours",
  "12.64 hours",
  "12.74 hours",
  random_answer_order = FALSE
)
```

## The modeling polynomial {#modeling-polynomial-1}

Sometimes, in order to model some simple relationship you need to build a function whose graph has a simple, curving shape.

A simple but surprisingly powerful approach is to use a ***low-order polynomial***. The ***order of a polynomial*** is the highest exponent on the input. For example, a straight-line function, $g_1(x) \equiv a_0 + a_1 x$, is a **first-order** polynomial. A ***quadratic***, $g_2(x) \equiv b_0 + b_1 x + b_2 x^2$ is a **second-order** polynomial.

Many modelers are tempted to extend the technique to third-, fourth-, fifth-order and even higher. This is only rarely worthwhile since all second-, fourth-, sixth- and higher-even-order ***monomials*** have basically the same U-shape, like a referee signalling a touch-down. Similarly, first-, third-, fifth- and higher odd-order monomial have  similar ![](www/cubic.png) shapes.  

An ofttimes better approach is to compose the polynomial with a curved but monotonic function, such as a logarithm.

::: {.why}
Explain why we are now using subscripts for the names of scalars.
:::

## Tukey's ladder (optional)

::: {.todo}
[[Tukey's ladder]] Using a simple straight line with input composition and/or output composition. You can get a variety of curve shapes using only a linear function.
:::


::: {.todo}
As exercises?



Log transformation of used car prices

[[Perceived brightness or loudness]] is a log scale.


:::

## Function multiplication

The third in our repertoire of methods for making new function out of old is plain old multiplication. With two functions $f(x)$ and $g(x)$, the product is simply $f(x)g(x)$.

It's essential to distinguish between function multiplication and function composition:

$$\underbrace{f(x) g(x)}_\mbox{multiplication}\ \ \ \ \underbrace{f(g(x)) \ \ \mbox{or}\ \ \ g(f(x))}_\mbox{composition}$$

In function composition, only one of the functions---the ***interior function*** is applied to the overall input, $x$ in the above example. The other function gets its input from the output of its partner. 

In multiplication, each of the functions is applied to the input individually. Then their outputs are multiplied to produce the overall output.

In function composition, the order of the functions matters: $f(g(x))$ and $g(f(x))$ are in general completely different functions.

In function multiplication, the order doesn't matter because multiplication is ***commutative***, that is, if $a$ and $b$ are each quantities, $a \times b = b \times a$. 



***Transient vibration***

A guitar string is plucked to produce a note. The note is, of course, vibrations of the air created by vibrations of the string. 

After plucking, the note fades away. An important model of this is a sinusoid (of the correct period to correspond to the frequency of the note) times an exponential.

Function multiplication is used so often in modeling that you'll see it in many modeling situations. Here's one example that is important in physics and communication: the ***wave packet.*** Overall, the wave packet is a localized oscillation as in Figure \@ref{fig:wave-packet}. 

```{r wave-packet, echo=FALSE, fig.cap="A *wave packet* constructed by multiplying a sinusoid by a hump function."}
wave_packet <- function(x, A=1, center = 0, width = 5, P=1, phase = 0) {
  A*sin(2*pi*x/P)*dnorm((x - center)/width)
}
slice_plot(wave_packet(x, A=2, center=3, width  = 4, P  = 10/5 ) ~ x, 
           domain(x = c(-15, 15)), npts = 500)
```
This is the product of two simple functions: a hump times a sinusoid.

```{r making-wave-packet, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="The two components of the wave packet in Figure \\@ref(fig:wave-packet)"}
sinusoid <- makeFun(sin(2*pi*x/P) ~ x, P=10/5)
envelope <- makeFun(2*dnorm((x - center)/width) ~ x, center=3, width=4)
slice_plot(envelope(x) ~ x, domain(x = c(-15, 15)), color="blue") %>%
  gf_labs(title="The \"envelope\" of the wave packet")
slice_plot(sinusoid(x) ~ x, domain(x = c(-15, 15)),
           color="red", npts=500) %>%
  gf_labs(title="The oscillation in the wave packet. ")
```
EXERCISES: Say which two kinds of functions are being multiplied here.

```{r echo=FALSE}
slice_plot(sin(2*pi*t/2.1)*sin(2*pi*t/20) ~ t, domain(t=c(-30, 40)), npts=500)
```
```{r sine-in-sine1, echo=FALSE, results="markup"}
etude2::etudeQ(
  "What are the two basic modeling functions being multiplied?",
  "+A sinusoid and another sinusoid with a faster period.+",
  "A hump and a sinusoid.",
  "A sigmoidal function and a sinusoid.",
  ""
)
```


```{r sine-in-sine2, echo=FALSE, results="markup"}
etude2::etudeQ(
"What is the period of the envelope?", 
"10" = "It's true that the broad peaks in the overall function occur every 10 time units. But a sine wave has two excursions from zero every cycle, one positive and one negative. So if the period of the envelope were 10, we would see an amplitude of the faster sinusoid near zero every 5 time units.",  
"+20+",
"30" = "If this were so, the dips in amplitude of the faster sign would occur every 15 time units.",
random_answer_order = FALSE
)
```

Other examples along the same lines.

EXERCISE: Provide a sigmoid and a series of humps. Ask the student to sketch out the product.

## All together now!

Two or all three of the techniques for combining functions---linear combinations, function composition, and function multiplication---can be used in the same function.

Consider the function for the length of the day
$$\text{day_length}(L, \delta) \equiv \frac{2}{15} \arccos\left(-\tan(L)*\tan(δ)\right)$$
The 2/15 is scaling the output of $\arccos()$: scaling is in linear combination.

The $\arcos()$ is being composed with an interior function that is itself a scaled product of two functions. 

EXERCISE: The algebra of composition of the hump function $e^{-x^2}$.

<!--
Air pressure vs time for a car driving up a mountain.

[[Solar declinsion as function of time of year.]] Then turn this into day-length. See <https://physics.stackexchange.com/questions/28563/hours-of-light-per-day-based-on-latitude-longitude-formula>

Sun's declination angle
$$\delta = \Phi \cos\left(C (d-d_r)/dy\right)$$ where $\Phi =23.44^\circ$, $C=360^\circ$, $d_r$ is Julian day for summer solstice in northern hemisphere: June 21 which is day 172, $d_y$ is the number of days per year: 365.25.

```{r}
solar_dec <- makeFun(23.44*cos(2*pi*(d - 172)/365.25) ~ d)
slice_plot(solar_dec(d)~ d, domain(d=c(0,1000)))
```
Hour angle of sunrise
```{r}
ha <- makeFun(acos( 
  #cos(90.833*pi/180)/(cos(L)*cos(d))
                - tan(L)*tan(d)) ~ L + d)
ha(pi/4, 23*pi/180)
slice_plot((180/pi)*ha(pi/4, ang) ~ ang, domain(ang=pi*c(-23, 23)/180))
# Length of day at latitude L, Julian day d
dlen <- makeFun(2*(180/pi)*ha(L, (pi/180)*solar_dec(d))/15 ~ L + d)
slice_plot(dlen(66.5*pi/180, d) ~ d, domain(d=c(0,365)), npts=5000) %>%
  gf_hline(yintercept=c(0, 24), color="blue") %>%
  gf_lims(y=c(0,24))
```

Length of day: $2 \text{ha}(L, d)/15$
Composition example: write ha() to accept degrees rather than radians


ha = arccos (cos(90.833) /(cos(L)*cos(δ)) - tan(L)*tan(δ))

[[Instantaneous power of AC voltage]] Power = I^2 R = V^2/R
V will be a sinusoid


Log transformation of prices (log income rises steadily, so compose exp with a straight-line function.

Log-log and semi-log plots are an example of composing a log with a function to get a simpler function.




[[For integration chapter]] Wind energy potential: not the cube of the average wind velocity, but the average of the cube of wind velocity.



## (from original draft) Combining and composing functions


::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Fun-4-c-1", "Use linear combination and direct multiplication of functions of one variable to produce functions of multiple variables")
state_objective("Fun-4-c-2", "Recognize that composition of functions does not itself create a function of multiple variables.")
```
:::

Linear combination and composition of functions are often used hand-in-hand to construct models. For instance, our model of the data from cooling water was a linear combination of a constant function and a sinusoidal function $$\mbox{temperature}(t) \equiv 25.9 + 60.7\, e^{-0.019 t}$$
Similarly, our model from the Providence tide data was 
$$\mbox{tide}(t) \equiv 1.02 + 0.50 \sin(2\pi (t - 15.4)/12.56)$$
In both cases, the sine part of the combination is a mathematical ($\exp(x)$ or $\sin(x)$) function composed with a linear interior function: $-0.019 t$ for the cooling water, and $2\pi (t - 15.4)/12.56$ for the tides.

Each of these models was a function of one input. (Coincidentally, the input in both cases was "time", measured in minutes in the cooling-water example and hours in the tide example.)

--> 

<!--
::: {.todo}
**The modeling cycle**?????????

Redo the length of day with the correction for diffraction and the angle subtended by the sun.
:::
-->

<!--chapter:end:Fun-assembling.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Functions with multiple inputs {#fun-multiple-inputs}

We can use ***linear combination*** and ***function multiplication*** to build up custom functions from the basic modeling functions. Similarly, linear combination and function multiplication provide ways to construct functions of multiple inputs. 

## f(x) times g(t)

For example, soon after a guitar string is plucked it conforms to a sinusoid pattern of **displacement** from the straight-line connecting the two fixed ends of the string: one set by finger pressure on the fret and the other at the bridge.

<iframe width="560" height="315" src="https://www.youtube.com/embed/F0NsJ7J8rYM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

For a string of length $L$, the string displacement is a function of position $x$ along the string and is a linear combination of functions $$g_k(x) \equiv \sin(k \pi x /L)$$ where $k$ is an integer.  A few of these functions are graphed in Figure \@ref(fig:guitar-string-modes).

```{r guitar-string-modes, echo=FALSE, fig.height=3, fig.width=7, fig.show="hold"}
slice_plot(sin(pi*x/100) ~ x, domain(x=c(0, 100)), 
           label_text = "k=1") %>% 
  slice_plot(sin(2* pi*x/100) ~ x, color="blue",
             label_text="k=2", label_x = 0.95) %>%
  slice_plot(sin(3* pi*x/100) ~ x, color="green",
             label_text="k=2", label_x = 0.90) %>%
  gf_labs(x="", y="", title="Three modes of a guitar string.")
string_shape <- function(x, t) {
  sin(pi*x/100)*sin(pi*t) - 2*sin(2*pi*x/100)*sin(2*pi*t) - 1.5*sin(3*pi*x/100)*sin(3*pi*t)
}

slice_plot(
  string_shape(x, t=pi/2) ~ x,
  domain(x=c(0,100))
) %>%
  gf_labs(x="Position along string (%)", y="Side-to-side displacement (mm) ", title="A linear combination of the modes")
```

Shapes of the sort in Figure \@ref(fig:guitar-string-modes) are a stop-motion flash snapshot of the string. The string's shape also changes in time, so the string's displacement is a function of both $x$ and $t$. The displacement itself is a sinusoid whose time period depends on the length and tension of the string as well as the number of cycles of the spatial sine: 
$$g_k(x, t) \equiv \sin(\frac{k \pi}{L} x) \ \sin(\frac{k \pi}{P}t)$$ Figure \@ref(fig:string-motion) shows a few snapshots of the 1.5 cycle string at different moments in time, and the motion of the linear combination.

```{r string-motion, echo=FALSE, fig.height=3, fig.width=7, fig.show="hold"}
slice_plot(sin(3*pi*x/100) * sin(.5) ~ x, 
           domain(x=c(0,100))) %>%
  slice_plot(sin(3*pi*x/100) * sin(.6) ~ x, alpha = 0.5) %>%
  slice_plot(sin(3*pi*x/100) * sin(.7) ~ x, alpha = 0.25) %>%
  gf_labs(title="Shape at three different instants",
          x = "", y="Side-to-side displacement")
slice_plot(string_shape(x, .5) ~ x, domain(x=c(0,100))) %>%
  slice_plot(string_shape(x, .525) ~ x, alpha = 0.7) %>%
  slice_plot(string_shape(x, .55) ~ x, alpha = 0.5) %>%
  slice_plot(string_shape(x, .575) ~ x, alpha = 0.37) %>%
  slice_plot(string_shape(x, .6) ~ x, alpha = 0.25) %>%
  gf_labs(y="Side-to-side displacement (mm)",
          x = "Position along string (%)",
          title="Linear combination of vibrating modes at different times.")
```

## Two-variable modeling polynomial

In Section \@ref(modeling-polynomial-1) we introduced the low-order polynomial, either $g_1(x) \equiv a_0 + a_1 x$ or $g_1(x) \equiv b_0 + b_1 x + b_2 x^2$ as a general-purpose way of generating a function with a smoothly curved shape. The same applies in constructing simple functions of two variables. 

Almost always, you should use at least a first-order polynomial, which is:
$$h_1(x, y) \equiv a_0 + a_x x + a_y y$$
But there is an important extension of this, using what's called a ***bilinear term*** or, more evocatively in statistics, an ***interaction term***.  This is
$$h_2(x, y) \equiv \underbrace{b_0}_\mbox{intercept} + \underbrace{b_x\, x + b_y\, y}_\mbox{linear terms} + \underbrace{b_{xy}\,x\, y}_\mbox{bilinear term}$$

The bilinear term arises  in models of phenemona such as the spread of epidemics, the population dynamics of predator and prey animals, and the rates of chemical reactions. In each of these situations one thing is interacting with another: a predator killing a prey animal, an infective individual meeting a person susceptible to the disease, one chemical compound reacting with another.

Under certain circumstances, modelers include one or both ***quadratic terms***, as in 
$$h_3(x, y) \equiv c_0 + c_x\, x + c_y\, y + c_{xy}\,x\, y + \underbrace{c_{yy}\, y^2}_\mbox{quadratic in y}$$
The skilled modeler can often deduce which terms to include from basic facts about the system being modeled. We'll need some additional calculus concepts before we can explain this in a straightforward way.


::: {.why}

Explain why we are not using letter subscripts on the scalars in the linear combination.

:::


## Function composition (not!)

We left function ***composition*** out of the list of ways to build multivariable functions out of simpler functions with a single input.  

For instance, consider the two functions $f(x)$ and $g(t)$. The composition $f(g(t))$ has only **one** input: $t$. Similarly, $g(f(x))$ has only one input: $x$.



EXERCISE:

In Section \@ref(function-composition) you saw a function giving the declination of the sun as a function of day of year, and length-of-day as a function of latitude and sun's declination. Putting these together let's us assemble day-length as a function of latitude and day of year.

Give function. DRAW CONTOUR PLOT, take slices. Day length as seen by a migrating bird. [Plug in a simple sinusoid for latitude to reduce the function to day-length versus day-of-year.]




```{exercise, NOAA, name="Tables are functions"}
```
`r knitr::knit_child(exercise_file("01", "NOAA.Rmd"))`

Contour plot of length of day versus latitude.


EXERCISE:

Recall the Pythagorean theorem: $C^2 = A^2 + B^2$. Let's write this as a function that takes as inputs the lengths of the two legs and produces as output the length of the hypotenuse. 

$$\mbox{hypotenuse}(x, y) \equiv \sqrt{\strut x^2 + y^2}$$

This can be seen as a composition of a function $f()$ together with a linear combination of two power-law functions $g()$ and $h()$ of different inputs. What is the function $f()$? What are $g()$ and $h()$? What are the scalars in the linear combination?

EXERCISE: What are the two pieces in this piecewise function of two variables?

```{r}
bigger <- makeFun(ifelse(y > x, y, x) ~ x + y)
contour_plot(bigger(x,y) ~ x+y, domain(x=c(-2,2), y=c(-2,2)))
```


<!--chapter:end:Fun-multiple-inputs.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Piecewise functions {#fun-piecewise}



::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Fun-4-b-4a", "Identify graphs of a piecewise linear function.")
state_objective("Fun-1C-d", "Construct a hock-stick function by piecewise combination of a constant function and a straight-line function with non-zero slope.")
state_objective("Fun-4-b-4b", "Recognize traditional curly-brace notation for piecewise functions.")
state_objective("Fun-4-b-4c", "Be able to create a piecewise function in R.")
state_objective("Fun-4-b-4d", "Distinguish between continuous and discontinuous functions")
```
:::

Each of our basic modeling functions, with two exceptions, has a domain that is the entire number line $-\infty < x < \infty$. No matter how big or small is the value of the input, the function has an output. Such functions are particularly nice to work with, since we never have to worry about the input going out of bounds.

The two exceptions are:

1. the logarithm function, which is defined only for $0 < x$.
2. some of the power-law functions: $x^p$. 
    - When $p$ is negative, the output of the function is undefined when $x=0$. You can see why with a simple example: $g(x) \equiv x^{-2}$. Most students had it drilled into them that "division by zero is illegal," and $g(0) = \frac{1}{0} \frac{1}{0}$, a double law breaker. 
    - When $p$ is not an integer, that is $p \neq 1, 2, 3, \cdots$ the domain of the power-law function does not include negative inputs. To see why, consider the function $h(x) \equiv x^{1/3}$. 
    
It can be tedious to make sure that you are on the right side of the law when dealing with functions whose domain is not the whole number line. The designers of the hardware that does computer arithmetic, after several decades of work, found a clever system to make it easier. It's a standard part of such hardware that whenever ta function is handed an input that is not part of that function's domain, one of two special "numbers" is returned. To illustrate:
```{r warning=FALSE}
sqrt(-3)
(-2)^0.9999
1/0
```
`NaN` stands for "not a number." Just about any calculation involving `NaN` will generate `NaN` as a result, even those involving multiplication by zero or cancellation by subtraction or division.^[One that does produce a number is `NaN^0`.] For instance:
```{r warning=FALSE}
0 * NaN
NaN - NaN
NaN / NaN
```

Division by zero produces `Inf`, whose name is reminiscent of "infinity." `Inf` infiltrates any calculation in which it takes part:
```{r warning=FALSE}
3 * Inf
sqrt(Inf)
0 * Inf
Inf + Inf
Inf - Inf
1/Inf
```
::: {.scaffolding}
To see the benefits of the `NaN` / `Inf` system let's plot out the logarithm function over the graphics domain $-5 \leq x \leq 5$. Of course, part of that graphics domain, $-5 \leq x \leq 0$ is not in the domain of the logarithm function and the computer is entitled to give us a slap on the wrists. The `NaN` provides some room for politeness. 

Open a sandbox and see what happens when you make the plot.
```{r eval=FALSE}
slice_plot(log(x) ~ x, domain(x=c(-5,5)))
```
:::

In a purely mathematical sense, the problem with functions being undefined over an extended part of a domain has been handled with cunning and imagination. But the solution---the invention of complex numbers---is not our concern here. Instead, we're going to embrace functions that have a domain smaller than the whole number line and see what we can do with them. 

To illustrate, let's use computer notation to create a function whose domain is $x < 1$. To do this, we need a way to write "if," as in, "If $x$ is 1 or greater, return `NaN`." We'll use a function in R that let's ask a TRUE/FALSE question and, depending on the answer, do one or another calculation. The question-answering R function is `ifelse()` whose name is remarkably descriptive. The `ifelse()` function takes three arguments. The first is the question to be asked, the second is the value to return if the answer is "yes," and the third is the value to return for a "no" answer.

```{r warning=FALSE}
g <- makeFun( ifelse(x < 1, x, NaN) ~ x)
slice_plot(g(x) ~ x, domain(x = c(-2, 2)))
```
What takes getting used to here is the expression `x < 1` which is a ***question*** not a statement of fact. There's no standard traditional mathematical notation for questions, although some people use a question mark as in $x \stackrel{?}{<} 1$.

The table shows computer notation for some common sorts of questions.

R notation              | English
------------------------|---------
`x > 2`      | "Is $x$ greater than 2?"
`y >= 3`     | "Is $y$ greater than or equal to 3?"
`x == 4`     | "Is $x$ exactly 4?"
`2 < x & x < 5`| "Is $x$ between 2 and 5?"^[Literally, "Is $x$ both greater than 2 and less than 5?"]
`x < 2 | x > 6` | "Is $x$ either less than 2 or greater than 6?"
`abs(x-5) < 2` | "Is $x$ within two units of 5?"

## Piecewise functions

Having an ability to split up the domain of a function and provide different formula for each of the pieces allows us to
construct ***piecewise functions***. To illustrate, the function $h(x) \equiv |x|$. You'll recognize this as the "absolute value" function. The intuitive algorithm is to "strip the negative sign, if any" from the input. But with the ability to divide the domain into pieces, we gain access to a less mysterious sort of arithmetic operation and can re-write $$h(x) \equiv \left\{ 
\begin{array}{cl} x & \text{for}\ 0 \leq x\\-x & \mbox{otherwise}\end{array}
\right.$$ 
Or, in computer notation
```{r}
h <- makeFun(ifelse(x >= 0, x, -x) ~ x)
```
Note that the absolute value function is built-in to R in the form of the `abs()` function.

Less familiar is the ***Heaviside function*** which has important uses in physics and engineering:

$$\mbox{Heaviside}(x) \equiv \left\{ 
\begin{array}{cl} 0 & \text{for}\ x < 0\\1 & \mbox{otherwise}\end{array}
\right.$$
In computer notation, this is 
```{r}
Heaviside <- makeFun(ifelse(x < 0, 0, 1) ~ x)
```

```{r echo=FALSE}
slice_plot(0 ~ x, domain(x = c(-10, 0)), size=2) %>%
  slice_plot(1 ~ x, domain(x = c(0,10)), size=2) %>%
  gf_labs(title="The Heaviside function") %>%
  gf_lims(y = c(-0.5, 1.5))
```
The vertical gap between the two pieces is called a ***discontinuity***. Intuitively, you cannot draw a discontinuous function ***without lifting the pencil from the paper***. The Heaviside function has a discontinuity at $x=0$.

Similarly, the ***ramp function*** is a kind of one-sided absolute value:
$$\mbox{ramp}(x) \equiv \left\{ 
\begin{array}{cl} x & \text{for}\ 0 \leq x\\0 & \mbox{otherwise}\end{array}
\right.$$
Or, in computer notation:
```{r}
ramp <- makeFun(ifelse(0 < x, x, 0) ~ x)
slice_plot(ramp(x) ~ x, domain(x=c(-3, 3)))
```

A linear combination of two input-shifted ramp functions gives a piecewise version of the sigmoid.
```{r}
sig <- makeFun(ramp(x+0.5) - ramp(x-0.5) ~ x)
slice_plot(sig(x) ~ x, domain(x=c(-3, 3)))
```
::: {r .workedexample}

Figure \@ref(fig:gas-use-2) is a graph of monthly natural gas use in the author's household versus average temperature during the month. (Natural gas is measured in cubic feet, appreviated *ccf*.)

```{r gas-use-2, echo=FALSE}
gf_point(ccf ~ temp, data = Home_utilities, alpha=0.5) %>%
  gf_labs(title="Household natural gas use", x = "Average temperature for the month (deg. F)", y = "Volume of gas used (cubic feet)") %>%
  gf_lims(x = c(0, 85)) %>%
  slice_plot(4.3*ramp(62 - temp) + 15 ~ temp, color="blue", size=2, alpha=0.5)
```
The graph looks somewhat like a hockey stick. A sloping straight-line dependence of ccf on temperature for temperatures below $60^\circ$F and constant for higher temperatures.  The shape originates from the dual uses of natural gas. Gas is used for cooking and domestic hot water, the demand for which is more of less independent of outdoor temperature at about 15 ccf per month. Gas is also used for heating the house, but that's needed only when the temperature is less than about $60^\circ$F.  

We can accomplish the hockey-stick shape with a linear combination of the ramp() function and a constant. The ramp function represents gas used for heating, the constant is the other uses of gas (which are modeled as not depending on temperature. Overall, the model is  $$\text{gas_ccf}(x) \equiv 4.3\,  \mbox{ramp}(62-x)  + 15$$
Even simpler is the model for the other uses of natural gas:
$$\text{other_ccf}(x) \equiv 15$$. 
:::

::: {.todo}
Do heating degree days in integration.

Similarly with power from wind turbines
:::

Our last example concerns a bit of familiar technology: music synthesis. Generating a pure tone electronically is easy done using a sinusoid. Generating a note with rich instrumental timbre can be accomplished by a linear combination of sinusoids. Of course, the note will be localized in time. This could be accomplished by multiplying the sinusoids by a hump function envelope. 

It turns out that our standard hump function, `dnorm()`, does not generate a realistic sound. Instead, a more complicated envelope is used, such as the [ADSR function](https://en.wikipedia.org/wiki/Envelope_(music)) shown in Figure \@ref(fig:ADSR). The function has six (!) parameters: the time the key is pressed, the duration A of the "attack" phase when the sound amplitude is increasing in response to the impulse imposed on the key, a decay of duration D to an output level S that lasts until the key is released, then a decay to zero over duration R. It's reasonable to think of the D and S phases as a piecewise linear approximation to exponential decay.

```{r ADSR, echo=FALSE, out.width="65%", fig.align="center", fig.cap="The ADSR envelope function used in music synthesis consists of 6 pieces including zero output before the key is pressed and after the pulse ends. [Source](https://en.wikipedia.org/wiki/Envelope_(music))"}
knitr::include_graphics("www/adsr.png")
```

```{r echo=FALSE}
adsr <- function(a,d,ts, s,r) {
  function(t) {
    ifelse( 
      t < a, 2*t,
      ifelse(
        t < (a+d), 2*a - ((t-a)/d)*(s - 2*a),
        elseif(
          t < (a+d+ts), s,
               elseif(
                 t > a + d + ts + r, 0,
                s - s*(t-(a + d + ts))/r
                )
        )
      )
    )
  }
}
```





<!--chapter:end:Fun-piecewise.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Magnitudes {#magnitudes}

Undoubtedly you are comfortable with the standard way of writing numbers, for instance 33 or 512 or 1051. Elementary school students master the comparison of such numbers to one another. Which is greater: 512 or 33? Which is less, 1051 or 512? You can answer such questions at a glance because the comparison here can be accomplished simply by counting the number of digits. 1051 has four digits, so it is larger than the three-digit number 512. There are two digits in 33, so it smaller than 512. When two numerals have the same number of digits---say, 337 and 512---you can't answer the "greater than" question by simple counting. Instead, you proceed from left to write and compare the number in each place. So, for 512 and 337, you compare 5 to 3 and ... since 5 is greater than 3, 512 is greater than 337. If the two leading digits are the same, go on to the next digit and so on for all the digits in turn. 

Things were not always this simple. Our number system that uses *place* and *Arabic* numerals is a human invention. An example of an earlier number system is Roman numerals. Here, comparison is hard. For instance, which of these three numbers is bigger?

$$\mbox{MLI or CXII or XXXIII}$$
The typographically shorter number is the largest, and vice versa. Even when two Roman numerals have the same length, it's not trivial to compare them on a place-by-place basis. For instance, IC is about fifteen times bigger than VI, even though I is much smaller than V.

## Counting digits

Digit counting provides an easy, fast way to perform many calculations, at least approximately. What is $\sqrt{10000}$? There are five digits, and the square root of a number will have "half the number of digits." So, $\sqrt{10000} = 100$. What is $10 \times 34$? Easy: 340. Just append the one zero from 10 to the end of 34. What is $1000 \times 13$? Just as easy: 13,000. We even punctuate written numbers with commas and a period in order to facilitate counting digits. 

Imagine having a digit counting function called digit(). It takes a number as input and produces a number as output. We don't have a ***formula*** for digit(), but for some inputs the output can be calculated just be counting. For example:

- digit(10) $\equiv$ 1
- digit(100) $\equiv$ 2
- digit(1000) $\equiv$ 3
- ... and so on ...
- digit(1,000,000) $\equiv$ 6
- ... and on.

The digit() function easily can be applied to the product of two numbers. For instance:

- digit(1000 $\times$ 100) = digit(1000) + digit(100) = 3 + 2 = 5.

Similarly, applying digit() to a ratio gives the difference of the digits of the numerator and denominator, like this:

- digit(1,000,000 $\div$ 100) = digit(1,000,000) - digit(100) = 6 - 2 = 4

## Using digit() to understand magnitude

We haven't shown you the digit() function for anything but the handful of discrete inputs listed above. It was a heroic task to produce the continuous version of digit(). The method is sketched out in \@ref(fractional-digits). 

In practice, digit() is so useful that it's one of our basic modeling functions, 
$$\text{digit(x)} = 2.302585 ln(x)$$ or, in R, `log10()`.

::: {.todo}
Human eye and ear
:::

## Composing $\ln()$ with exponential and power-law functions

## Fractional digits (optional) {#fractional-digits}

So far, we have the digit() function in a tabular form:

input | output
----|-----------
$\vdots$ | $\vdots$
0.01 | -2
0.1 | -1
1   | 0
10   | 1
100 | 2
1000 | 3
10,000 | 4
100,000 | 5
1,000,000 | 6
$\vdots$ | $\vdots$

Here's the point-plot presentation of the table:

```{r log-1st-try, echo=FALSE, fig.cap = "Connecting the data points for the digit function to make a continuous function."}
L10 <- tibble::tribble(
  ~ input, ~ output,
  0.01, -2,
  0.1, -1,
  1, 0,
  10, 1,
  100, 2,
  1000, 3,
  10000, 4,
  100000, 5,
  1000000, 6
) 
ticks <- c(1,10,100,10000,50000,
           100000,200000,500000,750000, 1000000)
gf_point(output ~ input, data = L10) %>%
  gf_labs(title = "The digit function") %>%
  gf_refine(scale_x_sqrt(breaks=ticks, 
                         labels=format(ticks, scientific=FALSE, big.mark=","))) %>%
  gf_line()
```
We've imagined digits() to be a continuous function so we've connected the gaps with a straight line. Now we have a function that has an output for any input between 0.01 and 1,000,000, for instance, 500,000.

The angles between consecutive line segments give the function plotted in Figure \@ref(fig:log-1st-try) an unnatural look. Still, it is a continuous function with an output for any input even if that input is not listed in the table.

Starting around 1600, two (now famous) mathematicians, [John Napier](https://en.wikipedia.org/wiki/Henry_Briggs_(mathematician)) (1550-1617) and [Henry Briggs](https://en.wikipedia.org/wiki/Henry_Briggs_(mathematician)) (1561-1630) had an idea for filling in gaps in the table. They saw the pattern that for any of the numbers $a$ and $b$ in the input column of the table
$$ \text{digit}(a \times b) = \text{digit}(a) + \text{digit}(b)$$
This is true even when $a=b$. For instance, digit(10)=1 and digit(10$\times$ 10) = 2.

Consider the question what is digit(316.2278)? That seems a odd question unless you realize that $316.2278 \times 316.2278 = 100,000$. Since digit(100000) = 5, it must be that digit(316.2278) = 5/2.

Another question: what is digit(17.7828)? This seems crazy, until you notice that $17.7828^2 = 316.2278$. So digit(17.78279) = 5/4.

It happens that for a couple of thousand years mathematicians have known how to compute the square root of any number to a high precision. By taking square roots and dividing by two, it's easy to fill in more rows in the digit()-function table. You get even more rows by noticing other simple patterns like $$\text{digit}(a/10) = \text{digit}(a) -1  \ \ \and \ \ \ \text{digit}(10 a) = \text{digit}(a) + 1$$

Here are some additional rows in the table

input | output | Why?
------|--------|------
316.2278 | 2.5 | From $\sqrt{\strut100,000}$
17.17828 | 1.25 | From $\sqrt{\strut 316.2278}$
4.21696 | 0.625 | From $\sqrt{\strut 17.17828}$
31.62278 | 1.5  | From 316.2278/10
3.162279 | 0.5 | From 31.62278/10

```{r echo=FALSE}
# devtools::install_github("collectivemedia/tictoc")
library(tictoc)
tic()
start <- tibble::tibble(input = 10^{-5:6}, output = -5:6)
for (k in 1:6) {
  root <- start %>% 
    mutate(input = sqrt(input), output = output/2)
  root2 <- start %>%
    mutate(input = sqrt(sqrt(input)), output = output/4)
  root3 <- start %>%
    mutate(input = sqrt(sqrt(sqrt(input))), output = output/8)
  square <- start %>%
    mutate(input = input^2, output = 2*output)
  cube <- start %>%
    mutate(input = input^3, output = 3*output)
  tenths <- start %>%
    mutate(input = input/10, output = output - 1)
  tens <- start %>%
    mutate(input = input*10, output = output + 1)
  start <- rbind(start, root, root2, root3, square, cube, tenths, tens) %>% unique() %>% 
    arrange(input)
}
toc()
```

You can play this game for weeks. We asked the computer to play the game for about half a second and expanded the original digit() table to `r nrow(start %>% filter(output >= -2, output <= 6))` rows.

Figure \@ref(fig:expanded-log) plots the expanded digits() function table.

```{r expanded-log, echo=FALSE}
ticks <- 1:10

gf_point(output ~ input, 
         data = start %>% filter(output <= 1, output >= 0)) %>%
  gf_labs(title = "The digit function with more entries") %>%
  gf_refine(scale_x_continuous(breaks=ticks)) %>%
  gf_line()
```
Now we have a smooth function that plays by the digit rules of multiplication.

Henry Briggs and his assistants did a similar calculation by hand. Their work was published in 1617 as a table. 

```{r briggs-first-page, echo=FALSE, out.width="100%", fig.cap="Part of the first page of Henry Briggs table of logarithms", fig.align="center"}
knitr::include_graphics(normalizePath("www/Briggs-starttable.png"))
```

The table was called the *Chilias prima*, Latin for "First group of one thousand." True to its name, the table gives the output of digits() for the inputs 1, 2, 3, ..., 998, 999, 1000. For instance, as you can see from the top row of the right-most column, digits(67) = 1.82607480270082. 

In everyday speech, 67 has two digits. The authors of *Chilias prima* sensibly didn't use the name "digit()" for the function. They chose something more abstract: "logarithm()". Nowadays, this function is named $\log_{10}()$. In R, the function is called `log10()`.

```{r}
log10(67)
```

Our main use for $\log_{10}()$/`log10()` will be to count digits in order to quickly compare the magnitude of numbers. The difference digits($x$) - digits($y$) tells how many factors of 10 separate the magnitude of the $x$ and $y$.

Another important logarithmic/digit-counting function is $\log_2()$, written `log2()` in R. This counts how many ***binary digits** are in a number. For us, $\log_2(x)$ tells how many times we need to double, starting at 1, in order to reach $x$. For instance, $\log_2(67) = 6.06609$, which indicates that $67 = 2\times 2 \times 2 \times 2 \times 2 \times 2 \times 2^{0.06609}$

$\log_2(x)$ and $\log_{10}(x)$ are proportional to one another. One way to think of this is that they both count "digits" but report the results in different units, much as you might report a temperature in either Celsius or Fahrenheit. For $\log_2(x)$ the units of output are in ***bits***. For $\log_{10}(x)$ the output is in ***decades***.

A third version of the logarithm function is called the ***natural logarithm*** and is denoted $\ln()$ in math notation and simply `log()` in R. We'll need additional calculus concepts before we can understand what justifies calling $\ln()$ "natural."

EXERCISE: Compute $10^y$ to convert a "number of digits" into the number whose digits are being counted. For instance, $10^2.5$ is 316.228. 

EXERCISE: How many binary digits in 64? in 127? 

EXERCISE: $\log_{10}(x)$ and $\log_2(x)$ are proportional to one another. What's the constant of proportionality? 



<!--chapter:end:Magnitudes.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Dimensions {#dimensions}


<!--chapter:end:Dimensions.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# (PART) Block 2: Differentiation {.unnumbered}


<!--chapter:end:Diff-part-marker.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Outline of Block 2 {.unnumbered}

::: {.todo}
This section is for development purposes only. It is not to be included in the released text.
:::

This outline was that established during the May 17-19, 2021 working sessions at USAFA. It's copied directly from the Teams document. I've made some modifications which are noted in [[square brackets]] for deletions and **bold face** for additions..


1. Revisiting Calculating slope
    a. NTI:
    b. Admin:
        i. Sizeable quiz on interlude
    c. Outcomes:
        i. Calculate a slope given two points
        ii. Given a graph of a function, identify graph of its slope function $s(x)$
        iii. Given a graph of the slope function, identify features of the original function
    d. Readings: Chapter \@ref{difference-and-change}

2. Derivatives
    a. NTI:
        i. Explain that limits are the solution to the problem of having an arbitrary $h$ in the slope function
    b. Topics:
        i. Show that there are different definitions of the slope function $s(x)$ that depend on the $h$ selected
        ii. Describe derivative as the limit where $h \rightarrow 0$
        iii. Derivatives of basic modeling functions
        iv. Linear properties of derivatives
        v. [[Derivative is a number; differentiation is an operator]] **We need to introduce operators earlier.
        v. Functions as arguments in R
        vi. Introduce notation: $f'$, $\frac{df}{dx}$, $\partial_x f(x)$, [[$D_x f]]$
        vii. Introduce NaN and Inf
    c. Outcomes:
        i. Find the derivatives of all basic modeling functions
        i. Know the basic properties of differentiation
        i. Continue working graphing a derivative
    d. Readings

3. Relationships between functions and their derivatives Part I
    a. NTIs:
        i. Split students into A/B pairs. A has graph of derivative. B can't see that graph, but can talk to A. B should sketch out the original function based on information from A, who can see B's sketch. Then reveal answer.
    b. Topics:
        i. Argmax/Argmin
        i. Max/min
        i. Critical points
    c. Outcomes: 
        i. Determine a function’s critical points graphically and algebraically
        i. Determine whether a critical point is a max or min
    d. Readings

4. Relationships between functions and their derivatives Part II
    a. NTI:
        i. Curvature. Up like a cup, down like a frown
        ii. 2nd derivative indicates curvature
    b. Topics:
        i. Second derivatives are the slope of the slope
        i. Concavity is the slope of the slope
    c. Readings

5. Relationships between functions and their derivatives Part III
    a. Topics:
        i. Knowing $f'(x_0) = 0$ tells you there is a min/max/saddle at $x_0$
        ii. 2nd derivative to distinguish between the three
        iii. Chain of derivatives
    b. Outcomes:
        i. Be able to find max/min, argmax/argmin of given functions graphically.
        ii. **Construct derivative function and use `findZeros()` to locate the argmax and evaluate the function to get the corresponding max
    c. Readings

6. Continuity, Differentiability, Smoothness, Splines
    a. Topics:
        i. Spline is a piecewise function
        i. Order of continuity
        i. Determine graphically whether a function is continuous or not
        i. Determine order of continuity of a function
    b. Outcomes
        i. **Be able to determine if first derivative of function is continuous from graph of function**
        ii. **Be able to determine if second derivative of function is continuous from graph of function (or by taking the 1st derivative and plotting that)**
        iii. **Be able to demonstrate discontinuities in 3rd derivative of spline**
    c. Readings

7. Differentiation of Products of Functions
    a. Outcomes
        i. Identify when it is necessary to differentiate with product rule
        i. Find the derivative using product rule
    b. Readings

8. Differentiation of Composite Functions
    a. Topics
        i. Identifying a composite function
        i. Chain rule
    b. Homework: Save quotient rule for HW
    c. Outcomes: 
        i. Identify when it is necessary to differentiate with chain rule
        i. Find the derivative using chain rule
    d. Readings

9. Local Polynomial Approximation
    a. NTI:
        i. Killer application of Taylor polynomials when Brook Taylor figured out that tools of Newton developed
    b. Topics:
        i. List the benefits of approximation and specifically the benefits of quadratic approximation as opposed to linear approximation
        ii. [[Average rate of change]] moved to Block 1 slope function 
        iii. Tangent Lines
        iv. Quadratic equations
        v. Factorials
    c. Readings

10. Approximation at a point
    a. NTI:
        i. R exercise in constructing a Taylor polynomial computationally
    b. Outcomes:
        i. Calculate a Taylor polynomial
        ii. Use Taylor polynomials to simplify mathematical operations (derivatives, limits, etc.)



<!--chapter:end:Diff-outline.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# Change relationships

The questions that started it all had to do with motion. There were words to describe speed: fast and slow. There were words to describe force: strong and weak, heavy and light. And there were words to describe location: far and near, long and short. But what were the relationships among these things? And how did time fit in, an intangible quantity that had aspects of location (long and short) and of speed (quick and slow)?

Galileo (1564-1642) started the ball rolling. As the son of a musician and music theorist, he had a sense of musical time, a steady beat of intervals. As a student of medicine in Pisa, he noted that swinging pendulums kept reliable time, regardless of the amplitude of their swing. After accidentally attending a lecture on geometry, he turned to mathematics and natural philosophy. Inventing the telescope, his observations put him on a collision course with the accepted classical truth about the nature of the planets. Seeking to understand gravity, he built an apparatus that enabled him to measure accurately the position in time of a ball rolling down a straight ramp. The belled gates he set up to mark the ball's passage were spaced arithmetically in musical time: 1, 2, 3, 4, .... But the distance between the gates was geometric: 1, 4, 9, 16, .... Thus he established a mathematical relationship between increments in time and increments in position. Time advanced as 1, 1, 1, 1, ... and position as 1, 3, 5, 7, .... He observed that the ***second*** increments of position, the increments of the increments 1, 3, 5, 7, ..., were themselves evenly spaced: 2, 2, 2, ....

Putting these observations in tabular form, and adding columns for the 

- first increment  $y(t) \equiv x(t+1) - x(t)$ and the
- second increment $y(t+1) - y(t)$

$t$ | $x(t)$ | first increment | second increment
----|--------|-----------------|---------------
0   | 0      | 1        | 2
1   | 1      | 3        | 2
2   | 4      | 5        | 2
3   | 9      | 7        | 
4   | 16     |          |

Galileo had neither the mathematics nor the equipment to measure motion continuously in time. So what might be obvious to us now, that position is a function of time $x(t)$, would have had little practical significance to him. But we discover in his first increments of $x$ something very much like our ***slope function***. 

$${\cal D}_t\, x(t) \equiv \frac{x(t + 1) - x(t)}{1}$$
From his data, he observed that ${\cal D}_t\, x(t)$ increases linearly in $t$: $${\cal D}_t x(t) = 2 t + 1$$

Calculating the second increments of $x$ is done by the "slope function of the slope function," which we can call ${\cal D}_{tt}$:
$${\cal D}_{tt} x \equiv {\cal D}_t \left[{\cal D}_t x(t)\right] = 2(t+1) + 1 - (2 t + 1) = 2$$

Newton considered the problem for continuous time rather than Galileo's discrete time. He reframed the slope function from the big increments of the slope operator ${\cal D}_t$ to imagined vanishingly small increments of a operator that we shall denote $\partial_t$ and call ***differentiation***.

The kind of question for which Newton wanted to be able to calculate the answer was, "How to find the function $x(t)$ whose second increment, $\partial_{tt} x(t) = 2$?" His approach, which he called the "method of fluxions," became so important that its name became, simply, "Calculus."

## Slopes and increments.

The mathematical tools of Newton's day are the basis of today's conventional high-school curriculum.^[Perhaps this says something about how well education has kept up with technology.] We have today completely different tools based on the ability to do arithmetic and function evaluation very quickly with computers. We're going to use these new tools to explore the problem of relating the slope-function operator ${\cal D}_t$ to the differential operator $\partial_t$. We have the great advantage of being able to look backwards and so can focus on the functions that experience reveals have been the most widely useful: the basic modeling functions. 

Our goal in this section is to discover what are the slope functions of our basic modeling functions. Recall that the slope-function operator can be written as a ratio of rise-over-run:
$${\cal D}_t x(t) \equiv \frac{x(t+h) - x(t)}{h}$$ where $h$ is the length of the "run." We'll start with two of the basic modeling functions that have considerable "personality": the sinusoid (`sin()`) and the sigmoid (`pnorm()`).

```{r sign-sig, echo=FALSE, out.width="50%", fig.show = "hold", fig.cap="The naked sinusoid and sigmoidal functions. A vertical blue line has been added to mark the input $t=0$", warning=FALSE, message=FALSE}
slice_plot(sin(t) ~ t, domain(t=c(-5, 2*pi))) %>%
  gf_labs(title="Sinusoid") %>%
  gf_vline(xintercept=0, color="blue", alpha=0.5)
slice_plot(pnorm(t) ~ t, domain(t=c(-5, 2*pi))) %>%
  gf_labs(title="Sigmoid") %>%
  gf_vline(xintercept=0, color="blue", alpha=0.5)
```
We'll use the computer to construct the slope functions for the sinusoid and sigmoid, which we'll call `Dsin()` and `Dsigma()` respectively.
```{r}
Dsin   <- makeFun((  sin(t+h) -   sin(t))/h ~ t, h=0.1)
Dsigma <- makeFun((pnorm(t+h) - pnorm(t))/h ~ t, h=0.1)
```

In the tilde expression handed to `makeFun()`, we've identified `t` as the name of the input and given a "small" default value to the `h` parameter. But R recognizes that both `Dsin()` and `Dsigma()` are functions of two variables, `t` and `h`, as you can see in the parenthesized argument list for the functions.
```{r}
Dsin
Dsigma
```
This is a nuisance, since when using the slope functions we will need always to think about `h`, a number that we'd like to describe simply as "small," but for which we always need to provide a numerical value. Let's look at `Dsin()` and `Dsigma()` for a range of values of `h`, as in Figure \@ref(fig:sin-sig-many-h).

```{r sin-sig-many-h, echo=FALSE, out.width="50%", fig.show = "hold", fig.cap="The slope functions of the sinusoid and sigmoid. Each curve shows the slope function for a particular numerical choice of `h`. Both panels show $h=2, 1, 0.5, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001$.", warning=FALSE, message=FALSE}
rain <- rev(hcl.colors(12)[-(1:3)])
slice_plot(Dsin(t, h=1) ~ t, domain(t=c(-5, 2*pi)), color=rain[1], label_text="h=1", label_x=0.56) %>%
  slice_plot(Dsin(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.58, color=rain[1]) %>%
  slice_plot(Dsin(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.54, color=rain[3]) %>%
  slice_plot(Dsin(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.53, color=rain[4]) %>%
  slice_plot(Dsin(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.51, color=rain[5]) %>%
  slice_plot(Dsin(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.5) %>%
  slice_plot(Dsin(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.49) %>%
  slice_plot(Dsin(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.48) %>%
  slice_plot(Dsin(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.44) %>%
  gf_labs(title="Slope functions of sinusoid") %>%
  gf_vline(xintercept=0, color="blue", alpha=0.5)
slice_plot(Dsigma(t, h=1) ~ t, domain(t=c(-5, 2*pi)), label_text="h=1", label_x=.28, color= rain[2]) %>%
  slice_plot(Dsigma(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.24, color=rain[1]) %>%
  slice_plot(Dsigma(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.54, color=rain[3]) %>%
  slice_plot(Dsigma(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.53, color=rain[4]) %>%
  slice_plot(Dsigma(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.51, color=rain[5]) %>%
  slice_plot(Dsigma(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.5) %>%
  slice_plot(Dsigma(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.49) %>%
  slice_plot(Dsigma(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.48) %>%
  slice_plot(Dsigma(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.44) %>%
  gf_labs(title="Slope functions of sigmoid") %>%
  gf_vline(xintercept=0, color="blue", alpha=0.5)
```

Some observations from this numerical experiment:

1. As $h$ gets very small, the slope function doesn't depend on the exact value of $h$.

    This will provide a way for us, eventually, to discard $h$ so that the slope function will not need an $h$ argument.
    
2. For small $h$, we have ${\cal D}_t \sin(t) = \sin(t + \pi/2) = \cos(t)$. That is, taking the slope function of a sinusoid gives another sinusoid, shifted left by $\pi/2$ from the original. Or, in plain words, the cosine is the slope function of the sine.
3. For small $h$, we have ${\cal D}_t \text{pnorm}(t) = \text{dnorm(t)}$. That is, the hump function is the slope function of the sigmoid function.

You can confirm these last two statements by comparison with the original functions, especially the alignment of the peaks of the slope functions with respect to the peak of the sinusoid and the half-way point of the sigmoid.

Now consider the slope functions of the logarithm and exponential functions.

```{r log-exp-many-h, echo=FALSE, out.width="50%", fig.show = "hold", fig.cap="The slope functions of the logarithm and exponential.", warning=FALSE, message=FALSE}
rain <- rev(hcl.colors(12)[-(1:3)])
Dlog <- makeFun((log(x+h) - log(x))/h ~ x)
Dexp <- makeFun((exp(x+h) - exp(x))/h ~ x)
slice_plot(Dlog(t, h=1) ~ t, domain(t=c(0.01, 2)), color=rain[2], label_text="h=1", label_x=0.8) %>%
  slice_plot(Dlog(t, h=2) ~ t, domain(t=c(0.01, 2)), alpha = 1, label_text="h=2", label_x=.90, color=rain[1]) %>%
  slice_plot(Dlog(t, h=0.5) ~ t, domain(t=c(0.02, 2)), alpha = 1, label_text="h=0.5", label_x=.7, color=rain[3]) %>%
  slice_plot(Dlog(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.6, color=rain[4]) %>%
  slice_plot(Dlog(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.5, color=rain[5]) %>%
  slice_plot(Dlog(t, h=0.001) ~ t, domain(t=c(0.05, 2)), alpha = 1, color=rain[6], label_text="h=0.001", label_x=.4) %>%
  slice_plot(Dlog(t, h=0.0001) ~ t, domain(t=c(0.1, 2)), alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.3) %>%
  slice_plot(Dlog(t, h=0.00001) ~ t, domain(t=c(0.125, 2)), alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.2) %>%
  slice_plot(Dlog(t, h=0.000001) ~ t, domain(t=c(0.125, 2)),alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.1) %>%
  gf_labs(title="Slope functions of logarithm") %>%
  gf_vline(xintercept=0, color="blue", alpha=0.5) %>%
  gf_lims(y=c(0,8))
slice_plot(Dexp(t, h=1) ~ t, domain(t=c(-2, 2)), label_text="h=1", label_x=.4, color= rain[2]) %>%
  slice_plot(Dexp(t, h=2) ~ t, alpha = 1, label_text="h=2", label_x=.3, color=rain[1]) %>%
  slice_plot(Dexp(t, h=0.5) ~ t, alpha = 1, label_text="h=0.5", label_x=.5, color=rain[3]) %>%
  slice_plot(Dexp(t, h=0.1) ~ t, alpha = 1, label_text="h=0.1", label_x=.55, color=rain[4]) %>%
  slice_plot(Dexp(t, h=0.01) ~ t, alpha = 1, label_text="h=0.01", label_x=.6, color=rain[5]) %>%
  slice_plot(Dexp(t, h=0.001) ~ t, alpha = 1, color=rain[6], label_text="h=0.001", label_x=.65) %>%
  slice_plot(Dexp(t, h=0.0001) ~ t, alpha = 1, color=rain[7], label_text="h=0.0001", label_x=.75) %>%
  slice_plot(Dexp(t, h=0.00001) ~ t, alpha = 1, color=rain[8], label_text="h=0.00001", label_x=.85) %>%
  slice_plot(Dexp(t, h=0.000001) ~ t, alpha = 1, color=rain[9], label_text="h=0.000001", label_x=.9) %>%
  gf_labs(title="Slope functions of exponential") %>%
  gf_vline(xintercept=0, color="blue", alpha=0.5) %>%
  gf_lims(y=c(0,8))
```
These numerical experiments with the logarithm and exponential functions are more evidence that, as $h$ gets small, the slope function doesn't depend on $x$. And, we find that:

- For small $h$, the slope function of the logarithm is a power-law function: ${\cal D}_t \ln(t) = \frac{1}{t}$.
- For small $h$, the slope function of the exponential is the exponential itself: ${\cal D}_t e^x = e^x$.

You can confirm these by evaluating the slope function of the exponential at $t=0$ and $t=1$, and the slope function of the logarithm at $t= 2, 1, 1/2, 1/4, 1/8.$

Such numerical experiments on the other naked modeling functions reveal a pattern: the slope function of the naked modeling functions tend to be similar to other naked modeling functions.

```{r child="D-naked-functions.Rmd"}
```



With this list of experimentally determined slope functions (for small $h$) we're ready to start using slope functions without having to use left-shift combinations like $f(t+h) - f(t)$.

## Slopes and motion

Having worked out a theory of slope functions, Newton was ready to express the laws of motion in continuous time. He did this by expressing position as $x(t)$, and then familiar concepts velocity and force in terms of slope functions of position and the "quantity of matter," which we call "mass." 

- Velocity is the slope function of position: $v(t) \equiv {\cal D}_t x(t)$.
- Net force is the slope function of velocity times mass: $F(t) \equiv m {\cal D}_t v(t)$

To take mass out of the formulation, we give a name specifically to the slope function of velocity; we call it ***acceleration***. 

- Acceleration is the slope function of velocity: $a(t) \equiv {\cal D}_t v(t)$.

With acceleration as a concept, we can define net force as mass times acceleration.

::: {.why}
We used **net force** as the quantity we related to mass and the slope function of velocity. There are different sources of forces which add up and can cancel out. Famously, Newton formulated the ***law of universal gravitation*** which ascribed the force between masses as proportional to the product of the two masses and inversely proportional to the square of the distance between them. But a mass on a table has no net force on it, since the table pushes back (push = force) on the mass to cancel out the force due to gravity. "Net force" takes such cancellation into account.
:::

## Differentiation

***Differentiation*** is a process of transforming a function to produce another function. There are several traditional notations for differentiation of a function named $f()$, for instance:

- Leibnitz: $\frac{df}{dx}$
- Partial: $\frac{\partial f}{\partial x}$
- One-line: $\partial_x f$
- Newton: $\dot{f}$
- Prime: $f'$

In this book, we will mainly use the one-line notation, $\partial_x f$, but it means exactly the same as the Leibnitz and Partial notations, which are much more widely used in textbooks. 

If you've studied calculus before, you have likely seen the $f'$ notation. This is admirably concise but is only viable in a narrow circumstance: functions that take a single input. What $f'$ leaves out is a means to specify a crucial aspect of differentiation, the **with-respect-to variable**. The general situation for differentiation involves functions of one or more variables, for example, $g(x, y, z)$. For such functions, you need to specify which is the with-respect-to variable. For instance, we can differentiate $g()$ three different ways, each way incrementing one or another of the three inputs:

$$\partial_z g(x, y, z) \equiv \frac{g(x, y, z+h) - g(x, y, z)}{h}\\ 
\ \\
\partial_x g(x, y, z) \equiv \frac{g(x+h, y, z) - g(x, y, z)}{h}\\ 
\ \\
\partial_y g(x, y, z) \equiv \frac{g(x, y+h, z) - g(x, y, z)}{h}$$

At this point in your studies, you haven't seen why you might choose to differentiate a function with respect to one variable or another. That will come in time. But we want to set you up with notation that won't narrow your options.

Both the Leibnitz and Partial notations are explicit in identifying the function and the with-respect-to-variable. For example, using the Partial differentiation notation, the three ways of differentiating our example function $g(x, y, z)$ are labeled :

$$\frac{\partial f}{\partial x},\ \ \ \frac{\partial f}{\partial y},\ \ \text{and}\ \ \frac{\partial f}{\partial z}$$

Our R/mosaic computer differentiation is longer but explicit:
```r
D(g(x, y, z) ~ x)
D(g(x, y, z) ~ y)
D(g(x, y, z) ~ z)
```
Notice that the R/mosaic operator is named `D()` and that it is a function. It follows the same pattern as `makeFun()` or `slice_plot()` or `contour_plot()`: the first argument is a tilde expression, for instance `g(x, y, z) ~ x`, which identifies the mathematical function to work with (`g()`) and the name of the with-respect-to input to that function. The R/mosaic notation makes it clear that differentiation is an ***operation*** on a function. The `D()` operator takes a function as input and produces as output **another function**. We've seen similar behavior with, say, `slice_plot()`, which takes a function as input and produces graphics as output. Both `D()` and `slice_plot()` need to know the identity of the with-respect-to variable as well as the function to work with. What's why both pieces of input are packaged into a tilde expression.

::: {.why}
We're calling `D()` an ***operator*** rather than a ***function***. The reason is purely for communication with other people. There are so many "functions" in a calculus course that we thought it would be helpful to distinguish between the kinds of functions that take quantities as input and produce a quantity as output, and the functions that take a *function* as input and produce a *function* as output. Both sorts are called "functions" in R terminology. But a sentence like, "Differentiation is a function that takes a function as input and produces a function as output," true though it be, is dizzying.
:::

::: {.takenote}
It is a fact of mathematical and scientific life that a variety of notations are used for differentiation. To some extent, this reflects historical precedence and, to be honest, nationalistic European politics of the 18th century. To make sense of mathematical writing in the many areas in which calculus is used, you have to recognize all of them for what they are. Your skill will be enhanced if you also memorize the names of the different styles. It's not all that different from the pattern in English of having multiple words for the same sort of object, for instance: car, automobile, junker, ride, wheels, crate, jalopy, limo, motor car, horseless carriage. 

In the days when carriages where pulled by horses, the phrase "horseless carriage" made a useful distinction. Today, when horses are rarely seen on the road, it make sense to trim down the notation to its essentials: ~~~horseless~~~ **car**~~~iage~~~. Think of $\partial_x$ as this sort of minification.^[Yes, "minification" is a word!]   
:::

## Using h

In working with differentiation, we introduced a quantity $h$ and then ignored it, saying that it doesn't really matter so long as it is "small." A reasonable person might wonder what "small" really means, and why we needed to introduce $h$ in the first place if we were eventually going to ignore it. 

One reason is that "small" and "zero," although related, are different. For example, refering to the slope functions `Dsin()` and `Dsigma()` that we created in an early example in this chapter, we see that setting $h$ to zero does not get us where we need to be:

```{r}
Dsin(t=1, h=0)
Dsigma(t=0, h=0)
```

In `NaN`, you can hear the echo of your fourth-grade teacher reminding you that it is illegal to divide by zero.

Think of $h$ as the solvent in paint. You don't want the solvent once the paint is on the wall; wet paint is a nuisance. But getting the paint from the can to the wall absolutely needs the solvent. 

We used the solvent $h$ earlier in the chapter in the numerical experiments that led us to the derivatives of the naked modeling functions, for instance $\partial_x e^x = e^x$ or $\partial_x \sin(x) = \cos(x)$.  Eventually, we'll construct an $h$-free theory of differentiation, reducing the process to a set of algebraic rules in which $h$ never appears. With this as our goal, let's continue using $h$ for a while to find some additional useful facts about derivatives.

***Linear combination*** is one of the ways in which we make new functions from existing functions. As you recall, linear combination involves ***scaling*** a function and ***adding*** the scaled functions. We can easily use $h$ to show what is the result of differentiating a linear combination of functions. We'll use $f(x)$ and $g(x)$ as the names that could stand for any function whatsoever. And we'll let $b$ be the name of a scalar. First, let's figure out what is $\partial_x\, b f(x)$, Going back to writing $\partial_x$ in terms of a slope function:
$$\partial_x\, b\,f(x) = \frac{b\, f(x + h) - b\,f(x)}{h}\\
\ \\
= b \frac{f(x+h) - f(x)}{h} = b\, \partial_x f(x)$$
In other words, if we know the derivative $\partial_x\, f(x)$, we can easily find the derivative of any scaled version of $f()$.

Now consider the derivative of the sum of two functions, $f(x)$ and $g(x)$:
$$\partial_x\, \left[f(x) + g(x)\right] =\\
\ \\
=\frac{\left[f(x + h) + g(x + h)\right] - \left[f(x) + g(x)\right]}{h} = \\
\ \\
= \frac{\left[f(x+h) -f(x)\right] + \left[g(x+h) - g(x)\right]}{h}\\
\ \\
= \frac{\left[f(x+h) -f(x)\right]}{h} + \frac{\left[g(x+h) - g(x)\right]}{h}\\
\ \\
= \partial_x\, f(x) + \partial_x\, g(x)$$

Using these two rules together, we can differentiate any linear combination of functions in terms of the differentiated functions themselves:

$$\partial_x\ \left[\strut a_1 g_1(x) + a_2 g_2(x) + a_3 g_3(x) + \cdots\right] =\ \ \ \ \ \ \ \ \ \ \\
\ \\
\ \ \ \ \ \ \ \ \ a_1 \partial_x\, g_1(x) + a_2 \partial_x\, g_2(x) + a_3 \partial_x\, g_3(x) + \cdots$$



Because of the way that $\partial_x$ can be "passed through" a linear combination, mathematicians say that differentiation is a ***linear operator***. Consider this new fact about differentiation as a down payment on what will eventually become a complete theory telling us how to differentiate a ***product of two functions*** or the ***composition of two functions***. 

EXERCISES: Simple drill on this




## Functions and perception

<!--
As you know, a function takes one or more inputs and returns a value as output. The functions we examine in CalcZ take *quantities* as inputs and return a *quantity* as an output. 
The algorithm that forms the body of the function describes arithmetic and other calculations that can turn the inputs into the output. 

On the other hand, we can also use ***tables*** as functions. With a table, you specify the input, look up that input in one of the colums of the table which brings you to the right row. Then read out from that row the value in another column to be the output. The quantitative operation needed for table lookup is simple comparison. The floor/corridor/door metaphor describes table lookup as well as function evaluation. 

In the previous block, we constructed functions to represent the patterns seen in data. In one example, we constructed a function $g(t) = A + B e^{-k t}$ to represent the temperature of water cooling in a mug as a function of time. In another example, we summarized the pattern of rising and falling tides. 

It's common sense that data is stored in tables. But we could easily represent any smooth mathematical function, such as our basic modeling functions, as a table look-up problem. Indeed, in the era before computers, many mathematical functions were used in exactly this manner: a printed table in which a person could search for a match to the input and retrieve a value for the output.

::: {.todo}
[Picture of some nice old table.]
:::

In the computer era, we still routinely represent functions this way: data stored in computer files. For instance, an MP3 file is not much more than a sequence of numbers that record a complicated function of time: the air pressure variations of sound. Similarly, digital images record functions of $x$ and $y$ over a limited domain. Given $x$ and $y$ as input, you can look up the output by going to the right pixel.

-->

We humans perceive the world using sight and sound and our other senses. Both sight and sound are highly complex biophysically, but the *process* can be broken down trivially into a relationship: *reality $\longrightarrow$ perception*. 

Perception takes place in your mind and brain, extended by sensors such as the retina of the eye and cochlea of the ear. There is considerable scientific understanding of how the retina and cochlea work, but perception itself is still somewhat mysterious and involves the interaction of sensor input with our memory and other cognitive processes.

Still, for both hearing and sight, we can analyze the *reality $\longrightarrow$ perception* process by adding an intermediate layer:

- For sight: *reality $\longrightarrow$ image $\longrightarrow$ perception*
- For hearing: *reality $\longrightarrow$ sound $\longrightarrow$ perception*

We know a tremendous amount about sound and image. For instance, we can reliably and effectively create or synthesize realistic sounds and images. Indeed, we can study the *image $\longrightarrow$ perception* process in isolation. And, as so often happens in science, we study the process by creating a mathematical representation of it.

This starts with the mathematical representation of image and sound. Since this is a calculus course, you won't be surprised when we propose that good mathematical representations are functions:

- Sound: a function of one variable, $t$ for time.
- Image: a function of two variables: the $x$ and $y$ coordinates of an image.

A sound or an image are often represented as multiple functions, for example the left and right channel of stereo sound, or the red, green, and blue planes of a digital image. But a single channel or plane is able to represent sounds or images with considerable fidelity. For simplicity, we'll stick to that: sound(t) and image(x,y).

What about the 3rd dimension? The retina is effectively a two-dimensional sensor. The third dimension comes in through the *reality $\longrightarrow$ image* part of the overall process.

Consider the image in Figure \@ref{fig:sand-1}. It is a picture of some indentations in a small area of sand, about two inches wide in the middle of a hiking trail. The dots are individual grains of sand.

```{r sand-1, echo=FALSE, out.width="50%", fig.align="center"}
knitr::include_graphics("www/sand-furrows.png")
```

Can you see three almost parallel furrows? How about the small crater in the upper left?

You can see the individual grains of sand because they contrast sharply with their neighbors.


You can think of the surface of the sand as a function of $x$ and $y$. It's lower in some places and higher in others. But, in fact, you can't see the height of an individual point in the photograph. In the right light, you wouldn't notice the furrows at all. But the way the picture is lighted, raking sunlight from the left, the *reality $\longrightarrow image* process translates the surface into broad regions of brightness and shadow. In the light regions, the surface slants toward the sun. In the shadows, the surface slants away from the sun. 

What you're mainly seeing in the photo is the ***slant*** or ***slope*** of the surface. The raking light has transformed *elevation* as a function of $x$ and $y$ into *slant* as a function of $x$ and $y$ and then encoded the slant as brightness. Ironically, it would be much less effective to present the surface height directly as an image 

The moral here is that sometimes the data in a function is not in the right form for us to extract useful information. But by transforming that data to represent contrast or difference or slope, the information can be revealed.

This Block is about transforming functions to show difference and slope. Such transformation, accomplished by mathematics rather than the raking light of the sun, can take a pattern that we're presented with and turn it into another pattern that can tell us what we want to know.

## Differencing

Our basic tool for showing difference and slope is a remarkable simple operator that takes a function as input. For a function with one input, the operator ${\cal D}()$ is defined as 
$${\cal D}(f) \equiv \frac{f(x + 0.1) - f(x)}{0.1}$$
Notice that the ${\cal D}()$ operator returns a **function**. The output is a linear combination of the input function $f()$ and a shifted version of $f()$.

For a function with two inputs, there are two versions of ${\cal D}$:
$${\cal D}_x(f) \equiv \frac{f(x+0.1, y) - f(x, y)}{0.1}$$

$${\cal D}_y(f) \equiv \frac{f(x, y+0.1) - f(x, y)}{0.1}$$


::: {.workedexample}
Suppose that $f(x)\equiv x^2$. What is the function ${\cal D}(f)$?

$$
{\cal D}(f) \equiv \frac{f(x+0.1) - f(x)}{0.1}\\ =
10\left((x+0.1)^2 - x^2\right)\\
=10 x^2 + 2x + 0.1 - 10x^2\\ = 2 x + 0.1$$
:::

::: {.workedexample}
Suppose that $f(x) \equiv 2 x + y$. Find the function ${\cal D(f)}$.

The linear combination will be 
$$\frac{1}{0.1}\left(\left[2 (x + 0.1) + y\right] - \left[2 x + y\right]\right) =\\ \\
20 x + 2 + 10y - \left[20 x - 10y\right] = 2$$
:::

```{r}
f <- makeFun(2 *x * y ~ x + y)
D(f(x, y) ~ x)
```


## Slope of an image

```{r}
set.seed(137)
diam <- 8
f <- rfun(~ x+ y)
contour_plot(f(x, y) ~ x + y, domain(x=c(-diam, diam), y=c(-diam,diam)), contours_at = -100, fill_alpha=1, contour_color=NA, n_fill=500, fill_scale=ggplot2::scale_fill_grey())
fx <- D(f(x, y) ~ x)
contour_plot(fx(x, y) ~ x + y, domain(x=c(-diam,diam), y=c(-diam, diam)), contours_at = -100, fill_alpha=1, contour_color=NA, n_fill=500, fill_scale=ggplot2::scale_fill_grey())
```

## Instantaneous rate of change

::: {.objectives latex-data=""}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-2b", "Distinguish the \"average rate of change\" from the \"instantaneous rate of change\".")
```
:::


Imagine a graph of the position of a car along a road as in Figure \@ref{fig:stop-and-go2}. 

::: {.todo}
This graph appears in an exercise in Fun-slopes
:::

Over the course of an hour, the car travelled about 25 miles. In other words, the ***average*** speed is 25 miles/hour: the *slope* of the red line segment. Given the traffic, sometimes the car was stopped (time C), sometimes crawling (time D) and sometimes much faster than average (time B).  

```{r stop-and-go2, echo=FALSE}
f <- rfun(~ t, seed=105, n=5)
raw <- function(t) 
        f(t) - t - 30*dnorm(t, 0, 3) + 60*dnorm(t,7,1)
speed <- function(t) {
    pmax(4*raw(20*(t-.5)), 0)
}
position <- antiD(speed(t) ~ t)
Pts <- tibble::tibble(
    t = c(0, 0.19, 0.4, 0.54, 0.65, 1),
    y = position(t) + 2,
    label=c("", "A", "B", "C", "D", "")
)
Intervals <- tibble::tribble(
    ~t0, ~ t1, ~color,
    0, 1, "red",
    # .54, .65, "orange",
    # .19, .4, "green",
    # .4, .54, "brown",
) %>%
    mutate(y0=position(t0), y1=position(t1))
slice_plot(position(t) ~ t, domain(t = c(0, 1)), size=2) %>%
    gf_labs(y = "x(t): Position from start of trip (miles)",
            x = "Time since start (hours)") %>%
    gf_text(0 ~ t, data = Pts, label=~label, color="blue") %>%
    gf_segment(2 + y ~ t + t, data = Pts[-6,], color="blue") %>%
    gf_segment(y0 + y1 ~ t0 + t1, data = Intervals, color=~color, alpha=0.5, size=3) %>%
    gf_refine(scale_color_identity())
```


The car's speedometer shows the speed at each moment---or ***instant***---of the trip. As you can see in Figure \@ref{fig:stop-and-go}, the speed varies and is sometimes less than the average speed, sometimes greater, and occasionally equal to the average speed over the trip. The general term for the kind of quantity presented by the speedometer is the ***instantaneous rate of change*** of the position function with respect to the input to that function. 

Figure \@ref{fig:instant-speed} shows the instantaneous rate of change of position with respect to time.

```{r instant-speed, echo=FALSE}
f <- rfun(~ t, seed=105, n=5)
raw <- function(t) 
        f(t) - t - 30*dnorm(t, 0, 3) + 60*dnorm(t,7,1)
speed <- function(t) {
    pmax(4*raw(20*(t-.5)), 0)
}
Pts <- tibble::tibble(
    t = c(0, 0.19, 0.4, 0.56, 0.65, 1),
    y = speed(t) + 5,
    label=c("", "A", "B", "C", "D", "")
)
slice_plot(speed(t) ~ t, domain(t=c(0,1)), npts=500) %>%
    gf_labs(y = "Instantaneous rate of change (miles/hour)", 
           x = "Time since start of trip.") %>%
    gf_text(2 ~ t, data = Pts, label=~label, color="blue") %>%
    gf_segment(5 + y ~ t + t, data = Pts[-1,], color="blue")
```
The two graphs in Figures \@ref{fig:stop-and-go} and \@ref{fig:instant-speed} show exactly the same car trip. The presentation of the data in the different graphs makes it easy to see some things and hard to see others. For instance, figuring out when the car is at a stand-still is harder in the position-vs-time graph than in the speed-vs-time graph. This is very much in the spirit of the sand-furrows example at the start of this chapter: it's much easier to perceive the furrows because the lighting highlights areas sloping toward the sun as bright and areas sloping away from the sun as dark. In Figure \@ref{fig:instant-speed} we're not using light-and-dark for the display. Instead, we're showing the instantaneous speed using the vertical axis. 

Recall that the interval between $t_B$ and $t_C$ had an ***average rate of change*** of about 39 miles-per-hour. Looking at the ***instantaneous rate of change*** tells the story differently: at time $t_B$ the car was accelerating to about 60 miles-per-hour. Then it gradually slowed, coming to a stop just before time $t_C$.

Figure \@ref{fig:stop-and-go} shows the function $\mbox{position}(t)$. Figure \@ref{fig:instant-speed} shows a different function, $\mbox{speed}(t)$. Although the two functions are different, they are intimately related: $\mbox{speed}(t)$ is the ***instantaneous rate of change*** of $\mbox{position}(t)$. 

Two central operations in calculus are:

1. Given a function $f(t)$, find the function $g(t)$ giving the instantaneous rate of change of $f()$. This process of deriving $g(t)$ from $f(t)$ is called ***differentiation***.
2. Given a function $g(t)$, find the $f(t)$ of which $g(t)$ is the instantaneous rate of change. This process of finding $f()$ given $g()$ is called ***anti-differentiation***.

::: {.workedexample latex-data=""}

The context of the situation being modeled determines whether it's appropriate to look at an average rate of change or an instantaneous rate of change. Figure \@ref{fig:instant-tree} shows the instantaneous rate of change in the volume of wood.

```{r echo=FALSE, eval=!exists("tree_growth")}
# This is a redefinition from Fun-slopes.Rmd for when we do partial compilation.
tree_volume <- makeFun(1000*pnorm(year, mean=15, sd=10)^3 ~ year)
# do the deriv numerically to avoid bug in derivative 
# of normal function.
tree_growth <- numD(tree_volume(year) ~ year)
```

```{r instant-tree, echo=FALSE}
slice_plot(tree_growth(year) ~ year, domain(year=c(0,50)))
```
It's tempting to look to the year where the growth rate is highest as the optimal harvest year. This is a mistake. The volume of wood being harvested is the ***accumulated growth*** not the instantaneous growth. Even though the instantaneous growth is higher at year 23 than year 30, it's still pretty high at year 30 and waiting until then (or later) accumates those years of higher-than-average growth. That's why the ***average rate of growth*** is a better thing to look at to determine optimal harvest time. Still, is it the right thing to look at? 

Between year 30 and 32, there is hardly any change in the value of the average-rate-of-change function. It's increasing a little, but is it really worthwhile to wait? One argument is that at year 29 you already have a valuable resource: wood that could be money in the bank. If the money were in the bank, you could invest it and earn more money *and* at the same time get a new seedling in the ground to start its growth. You're doing two things at once. Efficient!

To know what is the best year for harvest from this point of view, you want to calculate the effective "interest rate" on the present amount of wood that you earn in the form of new wood. That interest rate is the ratio of the *instantaneous* rate of growth of new wood divided by the amount of existing wood. Figure \@ref{fig:tree-interest} shows this function:

```{r tree-interest, echo=FALSE, warning=FALSE}
slice_plot(100* tree_growth(year)/tree_volume(year) ~ year,
           domain(year=c(0,50))) %>%
  gf_labs(y = "Growth relative to volume (%/year)") %>%
  gf_refine(scale_y_log10()) %>%
  gf_hline(yintercept=5, color="blue")
```
Early in the tree's life, the growth is high compared to the volume of the tree. That's because the tree is small. As the years pass, the tree gets bigger. Even though the rate of growth increases through year 23, the accumulated volume increases even faster, so there is a fall in the rate of return. 

The best time to harvest is when the annual "interest rate" paid by the growing tree falls to the level of the next best available investment. Suppose that investment would pay 10% per year. Then harvest the tree at year 24. If the next best investment paid only 5% (blue horizontal line), the harvest should be made at about year 29.
:::

<!--chapter:end:Diff-intro.Rmd-->

```{r include=FALSE, cache=FALSE}
library(mosaic)
library(mosaicCalc)
library(glue)
library(math141Z) # REPLACE THIS WHEN PACKAGES ARE RE-ALIGNED
library(here) # for file locations



exercise_file <- function(day,  file_name, course="141Z") {
    path=glue::glue("Exercises/DD-{course}-{day}/{file_name}")
    here(path)
}

if (!exists("objective_list"))
  objective_list <- list()

add_objective <- function(ID, text) {
  objective_list[[length(objective_list) + 1]] <<-
                      list(ID = ID, text = text)
}

state_objective <- function(ID, text) {
  add_objective(ID, text)
  format_objective(list(ID=ID, text=text))
}

format_objective <- function(obj) {
  knitr::asis_output(glue::glue("#. **[{obj$ID}]** *{obj$text}*\n\n"))
}

show_objectives <- function() {
  Tmp <- lapply(objective_list, tibble::as_tibble) %>%
    bind_rows()
  readr::write_csv(Tmp, file="objective-list.csv")

  lapply(objective_list, format_objective) %>%
    unlist() %>%
    paste(collapse="\n") %>%
    knitr::asis_output()
}

# A blank image for spacing
BlankImage <- gf_blank(hp ~ wt, data=mtcars) %>% gf_theme(theme_void())
```
# h and derivatives



We already established, by numerical experiment, the result of differentiating the naked modelings functions. To summarize,

```{r child="D-naked-functions.Rmd"}
```

A mathematician might prefer to replace the word "established" in the first sentence of this table with a weaker word "motivated" or "proposed." This is entirely fair. Indeed, let's do another experiment that will cause us to wonder just how solid are the conclusions presented in the table above.

Recall that we can easily define the slope function for any $f(x)$, for example the slope function of $\sin(x)$:
```{r}
Dsin <- makeFun((sin(x+h) - sin(x))/h  ~ x)
```
In justifying the entry for $\sin()$ in the table, we plotted `Dsin()` using small `h`, for instance, `h=0.000001`. It would be comforting to continue the experiment with even smaller `h`. Doing so, we discover a problem.


```{r Dsin-small-h, echo=FALSE, fig.cap="Graphing the slope function of $\\sin(x)$ for small enough $h$ produces a result inconsistent with the table for the naked modeling functions. Instead of producing a $\\cos(x)$, we get a ragged function."}
slice_plot(Dsin(t, h=0.000000000000001) ~ t, domain(t=c(-5,2*pi)))
```
Things get even worse for smaller $h$ still, as you can confirm for yourself using a computing sandbox.

It turns out that the reason for this behavior is the way in which computer arithmetic has been engineered. To demonstrate the non-mathematical behavior of computer arithmetic, consider what happens when we add and subtract using 1 and a number that is small compared to 1.

```{r}
options(digits=20)
0.000000000000000000000001
1 + 0.000000000000000000000001
1 - 0.000000000000000000000001
1 + 0.000000000000000000000001 - 1
```

Using technical computing successfully at a professional level requires some understanding of the ways in which computer arithmetic differs from mathematical arithmetic, And the R/mosaic `D()` operator has been constructed with these considerations in mind. But our purpose here is not to push the computer beyond it's arithmetic limits but to demonstrate that the differentiation table for the naked modeling functions is correct.

## The $h$ framework

In the end, all of the work we're going to do with $h$ will have a simple result: confirming the facts presented in the differentiation table. In a court of law, that confirmation could be established by appeal to established authority. For instance, look in any calculus textbook and you'll see the same facts as in the table. (And if you find an exception, you can be sure it's a typographical error!)

For most people, mathematical proof is not much different from appealing to established authority. Not everyone is skilled at following the deductive steps of a proof and almost everyone has been tricked into accepting a step that is not logically valid. And everyone makes mistakes. 

For the benefit of such people, rather than proving the facts in the differentiation table, we're going to reconstruct, hopefully in a fun way, the  framework developed by the mathematics community over roughly two centuries that enabled mathematicians to satisfy themselves that the methods used by the pioneers could be justified beyond doubt.

The basic problem, which everyone always agreed on, is that it's not proper to set $h$ to zero. The dispute is about how to handle $h$ in such a way that it can be held non-zero and yet give results where $h$ has evaporated as if it were never there. 

To illustrate how this can be done, consider the algebra of the slope function for $g(x) \equiv x$:

$${\cal D}_x g(x) \equiv \frac{g(x+h) - g(x)}{h} = \frac{(x+h) - x}{h} = \frac{h}{h} = 1$$ 
The above steps are completely justified for any $h$ whatsoever, so long as $h \neq 0$. So we have a formula ${\cal D}_x\, x = 1$ which will be correct no matter whether $h$ is big or small.



In contrast, consider the slope function of $f(x) \equiv x^2$:
$${\cal D}_x f(x) \equiv \frac{f(x+h) - f(x)}{h} = \frac{(x+h)^2 -x^2}{h} = \\
\ \\
= \frac{x^2 + 2hx + h^2 - x^2}{h} = \frac{2 h x + h^2}{h} = 2 x + h$$
Again, we have a formula which is correct for any $h$ whatsoever, so long as $h$ is not zero. Unfortunately, $h$ is still present in the formula. In differentiating $x^2$ we want to make $h$ go away.


I like to think of $h$ as a kind of *tire iron*, a small tool used to stretch the bead of a bicycle tire in order to pull it over the wheel rim. 

```{r echo=FALSE, out.width="30%", fig.cap="A tire iron in use", fig.align="center"}
knitr::include_graphics("www/tire-iron.png")
```

Once the tire iron has done its job, its removed and you would never know that it was ever there (except that the tire is now successfully mounted on the wheel).

But this is calculus, not bicycle mechanics. How do we know that removing the tire iron isn't damaging the mathematical wheel? 

Still in the spirit of having fun, let's try a more serious metaphor... imagining $h$ is actually a central character in a calculus play. The character $h$ is in the middle of the story but *never appears in the play*, like the missing character Godot in the famous play *[Waiting for Godot](https://en.wikipedia.org/wiki/Waiting_for_Godot#Godot)*. 

We said that $h$ in the slope function ${\cal D}_x x^2 = 2 x + h$, so long as $h$ is small, plays both a central role and has hardly any effect. An economizing director re-writes the play to take $h$ out of it, setting $h=0$ in the formula $2 x + h$: a non-speaking, offstage role.

We've already seen using legitimate algebra that $${\cal D}_x g(x) = 2 x + h$$ Re-writing by replacing $h$ with 0 streamlines the play, turning ${\cal D}_x x^2 = 2x +h$ from a dialog involving both $x$ and $h$ into a monologue with $h$ absent: $$\partial_x x^2 = 2 x$$ Simple.

And yet ... the director gets a letter from the Bit Players Union. 

> *We observe that you have eliminated the role of $h$ in the final production version of $\partial_x g(x)$. This is a violation of Union regulations. Recall that the basis for $\partial_x g(x)$ is the slope function ${\cal D}_x g(x)$. The slope function is defined as a ratio: $${\cal D}_x g(x) \equiv \frac{g(x+h) - g(x)}{h}$$ Eliminating $h$ entirely by replacing her with zero is a **division by zero error** forbidden by Article 3.16§B¶2 of the Unified Laws of Arithmetic. We ask that you comply with this Article by re-instating the role of $h$ in all evaluations of ${\cal D} g(x)$.*

Reading this, the director calls her lawyer. Is there a loophole for removing $h$ without breaking the mathematical prohibition on dividing by zero? 

::: {.forinstructor latex-data=""}
In 1734, famous philosopher [George Berkeley](https://en.wikipedia.org/wiki/George_Berkeley) (1685-1753) published a long-titled book: *The Analyst: A Discourse Addressed to an Infidel Mathematician: Wherein It Is Examined Whether the Object, Principles, and Inferences of the Modern Analysis Are More Distinctly Conceived, or More Evidently Deduced, Than Religious Mysteries and Points of Faith*. In *The Analyst*, Berkeley took issue with the arguments of that time that it is legitimate to divide by $h$ when, ultimately, $h$ will be replaced by zero. Calling $h$ an "evanescent increment," he asked, 

> *"And what are these same evanescent Increments? They are neither finite Quantities nor Quantities infinitely small, nor yet nothing. May we not call them the ghosts of departed quantities?"*

Interesting, Berkeley believed that the ghost of $h$ yielded correct results. His objection was that the framers of calculus had made two, canceling errors. 

> *"[B]y virtue of a two fold mistake you arrive, though not at science, yet truth."*

Berkeley was saying that calculus had not yet been put on a solid logical foundation. It was to be more than a century after Berkeley's death until this work was accomplished. Once accomplished, the results that had been claimed true all along were confirmed.
:::

The loophole involves a bit of legalistic cover, something like a corporation. As you may know, a corporation is a legal structure that makes if feasible for people to invest without being subject to unlimited liability. You bought stock in a company that later accidentally caused a catastrophe? The company will go out of business and your stock will be worthless. But you are legally obliged to fix the damage: you liability is limited. Such companies identify themselves as such with the legal suffix "Inc." and they are beholden to the state in certain ways, such as the requirement to pay taxes on profits.

In calculus, the equivalent of "Inc" is $\lim_{h \rightarrow 0}$. By prepending this to a calculation, you are allowed to carry out arithmetic operations such as dividing by $h$ without concern about liability for dividing by zero. You can perform any algebraic operations so long as they are legitimate when $h \neq 0$. For instance, it is taken as entirely correct to say:

$$\lim_{h\rightarrow 0}\frac{h}{h} = 1$$
Another privilege for the users of $\lim_{h\rightarrow 0}$ is that, at the end of the algebraic derivation, they are entitled to replace $h$ with zero so long as no divide by zero is required. The endpoint of the above is 1, where $h$ doesn't even appear. But consider
$$\lim_{h \rightarrow 0} \frac{(x + h)^2 - x^2}{h} = \\
\ \\
= \lim{h \rightarrow 0} 2 x + h$$

In the last step, an $h$ appears. But we are entitled to take the result of the derivation, $2 x + h$, and replace the $h$ by zero. Doing so doesn't entail any illegitimate operation such as dividing by zero.

At it's most basic, you're entitled to state $$\lim_{h\rightarrow 0} h = 0$$ which, to be honest, looks like nothing more than common sense.

## Approximations to naked modeling functions

Let's take a close look at two of our naked modeling functions: $e^x$ and $\sin(x)$. By "close" I mean very near to $x=0$. You already know that $e^0 = 1$ and $\sin(0) = 0$. But consider "very small" $x$, in the spirit of $\lim_{x\rightarrow 0}$. 

The fundamental approximations are these:

$$e^x \approx 1 + x\\
\ \\
\sin(x) \approx x\\
\ \\
\cos(x) \approx 1 - x^2$$

Figure \@ref(fig:small-x-naked} shows the naked functions along with the approximations. We're interested in "small" $x$. The left panel shows the functions for $-1 \leq x \leq 1$ and the right panel zooms in for small $x$, taking "small" in an arbitrary but everyday sense of, say, less than 0.01.

```{r small-x-naked, echo=FALSE, fig.show="hold", out.width="50%", fig.cap="Comparing the naked modeling functions exp(x), sin(x), and cos(x) to their simple approximations for small x. The functions are drawn as a broad gray line; the approximations are a thin blue line."}
options(digits=5)
slice_plot(exp(x) ~ x, domain(x=c(-1, 1)), size=3, alpha=0.25) %>%
  slice_plot(1 + x ~ x, color="blue") %>%
  gf_labs(title = "Exponential")
slice_plot(exp(x) ~ x, domain(x=c(-0.01, 0.01)), size=3, alpha = 0.25) %>%
  slice_plot(1+x ~ x, color = "blue")

slice_plot(sin(x) ~ x, domain(x=c(-1, 1)), size=3, alpha = 0.25) %>%
  slice_plot(x ~ x, color="blue") %>%
  gf_labs(title = "Sin")
slice_plot(sin(x) ~ x, domain(x=c(-0.01, 0.01)), size=3, alpha = 0.25) %>%
  slice_plot(x ~ x, color = "blue")
  
slice_plot(cos(x) ~ x, domain(x=c(-1, 1)), size=3, alpha=0.25) %>%
  slice_plot(1 - x^2 ~ x) %>%
  gf_labs(title = "Cos")
slice_plot(cos(x) ~ x, domain(x=c(-0.01, 0.01)), size=2, alpha = 0.25) %>%
  slice_plot(1 - x^2 ~ x, color = "blue")
```

Near $x=0$, the graph of the approximation is dead on center of the corresponding naked modeling function.

The approximations will break down for very small $x$ because none of the approximations involve dividing by $x$. And remember that the approximations are only good when $x$ is small!

Consider the result of differentiating $e^x$. The slope function is

$${\cal D}_x e^x \equiv \frac{e^{x+h} - e^x}{h} = e^x \left[\frac{e^h - 1}{h}\right]$$
Let's examine $\frac{e^h - 1}{h}$ for very small $h$. This is where we will use the approximation for $e^x$, but we'll write it with $h$ as the argument: $e^h \approx 1 + h$. Plugging this in to the bracketed quantity, we have $$\lim_{h \rightarrow 0}\frac{e^h - 1}{h} = \lim_{h\rightarrow 0}\frac{1 + h - 1}{h} = 1$$ Overall, this means $$\partial_x e^x = \lim_{h\rightarrow 0}{\cal D}_x e^x = e^x$$

Now to demonstrate that $\partial_x \sin(x) = \cos(x)$ a fact that is often used in calculus. Only calculus teachers need to know how to perform this demonstration, so the following is just FYI:.

The demonstration builds on a formula for the sine of the sum of two quantities:

$$\sin(x + h) = \sin(x)\cos(h) + \cos(x)\sin(h)$$

With this, we can simplify the slope function of $\sin$:
$${\cal D}_x \sin(x) \equiv \frac{\sin(x+h) - \sin(x)}{h} =\\
\ \\
= \frac{1}{h} \left[\strut\sin(x)\cos(h) + \cos(x)\sin(h) - \sin(x) \right]\\
\ \\
= \frac{1}{h} \left[\strut \sin(x) \left[\cos(h) -1\right] + \cos(x)\sin(h)\right]$$
Plugging in the approximations $\sin(h) = h$ and $\cos(h) = 1 - h^2$ we get
$${\cal D}_x \sin(x) = \frac{1}{h} \left[\strut\sin(x)(- h^2)  +\cos(x)h\right] = \\
\ \\
= \cos(x) - h \sin(x)$$
With the division by $h$ safely removed, we cn now apply the limit:
$$\partial_x \sin(x) = \lim_{h\rightarrow 0} \left[\strut \cos(x) - h \sin(x)\right] = \cos(x) - \sin(x) \left[\lim_{h\rightarrow 0}h\right] = \cos(x)$$

::: {.takenote}
We write the differentiation operator (with respect to input $x$) as $\partial_x$. We wrote the slope-function operator (again, with respect to $x$) as ${\cal D}_x$. 

The slope-function operator ${\cal D}_x$ is only a stepping stone on the path toward the real destination: differentiation. We're defining differentiation as a limit:

$$\partial_x\, f(x) \equiv \lim_{h\rightarrow 0} {\cal D}_x\, f(x)$$
In practice, however, we perform differentiation whenever possible not with the limit definition, but with the *consequences* of the limit definition, such as $\partial_x e^x = e^x$ or $\partial_x \sin(x) = \cos(x)$. Sometimes these consequences are called the ***rules of differentiation***. These rules are important particularly for carrying out differentiation using paper and pencil, but if a computer is available, the rules have been mastered for you by software. Since such software is widely available, we're going to step away from them for a bit. The next chapters present some of the uses for differentiation and contexts in which differentiation is used in constructing new modeling functions. 
:::



<!--chapter:end:Diff-h-naked.Rmd-->

