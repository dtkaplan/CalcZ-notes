# Optimization {#optim-and-shape}


To "optimize" means to make something as good as possible with the available resources. Optimization problems are common in science, logistics, industry, and any other area where one seeks the best solution to a problem. Some everyday examples: `r mark(2440)`

- How much salt to add to a stew. Stews can be too salty, or they can be not salty enough. Somewhere in the middle is the optimum.
- When to harvest trees being grown for lumber. Harvest too soon and you might be losing out on the prime growing years. Wait too long and trees will have settled in to slow growth, if any.
- Walking up too steep a slope is tiring and slows you down; that's why hiking trails have switchbacks. When the switchbacks are too shallow, it takes a long time to cover the distance. What's the most efficient angle to enable hikers to get up the hill in the shortest time. `r mark(2445)`

## Structure of the problem

In an optimization problem, there is one or more input quantity whose value you have to choose. The amount of salt; the years to wait from planting to harvesting a tree; the angle of the trail with respect to the slope. We'll call this the ***decision quantity***.  `r mark(2450)`

Similarly, there is one or more output quantity that you value and want to make as good as possible. The taste of the stew; the income produced by selling the lumber; the time it takes to walk up the hill. The output quantity is called the ***objective***.  `r mark(2455)`

The model that relates to inputs to the objective output is called the ***objective function***. Solving an optimization problem---once the modeling phase is complete---amounts to finding a value for the decision quantity (the input to the objective function) that produces the best level of the objective (the output from the objective function). `r mark(2460)`

Sometimes the objective is something that you want to ***minimize***, make as small as possible. In the hiking trail problem, we seek to minimize the amount of time it takes to walk up the trail. Sometimes you want to ***maximize*** the objective, as in the wood-harvest problem where the objective is to harvest the most wood per year. `r mark(2465)`

Mathematically, maximization and minimization are the same thing. Every minimization problem can be turned into a maximization problem by putting a negative sign in front of the objective function. To simplify the discussion, in talking about finding the solution to an optimization problem we'll imagine that the goal is to maximize. But keep in mind that many circumstances in the real world, "best" can mean to minimization.   `r mark(2470)`

The solution you seek in a maximization problem is called the ***argmax***. This is a contraction of two words: the *argument* (that is, input) that produces the *maximum* output. (For minimization, the solution is the ***argmin.) `r mark(2475)`

Once you have found the argmax you can plug that value into the objective function to find the value of the output. That value is the ***maximum***. 

::: {.takenote}
People often talk about "finding the maximum." This is misleading. The setup for an optimization problem is:

1. Construct (that is, model) the objective function.
2. Now that you know the objective function, find the input to that function---that is, the **argmax**---that produces the maximum output.
:::

To illustrate the setup of an optimization problem, imagine yourself in the situation of a contest to see who can shoot a tennis ball the farthest into a field with a slingshot. During the contest, you will adjust the vertical angle of launch, place the ball into the slingshot's cradle, pull back as far as possible, and let go. To win the contest, you need to optimize how you launch the ball. `r mark(2480)`

The objective is the distance travelled by the ball. For simplicity, we'll imagine that the velocity of the ball at release is fixed at $v_0$. You'll win or lose based on the angle of launch you choose. `r mark(2485)`

Before you head out into the field to experiment, let's do a bit of preparation. We'll model how far the ball will travel (horizontally) as a function of the angle of launch $\theta$ and the initial velocity $v_0$. `r mark(2490)`

The mathematics of such problems involves an area called ***differential equations***, an important part of calculus which we'll come to later in the course. Since you don't have the tools yet, we'll just state a simple model of how long the ball stays in the air.
$$\text{duration}(v_0, \theta) = 2 v_0 \sin(\theta)/g$$ $g$ is the acceleration due to gravity, which is about $9.8 \text{m}\text{s}^{-2}$, assuming that the contest is being held on Earth.

The horizontal distance travelled by the tennis ball will be $$\text{hdist}(v_0, \theta) = \cos(\theta) v_0\, \text{duration}(v_0, \theta) = 2 v_0^2 \cos(\theta)\sin(\theta) / g$$
Our objective function is hdist(), and we seek to find the argmax. The input $v_0$ is (we have assumed) fixed, so the decision quantity is the angle $\theta$.

The best choice of $\theta$ will make the quantity $\cos(\theta)\sin(\theta)$ as large as possible. So in finding the argmax, we don't need to be concerned with $v_0$ or $g$. 

Finding the argmax can be accomplished simply by plotting the function $\cos(\theta)\sin(\theta)$. We'll implement the function so that the input is in units of *degrees*.

```{r ball-theta, fig.cap="The distance travelled by a ball launched at an angle of $\\theta$$, according to the simple model is duration of flight and distance travelled.", fig.show="hold"}
f <- makeFun(cos(pi*theta/180)*sin(pi*theta/180) ~ theta)
slice_plot(f(theta) ~ theta, domain(theta=c(0,90)))
slice_plot(f(theta) ~ theta, domain(theta = c(40, 50)))
```
From the graph, especially the zoomed-in version, you can read off the argmax as $\theta = 45^\circ$.

Finding the argmax solves the problem. You may also want to present your solution by saying what the value of the output of hdist() is when the argmax is given as input. You can read off the graph that the maximum of $\cos(\theta)\sin(\theta)$ is 0.5 at $\theta = 45^\circ$, so overall the distance will be $v_0^2 / g$ `r mark(2495)`


::: {.todo}
Review Exercise: What is the dimension of $v_0^2 / g$?
:::

## Interpreting the argmax

The graphical solution given to the slingshot problem is entirely satisfactory. Whether that solution will win the contest depends of course on whether the model we built for the objective function is correct. There are potentially important things we have left out, such as air resistence. `r mark(2500)`

Solving the optimization problem has prepared us to go out in the field and test the result. Perhaps we'll find that the real-world optimum angle is somewhat steeper or shallower than $\theta = 45^\circ$. `r mark(2505)`

Besides the argmax, another important quantity to read from the graph in Figure \@ref(fig:ball-theta) is the ***precision*** of the argmax. In strict mathematical terms, the argmax is exactly 45 degrees. But in practical terms, it may not matter so much to the outcome if we are a little away from $45^\circ$. For example, according to the model, any angle in the range $40^\circ < \theta < 50^\circ$ would produce an output that is within 1% of the distance reached at the argmax. `r mark(2510)`

Contests are won or lost by margins of less than 1%, so you should not casually deviate from the argmax. On the other hand, $45^\circ$ is the argmax of the *model*. Reality may deviate from the model. For instance, suppose that air resistance or wind might might have an effect of about 1% on the distance. You can expect that such factors might change the optimal angle by as much or more than $\pm 5^\circ$. `r mark(2515)`

## Derivatives and optimization

We're now going to reframe the search for the argmax and it's interpretation in terms of  derivatives of the objective function with respect to the decision quantity ($\theta$ in the slingshot problem). For a function of one variable, this will not be an improvement from the look-at-the-graph technique to find the argmax. A genuine reason to use derivatives is to set us up in the future to solve problems with more than one variable, where it is hard to draw or interpret a graph. Also, describing functions in the language of derivatives can help us think more clearly about aspects of the problem, such as the precision of the argmax. `r mark(2520)`

With a graph such as Figure \@ref(fig:ball-theta), it's easy to find the argmax; common sense carries the day. So it won't be obvious at first why we are going to take the following approach:

Let's denote an argmax of the objective function $f(x)$ by $x^\star$. 
Let's look at the derivative $\partial_x f(x)$ in the neighborhood of $x^\star$. Referring to Figure \@ref(fig:ball-theta), where $x^\star = 45^\circ$, you may be able to see that $\partial_x f(x^\star)$ is zero; the line tangent to the function's graph at $x^\star$ is flat.   `r mark(2525)`

Seen another way, the slope of $f(x)$ to the left of $x^\star$ is positive; moving a tiny bit to the right (that is, increasing $x$ by a very small amount, leads to an increase in the output $f(x)$. Intuitively, as you approach the peak of a hill, you are walking uphill.) Just to the right of $x^\star$, the slope of $f(x)$ is negative; as you reach the top of a hill and continue on, you will be going downhill. So the derivative function is positive on one side of $x^\star$ and negative on the other, suggesting that it crosses zero at the argmax. `r mark(2530)`

Inputs $x^\star$ such that $\partial_x f(x^\star) = 0$ are called ***critical points***. Why not call them simply argmaxes? Because a the slope will also be zero at an argmin. And it's even possible to have the slope be zero at a point that's neither an argmin or an argmax. `r mark(2535)`

```{r ds1-1, echo=FALSE, results="markup"}
etude2::etudeQ(
  "Consider the function $f(x) \\equiv x^3$. Confirm that the value of the derivative $\\partial_x f(x = 0)$ and so $x^\\star = 0$ is a critical point. Which sort of critical point is $x^\\star=0$? (Hint: Draw the graph of $f(x)$ near $x=0$ to see what's going on.)" ,
  "An argmax" = "But $f(0) < f(x > 0)$, so $x^\\star=0$ can't be an argmax.",
  "An argmin" = "But $f(x < 0) < f(0)$, so $x^\\star=0$ can't be an argmin.",
  "+Neither+",
  random_answer_order = FALSE
)
```

At this point, we know that values $x^\star$ that give $\partial_x f(x^\star) = 0$ are "critical points," but we haven't said how to figure out whether a given critical point is an argmax, an argmin, or neither. This is where the behavior of $\partial_x f(x)$ *near* x=x^\star$ is important. If $x^\star$ is an argmax, then $\partial_x f(x)$ will be positive to the left of $x^\star$ and negative to the right of $x^\star$; walk up the hill to get to $x^\star$, at the top the hill is flat, and just past the top the hill has a negative slope. `r mark(2540)`

For an argmin, changing $x$ from less than $x^\star$ to greater than $x\star$;  you will be walking down into the valley, then level at the very bottom $x=x^\star$, then back up the other side of the valley after you pass $x=x^\star$. Figure \@ref(fig:d2-hill) shows the situation.  `r mark(2545)`

```{r d2-hill, echo=FALSE, fig.show="hold", out.width="50%", fig.cap="Top row: An objective function near an argmax (left) and an argmin (right). Bottom row: The derivative of the objective function"}
f <- makeFun(sin(2*pi*x/4)~ x)
f2 <- D(f(x) ~ x)
slice_plot(f(x) ~ x, domain(x=c(0.5, 1.5))) %>%
  gf_labs(title="Argmax x* = 1") %>%
  gf_vline(xintercept = 1, color="blue", alpha = 0.25, size=2) %>%
  gf_text(.7 ~ 1, label="x*", size=6, color="blue")
slice_plot(f(x) ~ x, domain(x=c(2.5, 3.5))) %>%
  gf_labs(title="Argmin x* = 3") %>%
  gf_vline(xintercept = 3, color="blue", alpha = 0.25, size=2) %>%
  gf_text(-1 ~ 3, label="x*", size=6, color="blue")
slice_plot(f2(x) ~ x, domain(x=c(0.5, 1.5))) %>%
  gf_labs(title="Argmax x* = 1", y="Derivative of f(x)") %>%
  gf_vline(xintercept = 1, color="blue", alpha = 0.25, size=2) %>%
  gf_text(-1 ~ 1, label="x*", size=6, color="blue")
slice_plot(f2(x) ~ x, domain(x=c(2.5, 3.5))) %>%
  gf_labs(title="Argmin x* = 3", y="Derivative of f(x)") %>%
  gf_vline(xintercept = 3, color="blue", alpha = 0.25, size=2) %>%
  gf_text(-1 ~ 3, label="x*", size=6, color="blue")
```

The bottom row of graphs in Figure \@ref(fig:d2-hill) shows the derivative of the objective function $f(x)$, that is, $\partial_x f(x)$. You can see that for the argmax of $f(x)$, the derivative $\partial_x f(x)$ is positive to the left and negative to the right. Similarly, near the argmin of $f(x)$, the derivative $\partial_x f(x)$ is negative to the left and positive to the right.  `r mark(2550)`

Stated another way, the derivative $\partial_x f(x)$ has a positive slope near an argmin and a negative slope near an argmax. 

Just as we differentiate $f(x)$ to find it's slope, so to find the slope of the function $\partial_x f(x)$ we can differentiate it. The result is called the ***second derivative***. We could write it $\partial_x \left[\partial_x f(x)\right]$, but for brevity we write it $\partial_{xx} f(x)$.  `r mark(2555)`

The second derivative of the objective function $f(x)$ at a critical point $x^\star$ is what tells us whether the critical point is an argmax, an argmin, or neither. 
Critical point $x^\star$ | $\partial_x f(x^\star)$ | $\partial_{xx} f(x^\star)$
-----------------------|---------------------|-------------------
argmax | 0 | negative
argmin | 0 | positive
neither| 0 | 0

```{r ds1-2, echo=FALSE, results="markup"}
etude2::etudeQ(
  "Returning to the function $f(x) \\equiv x^3$,  find the value of the second-derivative $\\partial_{xx} f(x^\\star)$ evaluated at the critical point $x = x^\\star = 0$. Which of these is $\\partial_{xx} f( x=0$? " ,
  "Negative" = "But you established in the previous exercise that the critical point $x^\\star=0$ is neither an argmin nor wan argmax.",
  "Positive" = "But you established in the previous exercise that the critical point $x^\\star=0$ is neither an argmin nor wan argmax.",
  "+Zero+",
  random_answer_order = FALSE
)
```


::: {.why}
When we differentiate a function $f(x)$, we produce a new function that we can call anything we like. To help readers follow the thread of the story, it's nice to name the new function $\partial_x f(x)$. That signals clearly to the reader the origins of the new function with respect to the original function $f(x)$. `r mark(2560)`

In words, $\partial_x f(x)$ is often called the ***derivative*** of $f(x)$ (with respect to x). To "derive" is a very general term and could mean just about any way of creating something new from something old. In calculus, "derivative" always means "created by differentiation." Perhaps it would have been better if history had led us to call $\partial_x f(x)$ by the name "differentiated $f(x)$" or "the differential function of $f(x)$."  `r mark(2565)`
:::

Graphically, we can read the second derivative $\partial_{xx} f(x)$ as the slope of the first derivative $\partial_x f(x)$ or as the ***concavity*** of the function $f(x)$ itself. When $\partial_{xx} f(x) < 0$, then $f(x)$ is ***concave down*** (a frown). Likewise, when $\partial_{xx} f(x) >0$ the $f(x)$ is ***concave up*** (a smile). When $\partial_{xx} f(x) = 0$, then $f(x)$ has no curvature. `r mark(2570)`

::: {.takenote}
To this point, we've translated features of functions that are evident on a graph into the language of derivatives:

i. The **slope** of a function $f(x)$ at any input $x$ is the **value** of the derivative function $\partial_x f(x)$ at that same $x$.
ii. The  **concavity** of a function $f(x)$ at any input is the **slope** of the derivative function, that is, $\partial_x f(x)$.
iii. Putting (i) and (ii) together, we get that the **concavity** of a function $f(x)$ at any input $x$ is the **value of the second derivative** function, that is, $\partial_{xx} f(x)$.
iv. At an argmax $x^\star$ of $f(x)$, the value of the derivative function $\partial_x f(x^\star)$ is zero and the value of the second derivative function $\partial_{xx} f(x^\star)$ is **negative**. (The situation at an argmin is similar, the derivative of the objective function is zero and the second derivative is **positive**.) `r mark(2575)`
:::

```{exercise, concavity, name="02/concavity"}
```
<details>`r MC_counter$reset()` `r knitr::knit_child(exercise_file("02", "concavity.Rmd"))`</details>


```{exercise, name="tree-loves-fish"}
```
<details>`r MC_counter$reset()` `r knitr::knit_child("Exercises/Diff/tree-loves-fish.Rmd")`</details>


::: {.todo}
Pick up on the Lorenz curve `{.intheworld}` in block 1. Compute the concavity of a lorenz function, showing that it's everywhere positive. Then examine the concavity of a composition of lorenz functions to determine if that is necessarily everywhere positive. `r mark(2580)`
:::


::: {.workedexample}
What's the critical point?

You're familiar with the quadratic polynomial: $$g(x) = a_0 + a_1 x + a_2 x^2$$
The graph of a quadratic polynomial is a ***parabola***, which might be concave up or concave down. As you know, a parabola has only one critical point, which might be an argmin or an argmax.

Let's find the critical point. We know that the critical point is $x^\star$ such that $\partial_x g(x_0) = 0$. Since we know how to differentiate a power law, we can see that
$$\partial_x g(x) = a_1 + 2 a_2 x$$ and, more specifically, at the critical point $x^\star$ the derivative will be
$$a_1 + 2 a_2 x^\star = 0$$
The above is an equation, not a definition. It says that whatever $x^\star$ happens to be, the quantity $a_1 + 2 a_2 x^\star$ must be zero. Using plain old algebra, we can find the location of the critical point $$x^\star = -\frac{a_1}{2 a_2}$$ `r mark(2585)`

:::


```{exercise, name="optim-violet"}
```
<details>`r MC_counter$reset()` `r knitr::knit_child("Exercises/Diff/optim-violet.Rmd")`</details>

```{exercise, name="optim-pink"}
```
<details>`r MC_counter$reset()` `r knitr::knit_child("Exercises/Diff/optim-pink.Rmd")`</details>


```{exercise, name="optim-red"}
```
<details>`r MC_counter$reset()` `r knitr::knit_child("Exercises/Diff/optim-red.Rmd")`</details>

```{exercise, name="optim-blue"}
```
<details>`r MC_counter$reset()` `r knitr::knit_child("Exercises/Diff/optim-blue.Rmd")`</details>

```{exercise, name="optim-purple"}
This is probably misplaced. and the `plot_lens()` function would have to be provided.
```
<details>`r MC_counter$reset()` `r knitr::knit_child("Exercises/Diff/optim-purple.Rmd")`</details>
::: {.todo}
Exercises involving computing derivatives
:::


::: {.todo}
Set the revenue maximizing price per Cournot's demand vs price curve shown earlier in Figure \@ref(fig:cournot-demand).

As described in "Marshallian Cross Diagrams and
Their Uses before Alfred Marshall:
The Origins of Supply and Demand Geometry", 
Thomas M. Humphrey, ECONOMIC REVIEW. MARCH-APRIL 1992 [Link to PDF](https://www.richmondfed.org/~/media/richmondfedorg/publications/research/economic_review/1992/pdf/er780201.pdf)

To find the revenue-maximizing price, Cournot
differentiated the revenue function $R(p) = pD(p) = pF(p)$
with respect to price. (Note that the product rule applies here.) He obtained the expression $pF'(p) + F(p)$, where $F'$ is the derivative of the demand function $F(p)$. Setting this expression equal to zero as required for a maximum and rearranging, he got $p = -F(p)/F'(p)$, which says that the revenue maximizing price must equal the ratio of the quantity demanded to the slope of the demand curve at that price.
:::



::: {.takenote}
In the previous discussion, we used phrases such as "every function has a derivative," or that "any function can be approximated" by a second-order polynomial. For most practical purposes, this is true. In the next chapter, however, we'll set mathematical conditions on "every" and "any" that will let us see when there can be an exception. `r mark(2590)`
:::


::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-10a", "Understand and distinguish between max (min) and argmax (argmin)")
state_objective("Deriv-10b", "Visually identify max (min) and argmax (argmin) in graphs of functions of one and two variables.")
```
:::


::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-10c", "Find max and min of a quadratic function using calculus and algebra")
```
:::


::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-11a", "Convert a word problem into an Objective function and constraint.")
state_objective("Deriv-11b", "Convert a multivariate objective function to a univariate objective function using the constraint")
```
:::

## Be practical!

Decision making is about choosing among alternatives. In some engineering or policy contexts, this can mean finding the setting for an input variable that will produce the "best" outcome. For those who have studied calculus, it's natural to believe that calculus-based techniques for optimization are the route to solving the problem.

This is a calculus course, so you may be surprised to find out that the next few classes are about developing justified skepticism about the use of calculus-based techniques. Or, to be more precise, we want to emphasize that the optimization techniques covered in the first semester are **only part** of a broader set of techniques for real-world decision-making problems.

In this session, we're going to: 

1. Review the calculus-based techniques and reduce them to an algorithm that can be implemented as an operator like differentiation or integration.
2. Discuss the properties of a mathematical optimum to help you understand the illusion of "best" and how to quantify what is "good enough." 


In a later Daily Digital, we'll introduce the concept of **incommensurate** outcomes and show you that the concept of a "single best solution" is often logically incoherent.

Such talk goes against the grain of many decision makers. They argue with some reason that "only best is good enough," or that looking for "good enough" solutions is a sign of laziness. There are some situations where it is indeed worthwhile to seek to reach an absolute maximum. It takes judgment and experience to recognize when you are in a situation and when you are not---which is more often the case.

## Functions, derivatives and maxima

Recall the setting for calculus-type maximization. You have a function with one or more inputs, say, $f(x)$ or $g(x,y)$ or, often, $h(x, y, z, \ldots)$ where $\ldots$ might be standing for tens or hundreds or thousands of variables or more.

If you can graph the function (feasible for one- or two-input functions), you can often easily scan the graph by eye to find the peak. The calculus-based techniques were developed for situations where such graphing is not possible and, instead, you have a formula for the function. (Such occasions are of great theoretical interest but not all that common in practice.) The basis of the calculus techniques is the observation that, at the argmax of a smooth function, the derivative of the function is 0. (This applies equally well to functions of multiple variables, where we say that the "gradient is zero" which means that each and every component of the gradient vector is zero.)

As an example, consider a problem that often appears in calculus textbooks. ([Example](https://www.dummies.com/education/math/calculus/calculate-the-optimum-volume-of-a-soup-can-practice-question/) You have been tasked to design a container for a volume V of liquid. It is desired to make the weight of the container as little as possible. (This is a *minimization* problem, then.) In classical textbook fashion, you are told that the container is to be a cylinder made out of a particular metal of a particular thickness. 

This is a lovely geometry/calculus problem. Whether it is relevant to any genuine, real-world problem is another question.

```{r echo=FALSE, fig.align="center", out.width="20%"}
knitr::include_graphics("www/cylinder.png")
```

Using the notation in the diagram, the volume and surface area of the cylinder is $$V(r, h) \equiv \pi r^2 h \ \ \ \mbox{and}\ \ \ A(r, h) \equiv 2 \pi r^2 + 2 \pi r h$$

Minimizing the weight of the cylinder is our objective (according to the problem statement) and the weight is proportional to the surface area. Since the volume $V$ is given (according to the problem statement), we want to re-write the area function to use volume:

$$h(r, V) \equiv V / \pi r^2 \ \ \ \implies\ \ \ A(r, V) = 2 \pi r^2 + 2 \pi r V/\pi r^2 = 2 \pi r^2 + 2 V / r$$
Suppose $V$ were specified as 1000 liters. A good first step is to choose appropriate units for $r$ to make sure the formula for $A(r, V)$ is dimensionally consistent. Suppose we choose $r$ in cm. Then we want $V$ in cubic centimeters (cc). 1000 liters is 1,000,000 cc. Now we can plot a slice of the area function:

```{r}
A <- makeFun(2*pi*r^2 + 2*V/r ~ r, V=1000000)
slice_plot(A(r) ~ r, domain(r=c(10, 100))) %>%
  gf_labs(x = "radius (cm)")
```

You can easily see that the minimum is near $r=50$cm. Since $h(r,V) = V/\pi r^2$, the required height of cylinder will be near $10^6 / \pi 50^2 = 127$cm. And, as is clear from the graph, the function's derivative is zero at the optimal $r$.

In calculus courses, the goal is often to find a **formula** for the optimal radius as a function of $V$. So we differentiate the objective function---that is, the area function for any $V$ and $r$ with respect to $r$,
$$\partial_r A(r, V) = 4 \pi r - 2 V / r^2$$
Setting this to zero (which will be true at the optimal $r^\star$) we can solve for $r$ in terms of $V$:
$$4 \pi r^\star - 2 V/(r^
\star)^2 = 0 \ \ \ \implies\ \ \  (r^\star)^3 = \frac{1}{2\pi} V \ \ \ \implies\ \ \  r^\star = \sqrt[3]{V/2\pi}$$

For $V = 1,000,000 cm^3$, this gives $r^\star = 54.1926 cm$ which in turn implies that the corresponding height $h^\star = V/\pi (r^\star)^2 = 108.3852 cm$. 

We've presented the optimum $r^\star$ and $h^\star$ to the nearest **micron**. Does that make sense? Think about it for a moment before reading on. 

## Meaningless precision

A good rule of thumb in modeling is this: "If you don't know what a sensible precision is for reporting your result, you don't have a complete grasp of the problem." Here are two reasonable ways to sort out a suitable precision.

1. Solve a closely related problem which for many practical purposes would have been equivalent.
2. Look at how big a change in the output of the objective function is produced by a change from the argmax.

Approach (2) is always at hand, since you already know the objective function. Let's graph the objective function near $r = 54.1926$ ...

```{r echo=FALSE}
slice_plot(A(r) ~ r, domain(r=c(50, 60))) %>%
  gf_point(A(54.1926) ~ 54.1926, shape="*", size=10) %>%
  gf_labs(x = "radius (cm)")
```

Look carefully at the axes scales. Deviating from the mathematical optimum by about 5cm (that is, 50,000 microns) produces a change in the output of the objective function by about 400 units **out of 55,000**. In other words, about 0.7%. 

It's true that $r^\star = 54.1926$ cm gives the "best" outcome. And sometimes such precision is warranted. For example, improving the speed of an elite marathon racer by even 0.1% would give her a 7 second advantage: easily the difference between silver and gold!

What's different is that you know exactly what is the ultimate objective of a marathon: finish faster. But you may not know the ultimate objective of the system your "optimal" tank will be a part of. For instance, your tank may be part of an external fuel pod on an aircraft. Certainly the designers of the aircraft want the tank to be as light as possible. But they also want to reduce drag as much as possible. A 54 cm diameter tube has about 17% more drag than a 50 cm tube. To save that much drag, it's probably well worth increasing weight by 0.7%.

In reporting the results from an optimization problem, you ought to give the decision maker all relevant information. Here, that might be as simple as including the above graph in your report.

We mentioned another technique for getting a handle on what precision is meaningful: (1) solve a closely related problem. This often requires some insight and creativity to frame the new problem. Here, we note that large capacity tanks often are shaped like a lozenge: a cylinder with hemi-spherical ends.  

```{r echo=FALSE, out.width="40%", fig.align="center"}
knitr::include_graphics("www/hemisphere.png")
```

Using $h$ for the length of the cylindrical portion of the tank, and $r$ for the radius, the volume and surface area are:
$$V(r, h) = \pi r^2 h + \frac{4}{3} \pi r^3 \ \ \ \mbox{and}\ \ \ A(r,h) = 2 \pi r h + 4 \pi r^2$$
Again, $V$ was specified as 1000 liters. So we can solve the $V$ formula for $h(r, V)$, then plug that in for $h$ in the area function to give $A(r, V)$.

```{r tb1-1, echo=FALSE, results="markup"}
askMC(
  "Which of these is correct? (Hint: Only one of the answers is dimensionally consistent.)",
  "+$h(r, V) = (V-4\\pi r^3/3)/ \\pi r^2$+",
  "$h(r, V) = (V-4\\pi r^2/2)/ \\pi r^2$",
  "$h(r, V) = (V-4\\pi r^3/3)/ \\pi r^3$",
  "$h(r, V) = \\sqrt{(V-4\\pi r^3/2)/\\pi r^3}$"
)
```


```{r tb1-2, echo=FALSE, results="markup"}
askMC(
  "Which of these is the correct expression for $A(r, V)$",
  "$A(r, V) = 2 V/r + \\frac{8 \\pi}{3} r^2$",
  "+$A(r, V) = 2 V/r + \\frac{4 \\pi}{3} r^2$+",
  "$A(r, V) = V/r + \\frac{20 \\pi}{3} r^2$",
  "$A(r, V) = V/r + 3 \\pi r^2$"
)
```

```{r tb1-3, echo=FALSE, results="markup"}
askMC(
  "Find $\\partial_r A(r, V)$ and set to zero. Solve for $r^\\star$ in terms of $V$. Which of these is correct?",
  "+$r^\\star = \\sqrt[3]{\\frac{3}{4\\pi} V}$+",
  "$r^\\star = \\sqrt[3]{\\frac{4}{3\\pi} V}$",
  "$r^\\star = \\sqrt[3]{\\frac{3\\pi}{4} V}$",
  "$r^\\star = \\sqrt[3]{\\frac{3}{2\\pi} V}$"
)
```


Find the optimum value of $r$ to minimize $A(r,V)$ when $V = 1000$ liters. 

```{r tb1-4, echo=FALSE, results="markup"}
askMC(
  "What is the optimal value of $r$ in cm to a precision of one micron?",
  "6.2035" = "Check that you were using the right units for $V$.",
  "+62.0351+",
  "52.0351",
  "46.0351"
)
```  

Use the sandbox to plot a graph of $A(r, V)$ for $V = 1000$liters. 

```{r rb2-2, exercise=TRUE, exercise.cap="Sandbox", exercise.nlines=4, eval=FALSE}
#A <- makeFun(function here ~ r, V = 1000000)
#slice_plot(A(r) ~ r, domain(r=c(55,70)))


```

```{r rb2-2-solution, echo=FALSE, eval=FALSE}
A <- makeFun(2*(V - 4*pi*r^3/3)/r + 4*pi*r^2 ~ r, V = 1000000)
slice_plot(A(r) ~ r, domain(r=c(55,70)))
```

```{r tb1-5, echo=FALSE, results="markup"}
askMC(
  "From the graph of $A(r, V)$ at $V=1000$ liters, read off a range of $r$ that produces $A$ no worse than 1% greater than the minimum. How wide is that range, approximately?",
  "$\\pm 0.1$cm$",
  "$\\pm 1$cm$",
  "+$\\pm 5$cm$+",
  "$\\pm 10$cm$",
  random_answer_order = FALSE
)
```


