

    

## Visualizing anti-differentiation

In this Block you'll learn several methods for calculating the anti-derivative of a function $f(t)$. 

## Anti-differentiation

You know how to differentiate any function, either by symbolic or numerical means. How do you anti-differentiate a function?

Let's consider two functions, $F(t)$ and $f(t)$. We started out knowing $f(t)$ and sought to calculate $F(t) = \int f(t) dt$. There are many styles of calculation for doing this; you'll meet a few of them in this Block.

Suppose you do some calculation and construct a candidate for $F(t)$. How can you confirm that $F(t)$ is in fact the anti-derivative of $f(t)$? Simple. Just calculate $\partial_t F(t)$ and see if it matches $f(t)$.

::: {.example data-latex=""}
Suppose $f(t) \equiv \frac{1}{t}$. What is $F(t) \equiv \int f(t) dt$? 

The answer to this question has been known for hundreds of years: $\int \frac{1}{t} dt = \ln(t)$.

How can we confirm that this is true? Just calculate $\partial_t \ln(t)$ and confirm that it is equal to $\frac{1}{t}$. 

It's important to point out that $\ln(t)$ is not the only function that is an anti-derivative of $\frac{1}{t}$. For instance, $F(t) = \ln(t) + 3.5$ is also an anti-derivative of $f(t) \equiv \frac{1}{t}$. To see this, just calculate the derivative $\partial_t F(t)$ and compare it to $\frac{1}{t}$.

Perhaps you can see that there is nothing special about 3.5. Any function of the form $\ln(t) + C$ is an anti-derivative of $\frac{1}{t}$.
:::

By virtue of your knowing derivatives of the pattern-book functions and the basic modeling functions, you automatically know the anti-derivatives of many functions.  

$$ \newcommand{\dantid}{\stackrel{\overset{\partial_t}{\longleftarrow}}{\underset{\int dt}{\longrightarrow}}}$$

For example:

- $$e^x \dantid e^x + C$$

- $$2x \dantid x^2 + C$$

- $$1 \antid x + C$$

- $$\cos(x) \dantid \sin(x) + C$$

You already know the left-going relationship $\overset{\partial_t }{\longleftarrow}$. Since anti-differentiation undoes differentiation, the right-going relationship $\overset{\longrightarrow}{\int dt}$ must also be true.

We'll make particular use of these relationships:

- $$a \dantid Aat + C$$
- $$at + b \dantid \frac{1}{2} a t^2 + b t + C$$
- $$a t^2 + b t + c \dantid \frac{1}{3} a t^3 + \frac{1}{2} t^2 + c t + C$$

## Accumulation from start to end

Let's return to the basin and the water tap. Suppose the basin is empty and that at time $0$ you turn on the tap with a rate of flow $f(t)$. The water starts to accumulate, it's volume being a function of time $V(t)$. How much water will there be at a time $0 < t$? In other words, what's the value of $V(t)$?

There is a special notation for this:

$$V(t) = \int_{0}^{t} f(t) dt$$

Traditionally, the operation $\int_a^b \_\_\ dt$ is called a ***definite integral***. Naturally, this suggests that there should be such a thing as an ***indefinite integral***. There is. An indefinite integral is simply another name for an ***anti-derivative***. 

Later, you'll see how to compute definite integrals. but for now we want to set forward 
INTRODUCE THE DEFINITE INTEGRAL from 0 to $t$.


::: {.example data-latex=""}
What happens when you accumulate nothing? Of course the answer is that you get nothing new. But that doesn't mean you have nothing. You will still have what you started with before the accumulation started. This is the basis for what may be an unexpected relationship:

$$0 \dantid C$$
:::







The  test for whether you

> ***Anti-differentiation*** is the reverse operation to ***differentation***, and *vice versa*.

The mathematical notation for differentiation is simple: $\partial_t f(t)$. For anti-differentiation the notation is typographically very different. The anti-derivative of a function $f(t)$ is written: `r mark(3150)`

$$\large \color{blue}{\int} f(\color{blue}{t}) \color{blue}{dt}$$
The math notation consists of several components, each of which has something to say. The components colored blue are part of the general notation. You can change the name of the input variable from $\color{blue}{t}$ to whatever you like, but you'll have to change the $\color{blue}{dt}$ accordingly. 

Our understanding of differentiation gives us a nice head start in understanding differentiation. Recall the derivatives of the pattern-book functions.

$f(x)$   | $\partial_x f(x)$
---------|-------------------
$e^{x}$  | $e^x$
$\ln{x}$ | $1/x$
$\sin{x}$ | $\cos{x}$
$\pnorm(x)$ | $\dnorm(x)$
$x^2$    | $2x$
$\vdots$ | $\vdots$

Transforming this into a table of anti-derivatives is merely a matter of re-labeling the columns:

$\int g(x)dx$   | $g(x)$
---------|-------------------
$e^{x}$  | $e^x$
$\ln{x}$ | $1/x$
$\sin{x}$ | $\cos{x}$
$\pnorm(x)$ | $\dnorm(x)$
$x^2$    | $2x$
$x^p$    | $p x^{p-1}$
$\vdots$ | $\vdots$

But unlike differentiation, anti-differentiation has no easy equivalents of the product rule or the chain rule (for compositions of functions). 





## For Euler chapter

As a practical matter, it's easier to find the flow given the volume. At any instant $t$, the relationship is:

\begin{equation}
f(t) = \frac{V(t+h) - V(t)}{h}
(\#eq:flow)
\end{equation}

Simply find the difference in the known volume at nearby times $t$ and $t+h$, and divide by $h$ to get the rate of change. That rate of change of volume is, in the physical situation of our example, the flow into the basin.

Finding volume given flow is a bit more complicated. Re-arranging Equation \@ref(eq:flow) gives:

\begin{equation}
V(t+h) = V(t) + h f(t)
(\#eq:volume)
\end{equation}

Equation \@ref(eq:volume) has $V()$ on both sides of the equation. It says that to know the volume at time $t+h$ we'll need to know f() at time $t$ as well as volume at time $t$. For instance, knowing both the flow and volume at time $t=0$ enables us to find volume at time $h$. 

\begin{equation}
V(h) = V(0) + h f(0)
(\#eq:volume1)
\end{equation}

But now that we know volume at time $h$, we can use that new fact to find volume at time $2h$:

\begin{equation}
V(2h) = V(h) + h f(h) = V(0) + h f(0) + h f(h)
(\#eq:volume2)
\end{equation}

The rightmost form in Eq. \@ref(eq:volume2) comes from substituting the value for $V(h)$ from Eq. \@ref(eq:volume1).

Having figured out volume at time $2h$, we can combine this new information with flow at time $2h$ to find volume at time $3h$:

\begin{equation}
V(3h) = V(2h) + h f(2h) \\
= V(0) + h\left[f(0) + f(h) + f(2h)\right]
(\#eq:volume3)
\end{equation}

Generalizing this pattern we can write the volume at any time $t=nh$ as
$$V(t=nh) = V(0) + h \left[f(0) + f(1h) + f(2h) + \cdots + f(nh)\right]\\
= V(0) + h \sum_{j=0}^{n} f(t=jh)$$
In other words, constructing $V(t)$ out of $f(t)$ involves adding together---accumulating!---$f(0), f(h), f(2h), \ldots$, and then multiplying by the size of $h$.

Keep in mind that $h$ is simply a little increment in $t$. Many people prefer to write this little bit of $t$ as a two-letter symbol: $dt$. 





## Euler

Block 2 introduced the derivative of a continuous function by looking at discrete differences. Given a function $g(t)$, we quantified the rate of change using the differencing operator. We called this the
$$\diff{t} g(t) \equiv \frac{g(t+h) - g(t)}{h}$$
As written above, $\diff{t}$ is properly called the ***finite-difference operator***, since no suggestion is made that $h$ is anything but a small number. We moved from $\diff{t}$ to $\partial_t$ by considering the ***limit*** as $h\rightarrow 0$ This move was fraught because of the concern about dividing by zero, but in the end we found simple algebraic expressions for the derivatives of the pattern-book functions as well as a few rules for handling the basic ways of combining functions using linear combinations, products, and function combinations. These rules were the sum rule, product rule, and chain rule respectively. `r mark(3140)`

In studying accumulation, we'll follow much the same path. The major difference is that our starting point is knowing a function like $\partial_t f(t)$: a derivative. From there will will construct a $f(t)$ from which $\partial_t f(t)$ *could have been derived, had we known it in the first place*. The idea is that sometimes information comes to us in the form of a rate of change and we need to figure out a function that could have generated that rate of change.  `r mark(3145)`

 `r mark(3155)`

The R/mosaic notation for the anti-derivative has exactly the same format as the derivative: 

    antiD(f(t) ~ t)


## Visualizing anti-differentiation {#anti-diff-viz}

Section \@ref(slope-fun-visualization) introduced a non-standard visualization of the slope function. We can build on that to show how the function $f(x)$ can be reconstructed from $\partial_x f(x)$. 

Figure \@ref(fig:euler-viz-1) shows a slope function visualization of some function $f(x)$

```{r euler-viz-1, echo=FALSE, fig.cap="A slope function $\\partial_x f(x)$ from which we are going to reconstruct the parent function $f(x)$. Each of the sloped segments has been given a label for later reference."}
Segs <- create_segments(sin(x) ~ x, domain(x=c(-pi,pi)), nsegs=30)
Segs$num <- c(letters, LETTERS)[1:nrow(Segs)]
Segs_orig <- Segs
gf_segment(y + yend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_text(0.25 ~ start, label=~num, size=3) %>%
    gf_labs(title="sin(x) slope-function visualization") %>%
    gf_lims(y=c(-1,1))

```
We'll reconstruct $f(x)$ one segment at a time. Recall that each of the segments shows a linear approximation to $f(x)$ at the input marked by the green dot. But in constructing the slope function, we threw away the information about the vertical placement of the segment. Now we have to recover that discarded information, as well as we can. `r mark(3160)`

The big clue for the reconstruction is that the function $f(x)$ was ***continuous***. But the piecewise function graphed in Figure \@ref(fig:euler-viz-1) is discontinuous; the endpoints of adjacent segments don't meet each other. `r mark(3165)`

That's easy to fix: we'll just move segment (b) vertically so that it becomes continuous with segment (a). 

```{r euler-viz-2, echo=FALSE, fig.cap="Moving segment (b) to become continuous with segment (a)."}
Segs$y[2] <- Segs$y[2] -0.199
Segs$yend[2] <- Segs$yend[2] - 0.199 
gf_segment(y + yend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_text(0.25 ~ start, label=~num, size=3) %>%
    gf_labs(title="sin(x) slope-function visualization. Segment (b) moved.") %>%
    gf_lims(y=c(-1,1))
```

Now that (a) and (b) are joined, we can join (c) to that:

```{r euler-viz-3, echo=FALSE, fig.cap="Moving segment (c) to become continuous with segments (a) and (b)."}
Segs$y[3] <- Segs$y[3] -0.4
Segs$yend[3] <- Segs$yend[3] - 0.4
gf_segment(y + yend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_text(0.25 ~ start, label=~num, size=3) %>%
    gf_labs(title="sin(x) slope-function visualization") %>%
    gf_lims(y=c(-1,1))
```

Continue this process one segment at a time to reconstruct $f(x)$.

```{r euler-viz-4, echo=FALSE, fig.cap="After joining all the segments together, the picture of $f()$ is complete."}
gf_segment(yf + yfend ~ x + xend, data = Segs_orig, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_text(0.25 ~ start, label=~num, size=3) %>%
    gf_lims(y=c(-1,1))
```
This process reconstructs the ***shape*** of $f(x)$ from $\partial_x f(x)$. But there is still something missing. We never touched segment (a). Its vertical location was arbitrary. So we have to qualify our claim to have reconstructed $f(x)$. What we've reconstructed is some function $\widehat{f}(x)$ whose derivative is $\partial_x f(x)$. There are other such functions; any function $\widehat{f}(x) + C$ can make a legitimate claim to being the anti-derivative of $\partial_t f(x)$.  We'll return to $C$ in later chapters, but for now we'll just name it: the ***constant of integration***. `r mark(3170)`

::: {.workedexample}
In the population example that started this chapter, we constructed a model for $\partial_t f(t)$, where $f(t) = \ln(P(t))$. We can plot $\partial_t f(t)$ using an ordinary graph as we did in Figure \@ref(fig:pop-growth), but let's use the slope-function representation instead. `r mark(3175)`

```{r pop-slope-fun, echo=FALSE, fig.cap="The slope-function visualization corresponding to the plot of $\\partial_t f(t)$ in Figure @@ref(fig:pop-growth)."}
G <- antiD(growth_fun(t) ~ t, lower.bound = 2020)
Segs <- create_segments(G(t) ~ t, domain(t=c(2020-1.5, 2100)), nsegs=30)
gf_segment(y + yend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_labs(x="Year", y="Percent growth per year")
```

To construct the original function $f(t)$, just connect the segments in Figure \@ref(fig:pop-slope-fun). 

```{r accum-pop-growth, echo=FALSE, fig.cap="Accumulating the annual growth in Figure @@ref(fig:pop-slope-fun)."}
gf_segment(yf + yfend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_labs(x="Year", y="Growth accumulated versus year")
```

The accumulated annual growth graph shows that by around 2075, the accumulated growth will be about 17% above the population of 2020. 

:::

::: {.forinstructor}
You may wonder why I'm introducing anti-differentiation with the "slope-function visualization" rather than the "area under a curve." One reason is that the slope-function visualization is closer to the logic of Euler's method, so in addition to showing how differentiation can be undone, we get a free introduction to Euler.  The other reasons are more fundamental to pedagogy and the challenges students have relating differentiation and anti-differentiation when presented as slope-and-area. I'll start with some history ... `r mark(3180)`

Pyrrhus was a Greek king who invaded Italy in 280 B.C. and fought the Romans. His first battle, at Heraclea, was a famous victory, as was his second battle a year later at Asculum. History records Pyrrhus saying, after his second victory, “If we are victorious in one more battle with the Romans, we shall be utterly ruined.” The victories were so costly that the army could not be sustained. `r mark(3185)`

Such ***pyrrhic victories*** occur throughout history: the British at Bunker Hill (1775),  Napolean at Borodino (1812), and Lee at Chancellorsville (1863).

A pyrrhic victory in mathematics pedagogy: the visualization of integration as the "area under a curve." This account of integration is utterly dominant among graduates of calculus courses. It is a brilliant and successful way to give the abstract operation of anti-differentiation an easily remembered visage. But it creates costs that are simply not worth bearing.  `r mark(3190)`

i. There are not so many genuine applications for finding areas under curves. Students, seeing them as emblematic of calculus, are often dis-motivated by the lack of connection between the rhetoric of the importance of calculus and the seeming unimportance of the primary application.
ii. It's extremely difficult to make a connection between the many genuine applications of anti-differentiation and the mental image of area.
iii. Because area is most easily shown as a fixed, delimited region, students are introduced to ***integration*** before they see ***anti-differentiation***. It can be hard for students to make the transition between "slope at a point" or "tangent line" and a slope ***function***. This cost has to be paid all over again when moving from integration to anti-differentiation.
iv. The Yin and Yang of calculus are differentiation and anti-differentation. Students are successfully taught that if $f(x)$ is a function, the derivative is the slope of that function and the integral is the area under the function. This creates an unhelpful illusion that derivatives and anti-derivatives are related through the function, that there is an intermediary between them. Translating this image to the metaphor of family generations, the picture looks like $f(x)$ is the **parent** of $\partial_x f(x)$ and $\int f(x)dx$ is the **parent** of $f(x)$. In other words, the incorrect image is encouraged that an anti-derivative is the **grandparent** of the derivative.  In fact the anti-derivative and the derivative have a parent-child relationship.
vi. At best, for many people, the relationship between the slope function and the area function is hard to see and sometimes mysterious. (Of course it's hard to see: the slope function is the ***second derivative*** of the area function.) This  `r mark(3195)`

For these reasons, I encourage instructors to avoid defining the calculus operation as "area under the curve." Make use of areas when they are part of a genuine application of calculus. Save the Riemann Sum for courses in analysis. For demonstrating anti-differentiation, the starting point should be a function which we know to be the derivative of the sought-after function. Teach Euler as connecting together short segments of slopes. Emphasis the connections between differentiation and anti-differentiation from the start: both are relationships between one function and another function, just as every person is both a mother and a child. The anti-derivative is the mother of the child, the derivative is the child of the mother. `r mark(3200)`
:::



## Symbolic anti-derivatives

The Euler method involves a ***finite*** $h$, which is just to say that $h$ must be ***non-zero***. Otherwise, $f(t_0 + h)$ would be exactly the same as $f(t_0)$. For some functions, however, it's possible to construct the anti-derivative without needing to deal with $h$ at all.  `r mark(3205)`

Recall that anti-differentiation undoes differentiation, and vice versa. In the previous Block, we found the symbolic derivatives of the basic modeling functions and general methods for differentiating functions constructed by linear combination, products, and function composition. Using the techniques from Block 2, tables can be constructed of functions and their derivatives, looking like this: `r mark(3210)`



Recall that every smooth, continuous function has a derivative defined everywhere in the function's domain. Similarly, every function has an anti-derivative, and even discontinuous, un-smooth functions have nicely behaved anti-derivatives. In this sense anti-differentiation is easy. It's only the algebra of anti-differentiation that can be hard or often literally impossible. `r mark(3215)`



    
::: {.todo}

Calculation of luminance using light intensity at different wavelengths integrated over the luminance function. https://en.wikipedia.org/wiki/Luminous_efficiency_function
:::

::: {.todo}
Pick up on the Lorenz curve `{.intheworld  data-latex=""}` in Blocks 1 and 2. Integrate to find the Gini coefficient. Show that the Gini coefficient is the same for very different types of inequality and that therefore it's not such a good measure. How about the integral over the poorest 25% or 50% of society." `r mark(3220)`
:::

-----


## Numerical anti-derivatives

We will devote about a third of this block on accumulation to algebraic techniques for calculating anti-derivatives. You will see these techniques in use in some of your future classes and work in science and engineering.  `r mark(3225)`

It's the nature of things that some people master the algebraic techniques and many do not. But it's easy to make mistakes. Even more fundamentally, there are many accumulation problems where the functions to be integrated do not have an algebraic form for the anti-derivative. In such cases, professionals use numerical techniques such as the Euler method. `r mark(3230)`

In order to give you a simple way to construct the anti-derivative of (just about) any function, while minimizing the amount of computer programming, we have packaged up anti-differentiation techniques into one, easy to use R function. This is `antiD()`. `r mark(3235)`

The `antiD()` function has the same interface as `D()` or `makeFun()`: the argument is a tilde expression of the sort `sqrt(x*sin(3*x)) ~ x`. The result returned from `antiD()` is a new R function that takes as its argument the "with respect to" variable. The sandbox provides a space to play with `antiD()` so that you feel comfortable using it. `r mark(3240)`

```{r eval=FALSE}
antiD(x^-2 ~ x)

f <- makeFun(sqrt(x*sin(3*x)) ~ x)
antiD(f(x) ~ x)
```

As you can see from the output of the sandbox, `antiD()` returns an R `function()`. The variable on the right of the tilde expression in the argument becomes the first of the arguments to that function. There is also a `C` argument: the constant of integration. `r mark(3245)`

`antiD()` knows a few of the algebraic integration techniques, roughly at the level of the basic modeling functions part of the course. When `antiD()` identifies the tilde expression as something it can handle, it returns a function whose body is the algebraic formula for the anti-derivative (although sometimes written in a cumbersome way).

When `antiD()` does not recognize its argument as a basic modeling function, the result is still an R function with the "with respect to" variable and `C` as arguments. But the body of the function is  unintelligible to a human reader (except perhaps for the `numerical_integration()`). The method of numerical integration is more sophisticated than Euler, and is highly precise and reliable. `r mark(3250)`

We're going to use `antiD()` in this daily digital simply because we want to focus on the process of differential modeling. The integrals you encounter will sometimes be ones you know how to handle algebraically. It's a good idea to do such integrals by hand and then compare to the results of `antiD()` to check your work. `r mark(3255)`

## Exercises





`r insert_calcZ_exercise("XX.XX", "GHNDR", "Exercises/Accum/integrals-of-the-day.Rmd")`

`r insert_calcZ_exercise("XX.XX", "KEsll","Exercises/Accum/what-is.Rmd")`

`r insert_calcZ_exercise("XX.XX", "9k3s","Exercises/Accum/MMAC-1.Rmd")`



**Example**: Find the numerical value of this definite integral.

$$\int^{7}_{3} e^{x^{2}} dx$$
**Example Solution in R**: 
```{r}
F<-antiD(exp(x^2)~x)
F(7)-F(3)
```


**Problem 1**: Find the numerical value of this definite integral.

$$\int^{5}_{2} x^{1.5} dx$$

Recall that for a definite integral of function $f()$, you find the anti-derivative $F(x) \equiv \int f(x) dx$ and evaluate it at the limits of integration. Here that will be $F(5) - F(2)$.



```{r tbf1-solution}
f <- antiD( x^1.5 ~ x )
f(5) - f(2)
```


```{r tbf-a, echo=FALSE, results="markup"}
askMC(
  "**Problem 1**: What's the numerical value of $$\\int_2^5 x^{1.5} dx  ?$$",
  0.58,6.32,"+20.10+",27.29,53.60,107.9,1486.8,
  random_answer_order = FALSE,
  id = knitr::opts_current$get()$label
)
```


```{r tbf-b, echo=FALSE, results="markup"}
askMC(
  "**Problem 2**: What's the numerical value of $$\\int^{10}_{0} \\sin( x^2 ) dx ?$$",
  "+0.58+",6.32,20.10,27.29,53.60,107.9,1486.8,
  random_answer_order = FALSE,
  id = knitr::opts_current$get()$label
)
```





```{r tbf-c, echo=FALSE, results="markup"}
askMC(
  "**Problem 3**: What's the numerical value of $$\\int^{4}_{1} e^{2x} dx ?$$",
  0.58,6.32,20.10,27.29,53.60,107.9,"+1486.8+",
  random_answer_order = FALSE,
  id = knitr::opts_current$get()$label
)
```

`r insert_calcZ_exercise("XX.XX", "iLeSB", "Exercises/Accum/sailing-over-time.Rmd")`




`r insert_calcZ_exercise("XX.XX", "YLELSE", "Exercises/Accum/falcon-tell-mug.Rmd")`


`r insert_calcZ_exercise("XX.XX", "ady5Fh", "Exercises/Accum/monkey-light-stove.Rmd")`

`r insert_calcZ_exercise("XX.XX", "JEslw", "Exercises/Accum/chain-of-differentiation.Rmd")`

::: {.todo}
This needs to be reconfigured to match the new pattern-book function/basic modeling function dicotomy.
:::

`r insert_calcZ_exercise("XX.XX", "EOSLE", "Exercises/Accum/basic-modeling-functions.Rmd")`



Remembering that accumulation is the opposite of differentiation, let's invert differentiation. The setting is that we know $\partial_t f(t)$ but do not yet know $f(t)$. Recall the familiar form of derivative, approximated by
$$\color{blue}{\partial_t f(t_0)} \equiv \frac{f(t_0+\color{blue}{h}) - f(t_0)}{\color{blue}{h}}$$
We've put the quantities we know in <span style="color: blue;">blue</span>. We know the derivative $\color{blue}{\partial_t f(t)}$ and we have selected some $\color{blue}{h}$. (In the population example, we set $\color{blue}{h}$ to be 1 year.) Re-arranging the above formula gives:
$$f(t_0+h) = \color{blue}{f(t_0)} + \color{blue}{h} \times \color{blue}{\partial_t f(t_0)}$$
Notice that we're saying we know $\color{blue}{f(t_0)}$. This is just like saying that we knew the population in year 2020; we know where we start.

The formula allows us to find $f()$ at time $h$ in the future. That is, we compute the unknown $f(t_0 + h)$ from what we already know.

This projection into the future using the (known) rate of change is called an ***Euler step***. The idea of inverting differentiation is to take one Euler step after another, constructing the future values one after the other
$f(t_0 + h)$, $f(t_0 + 2h)$, $f(t_0 + 3h), \cdots$. After each step, we know something more about $f()$ which we can use in taking the next step. For instance, once $f(t_0 + h)$ has been calculated, we get $f(t_0 + 2h)$ by applying the Euler step formula:

$$f(t_0 + 2h) = \color{blue}{f(t_0 + h)} + \color{blue}{h} \times \color{blue}{\partial_t f(t_0 + h)}$$
Notice that we've written $\color{blue}{f(t_0 + h)}$ in blue. Although we didn't know $f(t_0 + h)$ at the very start of the process, we figured it out by the first Euler step and it's ready for use in the second step.

