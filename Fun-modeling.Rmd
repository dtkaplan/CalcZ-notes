# Process of modeling

Seen very abstractly, a mathematical model, as we are using the term, is a set of ***functions*** that represent the relationships between inputs and outputs.  `r note(900)`

At the most simple level, building a model can be a short process:

1. Develop an understanding of the relationship you want to model. Often, part of this "understanding" is the pattern seen in data.
2. Choose a function type---e.g. exponential, sinusoidal, sigmoid---that you think would be a good match to the relationship.
3. Find parameters that scales your function to be able to accept real-world inputs and generate real-world outputs.

It's important to distinguish between two basic types of model:

1. ***Empirical models*** which are rooted in ***observation*** and ***data***.
2. ***Mechanistic models*** such as those created by applying fundamental laws of physics, chemistry, and such.

We are going to put off mechanistic models for a while, for two reasons. First, the "fundamental laws of physics, chemistry, and such" are often expressed with the concepts and methods of calculus. We are heading there, but at this point you don't yet know the core concepts and methods of calculus. Second, most students don't make a careful study of the "fundamental laws of physics, chemistry, and such" until *after* they have studied calculus. So examples of mechanistic models will be a bit hollow at this point.

The process of constructing a model that is a good match for data is called ***curve fitting***, or, more generally, ***fitting a model***.

## Variations from scaling

A good place to start building a model is to pick one of the basic modeling functions. This works surprisingly often. To remind you, here are our seven basic modeling functions:   `r note(910)`

- straight-line
- exponential
- power-law
- logarithm
- sinusoid
- hump
- sigmoid

It helps in making the selection to have ready to mind the basic shape of each of these function families. To review, revisit Section \@ref(function-shapes).

Remember also that, in general, we scale the inputs and scale the output. This means that in choose a model family, we don't have to worry at first about the numbers on the axes. (Of course, those numbers will be critically important later on in the process!) The scaling does, however, allow us to consider some variations on the shapes of the modeling functions. In particular, the ***input scaling*** lets us flip the shape right-for-left. And the ***output scaling*** lets us flip the shape top-for-bottom.

- $f(t)$, basic shape
- $f(-t)$, flipped right-for-left
- $-f(t)$, flipped top-for-bottom
- $-f(-t)$, flipped both top-for-bottom and right-for-left

For functions such as the sinusoid, flipping is not much use, since the flipped sinusoid curve is still a sinusoid. Similarly, a right-for-left flipped hump function has the same shape as the original. For the straight-line function, flipping of either sort accomplishes the same thing: changing the sign of the slope. 

For the exponential function, the two possible types of flipping---right-for-left and top-for-bottom---produce four different curves, all of which are exponential, shown in Figure \@ref(fig:four-variations).

```{r four-variations, out.width="50%", echo=FALSE, fig.cap="Four variations of the exponential functions.", fig.show="keep"}
slice_plot(exp(x) ~ x, domain(x=c(-2,2))) %>%
  gf_refine(theme_void()) %>%
  gf_labs(title="A. The exponential function, no flipping")
slice_plot(exp(-x) ~ x, domain(x=c(-2,2))) %>%
  gf_refine(theme_void()) %>%
  gf_labs(title="B. Flipped right-for-left")
slice_plot(-exp(x) ~ x, domain(x=c(-2,2))) %>%
  gf_refine(theme_void()) %>%
  gf_labs(title="C. Flipped top-for-bottom")
slice_plot(-exp(-x) ~ x, domain(x=c(-2,2))) %>%
  gf_refine(theme_void()) %>%
  gf_labs(title="D. Flipped both right-for-left and top-for-bottom")

```

<details>
<summary>`r ex.mark(8.3, "ds4e7", fname="Exercises/DD-141Z-04/exponential-bases.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/DD-141Z-04/exponential-bases.Rmd")`
</details>

## Curve fitting an exponential function {#fit-exponential}

The exponential function is particularly useful when the quantity we want to model shows ***constant proportional increase***. Many quantities in everyday life are this way. For instance, an increase in salary is typically presented in a format like "a 3% increase." The population growth of a country is often presented as "percent per year." Inflation in the price of goods is similarly described in percent per year. Interest on money in a bank savings account is also described as percent per year. But if you have the bad fortune to owe money to a loan shark, the proportional increase might be described as "percent per month" or "percent per week."   `r note(920)`

When you know the "percent increase per time" of a quantity whose initial value is $A$, the exponential function is easy to write down: $$f(t) = A (1+r)^t$$ 
The number $r$ is often called the ***interest rate*** or ***discount rate*** and is given in **percent**. 

::: {.takenote}
Regrettably, it's extremely easy amd common to forget the rules for addition with percent. If $r = 5\%$, then $(1+r) = 1.05$, not 6. Always keep in mind that $5\%$ means $\frac{5}{100}$.

Another source of error stems from the tradition in mathematics of using the number $e=2.718282$ as the "natural" base of the exponential function, whereas in $f(t) = A (1+r)^t$ the base is $1+r$. 

You can translate an exponential $b^t$ in any base $b$ to the "natural" base. This is just a matter of calculating the appropriate parameter $k$ such that $e^k = b$. Using logarithms, $$e^k = b\ \ \implies \ \  k=\ln(b)$$ For instance, if 
$r=5\%$ per year, we'll have $k = \ln(1+r) = \ln(1.05) = 0.488$ per year.

Almost everybody is happier doing arithmetic with numbers like 2 and 10 rather than $e=2.718282$. For this reason, you may see formulations of the exponential function as $g(t) \equiv 2^{a t}$ or $h(t) \equiv 10^{c t}$. Remember that $2^{a t}$ and $e^{at}$, although both exponential functions, are quantitatively different. If you want to write $2^{at}$ using the "natural" base, it will be $e^{\ln(2) a\, t }$. Similarly, $10^{ct} = e^{\ln(10) c\, t}$.
:::


Exponential functions also describe ***decrease*** or ***decay***. Just replace $t$ with $-t$. That is, a movie of a decreasing quantity is just the movie of an increasing quantity played backwards in time!    `r note(930)`

Figure \@ref(fig:Fun-4-intro-1) shows some data collected by Prof. Stan Wagon to support his making a detailed mechanistic model of an everyday phenomenon: [The cooling of a mug of hot beverage to room temperature](https://www.researchgate.net/profile/Gianluca_Argentini/post/Is_analogy_reasoning_between_heat_transfert_and_electriocity_allows_to_apply_the_electricity_laws_about_resistance_to_thermal_resistances/attachment/59d6573379197b80779ada64/AS%3A533325451403264%401504166104259/download/Stan+WAGON+How+quickly+does+water+cool.pdf). The mug started at room temperature, which was measured at 26 degrees C. At time 0 he poured in boiling water from a kettle and measured the temperature of the water over the next few hours.

```{r Fun-4-intro-1, echo=FALSE, fig.cap="Stan's data"}
set.seed(101)
gf_point(temp ~ time, data=CoolingWater %>% sample_n(20)) %>%
  gf_labs(x = "Time (minutes)", y="Temperature (deg. C)")
```

Our task is to translate this data into the form of a function that takes time as input and returns temperature as output. Such a model would be useful for, say, filling in the gaps of the data. For instance, we might want to find the temperature of the water immediately after being poured from the kettle into the mug.



Looking at the data, one sees that the temperature decreases along a curve: cooling fast at first and then more slowly. This is the pattern of the flipped right-for-left exponential. (Figure \@ref(fig:four-variations)(B)) We can imagine then that an exponential, $A e^{kt} + C$ will be a suitable model form for the cooling water.

What remains is to find the parameters $A$, $k$, and $C$.  Here is a general process for curve-fitting an exponential. Later, we'll apply this process specifically to the water-cooling situation. 

**General process for curve-fitting an exponential**

**Step 0**: Check that the data show an exponential pattern in one of its variations, namely a smooth increase or decrease and leveling out beyond some value of $t$. If this isn't true, reconsider whether you should be using an exponential function in the first place.

**Step 1** Do the data show exponential growth or exponential decay? If it's exponential growth, then the flat region in Step 0 will be to the left and $k$ will be positive. If exponential decay, the flat region will be to the right and $k$ will be negative.

Notice that the question of "growth or decay" depends only on the sign of the parameter $k$. You can have an exponentially decaying process that's increasing. Consider, for instance, the speed of a car as it merges onto a highway from a slow speed on the entrance ramp.  The car's velocity is increasing, but as you approach highway speed the rate of increase gets smaller. That's exponential decay.

**Step 2** Where is the baseline? We're going to put aside $k$ for the moment and find the value of the output that is being approached asymptotically, that is, the height of the level zone of the data. This height is the coefficient $A$ in the linear combination.

**Step 3** Once you know the baseline, you're set to find a numerical value for the parameter $k$. 

i. Pick a point in the data far from the baseline. Call the input $t_1$. 
ii. Scan forward or backward in time to find a point in the data that's vertically half way from the original point toward the baseline. Call the input at that point $t_{1/2}$. The difference between these, $t_1 - t_{1/2}$ is called the ***half-life*** or ***halving-time*** if it's negative and the ***doubling time*** if it's positive. 
iii. The parameter $k$ is $0.693 / (t_1 - t_{1/2})$. Double check the sign of $k$ to make sure it's consistent with what you saw in Step 1. (Incidently, $0.693 = \ln(2)$. The 2 is the same as the 2 in doubling or halving.)

**Step 4** Now that you have numerical values for the baseline $A$ and the parameter $k$, calculating the value of $B$ is straightforward.  
    i. Pick a $t_0$ that's reasonably well represented in your data. Find the vertical coordinate represented by the data near that $t_0$. Call that ${\cal D}$. 
    ii. Solve with respect to $B$ the equation $A + B e^{k t_0} = {\cal D}$. Things are particularly easy if you can use $t_0 = 0$. Then you just straight off calculate $B = {\cal D} - A$.

**Step 5** Plot the function $A + B e^{k t}$ using the values for $A$, $B$, and $t$ that you just found. If you are not satisfied, tweak the parameters a bit until you are.

**Exponential curve fitting applied to the water-cooling data**

Let's illustrate the general process on the water-cooling data, redrawn in Figure \@ref(fig:Fun-4-a-2-1).   `r note(950)`

```{r Fun-4-a-2-1, echo=FALSE, fig.cap="The cooling-water data, repeated here for convenience."}
set.seed(101)
gf_point(temp ~ time, data=CoolingWater %>% sample_n(20)) %>%
  gf_lims(y = c(20, NA))
```

Step 0: The data indicate a smooth curve. As $t$ gets large, the curve approaches a constant. So an exponential model is reasonable.

Step 1: The flat zone of the data is to the right. So we've got exponential decay and $k < 0$. 

Step 2: The curve looks like it's leveling out at a temperature of about 25 degrees C for large $t$. So $A \approx 25^{\circ} \text{C}$.

Step 3: 

i. The point furthest from the baseline is located at $t_1 = 15 \text{sec}$ with a value ${\cal D} \approx 80^\circ\text{C}$. 
ii. This if $55^\circ\text{C}$ from the baseline. We want to find the value of $t_1$ where the temperature will be half way from 80 to the baseline. That's a temperature of about $80 - 55/2 = 53.5$. Scanning over to the right, the function that I can imagine going through the data crossed $53^\circ$ at about $t_{1/2} = 40$. Thus, the half-life is estimated at 25s. 
iii. The parameter $k$ is therefore $k\approx $0.693 / \mbox{half-life}) = - 0.63 / 25 = -0.0277$. Since we identified in Step 1 that exponential decay is involved, we expect $k$ to be negative. It is.

Step 4. 

i. We know $A \approx 25$ and $k \approx -0.0277$. We also now that for $t=15$ the function output is about ${\cal D} = 80^\circ$. 
ii. This means $25^\circ + B e^{- 0.0277 \times 15} \approx 80^\circ = 25 + 0.66 B$. Solving for $B$ gives $B = (80 - 25)/0.66 = 83.3$

Step 5. Graph the function over the data.

```{r Fun-4-a-2-2, echo=FALSE, fig.cap="An exponential function that roughly aligns with the data."}
set.seed(101)
gf_point(temp ~ time, data=CoolingWater %>% sample_n(20)) %>%
  gf_lims(y = c(20, NA)) %>%
  slice_plot(25 + 83.3*exp(-.0277*time) ~ time, color="blue")
```


<details>
<summary>`r ex.mark(8.7, "vkwl4", fname=exercise_file("05", "chicken-choose-vase.Rmd"))` </summary>
`r MC_counter$reset()` `r knitr::knit_child(exercise_file("05", "chicken-choose-vase.Rmd"))`
</details>



<details>
<summary>`r ex.mark(8.11, "asevss", fname=exercise_file("05", "cow-type-kayak-new.Rmd"))` </summary>
`r MC_counter$reset()` `r knitr::knit_child(exercise_file("05", "cow-type-kayak-new.Rmd"))`
</details>


<details>
<summary>`r ex.mark(8.15, "j3xe", fname="Exercises/Fun/maple-hit-saucer.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/Fun/maple-hit-saucer.Rmd")`
</details>


## Curve fitting a periodic function {#fit-periodic}

Figure \@ref(fig:Fun-4-a-3-1) shows the tide level in [Providence, Rhode Island](https://www.google.com/maps/place/Providence,+RI/@41.8071,-71.4012,14z/data=!4m5!3m4!1s0x89e444e0437e735d:0x69df7c4d48b3b627!8m2!3d41.8071!4d-71.4012), starting at midnight on April 1, 2020 and recorded every minute for four and a half days. (These data were collected by the US National Oceanic and Atmospheric Administration. [Link](https://tidesandcurrents.noaa.gov/api/datagetter?begin_date=20200401%2010:00&end_date=20200405%201:59&station=8454000&product=one_minute_water_level&datum=mllw&units=metric&time_zone=gmt&application=web_services&format=csv))   `r note(960)`

```{r Fun-4-a-3-1, echo=FALSE, fig.cap="About four days of tide-level data from Providence, Rhode Island"}
#gf_line(level ~ hour, data = math141Z::Anchorage_tide[1:1000,])
gf_line(level ~ hour, data = math141Z::RI_tide) %>%
  gf_labs(y="Water level (meters)", x="Time (hours)", title="Tide level in Providence, Rhode Island") %>%
  gf_lims(y =c(0, 2))
```

It's not too hard to see what's going on. The tide rises and falls about every 12 hours. The difference between high tide and low tide is a little more than one meter. The tide gauge is calibrated so that a typical reading is 1 meter, although we don't know what that is respect to. (Certainly not sea level, since then the typical reading would be around zero.)

This simple description tells almost everything needed to construct an A/B model of the tide level using a sinusoid, that is, a function of the form $$A + B \sin(2\pi t/P)$$ The procedure is straightforward:

**Step 0**: Determine whether a sinusoid model is appropriate. As you know, sinusoids oscillate up and down repeatedly with a steady ***period***. That certainly seems the case here. But sinusoids are also steady in the ***peak*** and ***trough*** values for each cycle. That's only approximately true in the Providence data. Models inevitably involve approximation. We'll have to keep an eye on whether the ***fixed amplitude*** feature of sinusoids 

**Step 1**: Choose a sensible value to represent the low point repeatedly reached. 0.5 m seems appropriate here, but obviously the exact position of the trough of each cycle varies over the 4.5 day duration of the data.  Similarly, the peak is near 1.6 m. Parameter $A$ is the mean of the peak and trough values: $\frac{1.6 + 0.5}{2} = 1.05$ m here. Parameter $B$ is half the difference between the peak and trough values: $\frac{1.6 - 0.5}{2} = 0.55$. Parameter $A$ is called the ***baseline*** of the sinusoid. Paramter $B$ is the ***amplitude***. (Note that by convention, the amplitude is always *half* the high-to-low range of the sinusoid.)

**Step 2**: Estimate the period $P$ of the sinusoid. This can be done with a ruler: the horizontal duration of a complete cycle. I like to use the time between peaks, but the time between troughs would work just as well. Another good choice is the time between positive sloping crossings of the baseline. (But be careful. The time between *successive* baseline crossings, one positive sloping and the other negative, give just *half* the period.) 


On the scale of the above plot, it's hard to read off the time of the first peak. So, zoom in until it becomes more obvious.

```{r Fun-4-a-3-2, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="Zooming in on the start of the data (left) and on the last part of the data (right)."}
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour < 25)) %>%
  gf_labs(y="Water level (meters)", x="Time (hours)",
          title="Start of record") %>%
  gf_lims(y =c(0, 2))
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour > 90)) %>%
  gf_labs(y="Water level (meters)", x="Time (hours)",
          title = "End of record") %>%
  gf_lims(y =c(0, 2))
```

The left panel in Figure \@ref(fig:Fun-4-a-3-2) shows about 24 hours at the start of the record. The first peak is at about 7 hours, the second at about 18 hours. That indicates that the period is 18 - 7 = 11 hours. 

**Step 3** Plot out the model over the data. Replacing the symbols $A$, $B$, and $P$ with our estimates, the model is 

$$\require{color}
{\color{green}\mbox{tide}(t) \equiv 1.05 + 0.55 \sin(2\pi t/11)}$$

Figure \@ref(fig:Fun-4-a-3-3) shows this model in $\color{green}\mbox{green}$.

```{r Fun-4-a-3-3, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="The sinusoid fails to align with the timing of peaks and troughs."}
mod1 <- makeFun(1.05 + 0.55*sin(2*pi*hour/11) ~ hour)
mod2 <- makeFun(1.05 + 0.55*sin(2*pi*(hour-17)/11) ~ hour)
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour < 25)) %>%
  slice_plot(mod1(hour) ~ hour, color="green") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)") %>%
  gf_lims(y =c(0, 2))
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour > 90)) %>%
  slice_plot(mod1(hour) ~ hour, color="green") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)") %>%
  gf_lims(y =c(0, 2))
```



<details>
<summary>`r ex.mark(8.19, "YELXG", fname="Exercises/Fun/lamb-talk-gloves.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/Fun/lamb-talk-gloves.Rmd")`
</details>


<details>
<summary>`r ex.mark(8.23, "VBWD", fname="Exercises/Fun/spirometer.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/Fun/spirometer.Rmd")`
</details>

## Curve fitting a power-law function

You have been using power-law functions from early in your math and science education.  Some examples:   `r note(970)`

Setting | Function formula | exponent
--------|------------------|----------
Circumference of a circle | $C(r) = 2 \pi r$ | 1
Area of a circle | $A(r) = \pi r^2$ | 2
Volume of a sphere | $V(r) = \frac{4}{3} \pi r^3$ | 3
Distance traveled by a falling object | $d(t) = \frac{1}{2} g t^2$ | 2
Gas pressure versus volume | $P(V) = \frac{n R T}{V}$ | $-1$
... perhaps less familiar ... | | 
Distance traveled by a diffusing gas | $X(t) = D \sqrt{
\strut t}$ | $1/2$
Animal lifespan (in the wild) versus body mass | $L(M) = a M^{0.25} | 0.25
Blood flow versus body mass | $F(M) = b M^{0.75}$ | 0.75

The reason why power-law functions have their important role in science have to do with the logic of physical quantities such as length, mass, time, area, volume, force, power, and so on. We'll discuss this at length later in the course and the principles will appear throughout calculus.

As for finding the power law $f(x) \equiv A x^p$ that provides a good match to data, we'll need some additional tools to be introduced in Chapter \@ref(magnitudes).



## Hump and sigmoid functions

Our last two basic modeling functions express an important idea in modeling: ***localness***. To put this in concrete terms, imagine creating a function to depict the elevation above sea level of a long road as a function of distance in miles, $x$, from the start of the road. If the road were level at 1200 feet elevation, a sensible model would be $\mbox{elevation}(x) = 1200 \text{ft}$. If the road were gently sloping, a better model would be $\mbox{elevation}(x) = 1200 + 3 x$.   `r note(980)`

Now let's add a bump to the road. A bump is a local feature, often only a few feet wide. Or, perhaps the road is crossing a mountain range. That's also a local feature, but unlike a bump in the road a mountain range extends for many miles.

The basic modeling function suited to represent bumps in the road, or potholes, or mountain ranges is called a ***hump function***. 

A hump function has two parameters: the location of the hump, which we'll call the ***center*** parameter, and the sideways extent of the hump, which we'll call the ***spread***. Figure \@ref(fig:Fun-3C-1) shows a few hump functions with different parameters.

```{r Fun-3C-1, echo=FALSE, warning=FALSE, fig.show="hold", fig.height=3, fig.width=6}
slice_plot(dnorm(x, 1, 0.5) ~ x, domain(x=c(-3, 3))) %>%
    gf_labs(title="(a) Hump with center=1, spread = 0.5")
slice_plot(dnorm(x, -1, 0.05) ~ x, domain(x=c(-3, 3)), npts=500) %>%
    gf_labs(title="(b) Hump with center=-1, spread = 0.05")
slice_plot(dnorm(x, 0.25, 1.2) ~ x, domain(x=c(-3, 3)), npts=500) %>%
    gf_labs(title="(c) Hump with center=0.25, spread = 1.2") %>%
  gf_hline(yintercept = dnorm(1.2+0.25, 0.25, 1.2), color="red") %>%
    gf_segment(0.2016 + 0.2016 ~ -0.95 + 1.45, color="red", size=3, alpha=.03)
```

It's easy to read off the ***center*** parameter from a graph of a hump. It's the location of the top of the hump. (We mentioned before that a mathematical word for "the location of the top" is ***argmax***; the value for the input of the function that produces the maximum output.)

The ***spread*** parameter is also pretty straightforward, but you first have to become familiar with an unusual feature of the hump function. The output of the hump function *far from the center* is practically zero. But it is not exactly zero. You can see from the graphs that the hump function has long flanks which approach zero output more or less in the manner of an exponential function. This means that we can't measure the spread of the hump function by the distance between the zeros on either side of the peak. Instead, we need a ***convention*** that will allow us to be precise in quantifying what is admittedly a vague concept of width.

A simple convention is that the spread is the "half-width at half-height." Come down half-way from the peak of the hump. Panel (c) of Figure \@ref(fig:Fun-3C-1) marks that elevation with a thin, red, horizontal line. Along that line, measure the width of the hump, as marked by the thick red line in Panel (c). The ***spread*** parameter is half the width of the hump measured in this way.   `r note(990)`

If you have a keen eye, you'll notice that the red line is not exactly half-way down from the peak. It's down 39.35% from the peak. The official definition of width of a hump is not actually half-width at half-height, but that simple formulation will do for us for the present.

Another seeming oddity about the hump function is the value of the maximum. It would have seemed natural to define this as 1, so-called "unit height." The way it actually works is different: the maximum height is set so that the ***area*** under the hump function is 1.

This business with the area will make more sense when you've learned some calculus tools, specifically "differentation" and "integration." For now though ...

Consider another road feature, a local ***change*** from one elevation to another as you might accomplish with a ramp. The basic modeling function corresponding to a ***local change*** from one level to another is the ***sigmoid*** function. Figure \@ref(fig:Fun-3C-2) shows three sigmoid functions.

```{r Fun-3C-2, echo=FALSE, fig.show="hold", warning=FALSE, fig.height=3, fig.width=6}
slice_plot(pnorm(x, 1, 0.5) ~ x, domain(x=c(-3, 3))) %>%
    gf_labs(title="(a) Sigmoid with center=1, spread = 0.5")
slice_plot(pnorm(x, -1, 0.05) ~ x, domain(x=c(-3, 3)), npts=500) %>%
    gf_labs(title="(b) Sigmoid with center=-1, spread = 0.05")
slice_plot(pnorm(x, 0.25, 1.2) ~ x, domain(x=c(-3, 3)), npts=500) %>%
    gf_labs(title="(c) Sigmoid with center=0.25, spread = 1.2") 
```
The name "sigmoid" comes from vague resemblance of the graph to the letter S (which is "sigma" in Greek: Ï‚). 

The parameters of the sigmoid function are the same as for the hump function: ***center*** and ***width***. The center is easy to estimate from a graph. It's the value of the input that produces an output of 0.5, half-way between the max and min of the sigmoid. As with the hump function, the ***width*** is measured according to a convention. The width is the ***change in input*** needed to go from an output of 0.5 to an output of 0.8413. This use of 0.8413 must seem loony at first exposure, but there is a reason. We'll need more calculus tools before it can make sense.

Hump functions and sigmoid functions with the same center and width parameters have a very close relationship. The ***slope*** of the sigmoid function is the corresponding ***hump*** function. Figures \@ref(fig:Fun-3C-1) and \@ref(fig:Fun-3C-2) show corresponding hump and sigmoid functions. To the very far left, the sigmoid function is effectively flat: a slope near zero. Moving toward the center the sigmoid has a gentle slope: a low number. In the center, the sigmoid is steepest: a higher number. Then the slope of the sigmoid becomes gentle again before gradually falling off to zero. Near zero, then low, then higher, then low again, then falling off to zero: that's also the description of a hump function!

In R, the name of the sigmoid function is `pnorm()`. The hump is `dnorm()`. The parameters that specify center and spread are named `mean` and `sd`. The word "mean" accurately conveys the idea of "center." It would be nice to be able to say that `sd` comes from `s`prea`d`, but in fact `sd` is short for ***standard deviation***, a old-fashioned term familiar to students of statistics.   `r note(1000)`



::: {.why}
"Standard deviation" is off-putting to many people and it really should be replaced with something less weird, perhaps ***typical spread***. The "standard" can be interpreted as "widely used convention." What about "deviation?" The hump function first appeared two-hundred years ago as part of a theory of measurement error. In the context of "error", "deviation" might have made sense. But the theory of measurement error long ago became a more general theory of ***variation***. Errors are just one source of variation. 
:::


<details>
<summary>`r ex.mark(8.25, "IELWV", fname="Exercises/Fun/hump-intro.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/Fun/hump-intro.Rmd")`</details>


<details>
<summary>`r ex.mark(8.29, "CKSLE", fname="Exercises/Fun/sigmoid-intro.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/Fun/sigmoid-intro.Rmd")`</details>


<details>
<summary>`r ex.mark(8.33, "bllKR", fname="Exercises/Fun/sigmoid-bath.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/Fun/sigmoid-bath.Rmd")`
</details>

<details>
<summary>`r ex.mark(8.37, "EKCIE", fname="Exercises/Fun/ebola-sigmoid.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/Fun/ebola-sigmoid.Rmd")`
</details>




## The modeling cycle {#modeling-cycle}

Effective modelers treat models with skepticism. They look for ways in which models failure to capture salient features of the real world. They have an eye out for deviations between what their models show and what they believe they know about the system being modeled. They consider ways in which the models might not serve the purpose for which they were developed.  `r note(1010)`

When modelers spot a failure or deviation or lack of proper utility, they might discard the model but more often they make a series of small adjustments, tuning up the model until is successfully serves the purposes for which it is intended. 

Thus, modeling is a cyclic process of creating a model, assessing the model, and revising the model. The process comes to a sort of preliminary end when the model serves its purposes. But even then, models are often revised to check whether the results are sensitive to some factor that was not included or to check whether some component that was deemed essential really is so.


### Example: Cooling water

Looking back on the exponential fitted to the cooling water data in Section \@ref(fit-exponential), it looks like our original estimate of the half-life is a bit too small; the data doesn't seem to decay at the rate implied by $k = -0.0277$. Perhaps we should try a somewhat slower decay, say $k = -0.2$ and see if that improves things. 

::: {.scaffolding}
In the cooling water example, we're using only a subset of the data collected by Prof. Wagon. The next commands re-create that subset so that you can work with it. They also plot the data and an exponential model.  `r note(1020)`
```{r results="hide"}
# reconstruct the sample
set.seed(101)
Stans_data <- CoolingWater %>% sample_n(20)
# Plot the sample and overlay a model
gf_point(temp ~ time, data=Stans_data) %>%
  gf_lims(y = c(20, NA)) %>%
  slice_plot(25 + 83.3*exp(-.0277*time) ~ time, color="blue")
```

See if $k=-0.02$ provides a better fit to the model. (You can add another `slice_plot()` to be able to compare the original and $k=-0.02$ models.)
:::


Later in CalcZ, we'll study ***optimization***. There are optimization techniques for directing the computer to refine the parameters to best match the data. Just to illustrate, here's what we get:

```{r echo=FALSE}
set.seed(101)
Stans_data <- CoolingWater %>% sample_n(20)
```

```{r Fun-4-a-2-3, fig.cap="Polishing the fit using the rough model as a starting point."}
refined_params <- 
  fitModel(temp ~ A + B*exp(k*time), data = Stans_data, 
           start = list(A = 25, B = 83.3, k = -0.0277))
coef(refined_params)
new_f <- makeFun(refined_params)
gf_point(temp ~ time, data = Stans_data) %>%
  slice_plot(new_f(time) ~ time, color="blue")
```

The refined parameters give a much better fit to the data than our original rough estimates by eye. 

We had two rounds of the ***modeling cycle***. First, choice of an A/B expontential model and a rough estimate of the parameters A, B, and $k$. Second, refinement of those parameters using the computer.

Looking at the results of the second round, the experienced modeler can see some disturbing discrepancies. First, the estimated baseline appears to be too high. Related, the initial decay of the model function doesn't seem to be fast enough and the decay of the model function for large $t$ appears to be too slow. Prof. Stan Wagon noticed this. He used additional data to fill in the gaps for small $t$ and refined his model further by changing the basis functions in the linear combination. He hypothesized that there are at least two different cooling processes. First, the newly poured water raises the temperature of the mug itself. Since the water and mug are in direct contact, this is a fast process. Then, the complete water/mug unit comes slowly into equilibrium with the room temperature. `r note(1030)`


The newly refined model was a even better match to the data. But nothing's perfect and Prof. Wagon saw an opportunity for additional refinement based on the idea that there is a third physical mechanism of cooling: evaporation from the surface of the hot water. Prof. Wagon's additional circuits of the modeling cycle were appropriate to his purpose, which was to develop a detailed understanding of the process of cooling. For other purposes, such as demonstrating the appropriateness of an exponential process or interpolating between the data points, earlier cycles might have sufficed.

Here's a graph of the model Prof. Wagon constructed to match the data.

```{r Fun-4-a-2-4, echo=FALSE, out.width="70%", fig.align="center", fig.cap="A model that combines three exponentials provides an excellent fit."}
knitr::include_graphics(normalizePath("www/Wagon-water-fit.png"))
```

This is an excellent match to the data. But ... matching the data isn't always the only goal of modeling. Prof. Wagon wanted to make sure the model was physically plausible. And looking at the refined parameters, which include two exponential processes with parameters $k_1$ and $k_2$, he saw something wrong:

> *But what can we make of $k_1$, whose [positive value] violates the laws of thermodynamics by suggesting that the water gets hotter by virtue of its presence in the cool air? The most likely problem is that our simple model (the proportionality assumption) is not adequate near the boiling point. There are many complicated factors that affect heat transportation, such as air movement, boundary layer dissipation, and diffusion, and our use of a single linear relationship appears to be inadequate. In the next section [of our paper] we suggest some further experiments, but we also hope that our experiments might inspire readers to come up with a better mathematical model.*

The modeling cycle can go round and round!

### Example: The tides

**Step 4**: Evaluate and refine. The green model would make poor predictions. The model says "high tide" when the data say otherwise. What's missing is the ***phase*** of the sinusoid. A model that incorporates the phase is  `r note(1040)`

$${\color{blue}{\mbox{tide}(t)} \equiv 1.05 + 0.55 \sin(2\pi (t - t_0)/11)}$$

The new parameter, $t_0$, should be set to be the time of a positive-going crossing of the baseline. There's such a crossing at about time = 17. Happily, changing the phase does not itself necessitate re-estimating the other parameters: baseline, amplitude, period. This model, incorporating the phase, has been graphed in $\color{blue}\mbox{blue}$.

```{r Fun-4-a-3-4, echo=FALSE, out.width="50%", fig.show="hold", fig.cap="Shifting the **phase** of the sinusoid gives the flexibility needed to align the peaks and troughs of the model with the data. Performing this alignment for one peak makes it clear that the period is wrong."}
mod1 <- makeFun(1.05 + 0.55*sin(2*pi*hour/11) ~ hour)
mod2 <- makeFun(1.05 + 0.55*sin(2*pi*(hour-17)/11) ~ hour)
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour < 25)) %>%
  slice_plot(mod2(hour) ~ hour, color="blue") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)") %>%
  gf_lims(y =c(0, 2))
gf_line(level ~ hour, 
         data = math141Z::RI_tide %>% filter(hour > 90)) %>%
  slice_plot(mod2(hour) ~ hour, color="blue") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)")%>%
  gf_lims(y =c(0, 2))
```

For some modeling purposes, such as prediction of future tides, the phase information is impossible. For others, say, description of the amplitude of the tides, not. But getting the phase roughly right can help point out other problems. For instance, having the blue sinusoid for comparison makes it clear that the estimated period of 11 hours is too short. Maybe 13 hours would be better. Better still, since at $t=t_0 = 17$ the model is a close match to the data, let's use that as the estimate of the start of a cycle. But then, let's move much further along in the data to find another baseline crossing. To judge from the right panel, there is a baseline crossing at $t=103$. The difference between these two times is $103 - 17 = 86$ hours. Of course, the period is not 86 hours. Looking back at the whole set of data we can see 7 complete cycles between $t=17$ and $t=103$. So our new estimate of the period is $86/7 = 12.3$ hours.

With this refinement the model is
$${\color{green}{\mbox{tide}(t)} \equiv 1.05 + 0.55 \sin(2\pi (t - 17)/12.3)}$$

```{r Fun-4-a-3-5, echo=FALSE, fig.cap="With the phase about right, a better estimate can be made of the period: 12.3 hours."}
mod3 <- makeFun(1.05 + 0.55*sin(2*pi*(hour-17)/12.3) ~ hour)
gf_line(level ~ hour, data = math141Z::RI_tide) %>%
  slice_plot(mod3(hour) ~ hour, color="violet") %>%
  gf_labs(y="Water level (meters)", x="Time (hours)", title="Period 12.3 hours") %>%
  gf_lims(y =c(0, 2))
```
That's a pretty good match to the data! We might call it quits at that. First, let's polish up the parameter estimates, letting the computer do the tedious work of trying little tweaks to see if it can improve the fit.

```{r}
tide_mod <- 
  fitModel(level ~ A + B*sin(2*pi*(hour-hzero)/P),
  data = RI_tide,
  start=list(A=1.05, B=0.55, hzero=17, P=12.3))
coef(tide_mod)
```
```{r Fun-4-a-3-6, echo=FALSE, fig.cap="Polishing the parameters of the sinusoid"}
mod4 <- makeFun(tide_mod)
gf_line(level ~ hour, data = math141Z::RI_tide) %>%
  slice_plot(mod4(hour) ~ hour, color="red", npts=200) %>%
  gf_labs(y="Water level (meters)", x="Time (hours)", title="After polishing by the computer") %>%
  gf_lims(y =c(0, 2))
```
This last model seems capable of making reasonable predictions, so if we collected up-to-date data we might be able to fit a new model to predict the tide level pretty accurately a few days ahead of time. Also, the excellent alignment of the model peaks with the data tell us that the cyclic tide has a period that constant, at least so far as well can tell.  `r note(1050)`

With the period estimate $P=12.56$ hours, we can go looking for other phenomena that might account for the tides. The period of the day-night cycle is, of course 24 hours. So the tides in Providence come in and out twice a day. But not exactly. Something else must be going on. 

Isaac Newton was the first to propose that the tides were caused by the gravitational attraction of the Moon. A complete cycle of the Moon---moon rise to moon rise---takes about 50 minutes longer than a full day: the Earth revolves once every 24 hours, but in that time the Moon has moved a bit further on in its orbit of the Earth. So the Moon's period, seen from a fixed place on Earth is about 24.8 hours. Half of this, 12.4 hours, is awfully close to our estimate of the tidal period: 12.56 hours. The difference in periods, 8 minutes a day, might be hard to observe over only 4 days. Maybe with more data we'd get a better match between the tides and the moon. 

This is the modeling cycle at work: Propose a model form (A/B model with sinusoid), adjust parameters to match what we know (the Providence tide record), compare the model to the data, observe discrepancies, propose a refined model. You can stop the model when it is giving you what you need. The period 12.56 hour model seems good enough to make a prediction of the tide level a few days ahead, and is certainly better than the "two tides a day" model. But our model is not yet able to implicate precisely the Moon's orbit in in tidal oscillations.

Discrepancies between a model and data play two roles: they help us decide if the model is fit for the purpose we have in mind and they can point the way to improved models. That the tidal data deviates from the steady amplitude of our model can be a clue for where to look next. It's not always obvious where this will lead. 

Historically, careful analysis of tides led to a highly detailed, highly accurate model: a linear combination of sinusoids with diurnal periods 12.42, 12.00, 12.66, and 11.97 hours as well components with period 23.93, 25.82, 24.07, 26.87, and 24.00 hours. A tide-prediction model is constructed by finding the coefficients of the linear combination; these differ from locale to locale.  `r note(1060)`

<details>
<summary>`r ex.mark(8.39, "GSEww", fname="Exercises/Fun/sunset-in-LA.Rmd")` </summary>
`r MC_counter$reset()` `r knitr::knit_child("Exercises/Fun/sunset-in-LA.Rmd")`
</details>


