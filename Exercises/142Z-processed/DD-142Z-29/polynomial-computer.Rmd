---
title: "The Polynomial Computer"
author: "Daniel Kaplan"
runtime: shiny_prerendered
---

```{r include=FALSE, context="setup"}
etude2::load_learnr_safely()
library(etude2)
library(mosaic)
library(mosaicCalc)
library(viridis)
source("www/polycomp.R")
```


The day's topic is the translation of a continuous function of one variable, $f(x)$, whatever form it might be, into a polynomial, that is, a linear combination of power-law functions (with integer exponents):

$$f(x) \overset{?}{=}  s(x) \equiv a_0 + a_1 (x-x_0) + a_2 (x-x_0)^2 + \cdots + a_n (x-x_0)^n + \cdots $$
We've already seen that any continuous function can be approximated by a straight-line function near any given point. We have already looked extensively at using low-order polynomials (up to quadratic terms of potentially multiple variables) as a modeling tool. Now we're going to look at whether and when an approximation can be improved by adding higher-order terms such as $x^3$ and so on. And in order to say whether an approximation can be improved, we have to have some way to measure the quality of the approximation. 

In 1715, Brooke Taylor (1685-1731) introduced a method to find for any $n$ the "best" approximating polynomial. This amounts to specifying the polynomial coefficients $a_0$, $a_1$, $a_2$, and so on. Taylor produced a formula in terms of the derivatives of the function $f()$:

$$a_n = \frac{f^{(n)}(x_0)}{n!}$$
where $f^{(n)}$ means the "n^th" derivative. That is, 
$$f^{(0)}(x_0) = f(x)\left.\right|_{x_0}\\
f^{(1)}(x_0) = \partial_x f(x) \left.\right|_{x_0}\\
f^{(2)}(x_0) = \partial_{xx} f(x) \left.\right|_{x_0}\\
f^{(3)}(x_0) = \partial_{xxx} f(x) \left.\right|_{x_0}\\
\mbox{... and so on}
$$

```{r pc1-1, echo=FALSE, results="markup"}
askMC(
  "Consider $f(x) \\equiv e^x$ and take $x_0 = 0$. Use Taylor's formula to find the coefficients from $a_0$ to $a_5$. Which choice below is right?",
  "1, 1, 1, 1, 1, 1" = 
    "Remember the factorials in Taylor's formula.",
  "1, 2, 3, 4, 5, 6",
  "1, 1/2, 1/3, 1/4, 1/5, 1/6" = 
    "Remember that, say, 4! = 4 x 3 x 2",
  "+1, 1, 1/2, 1/6, 1/24, 1/120+",
  random_answer_order = FALSE
  
)
```



For Taylor, "best approximation" means that the all the orders of derivative of $f(x)$ and those of $s(x)$ match exactly at $x=x_0$. This can be proved simply by differentiating the polynomial with coefficients $a_n$ given by Taylor's formula. Or, seen another way, Taylor's formula was invented with this property in mind. Today, in contrast, people are much more likely to think of "best" as a least-squares or other statistical approximation.

Taylor's invention was important largely for reasons that are no longer relevant. There is a handful of facts based on Taylor's formula that are still useful when working algebraically with sinusoids, exponentials, and logs. Also still important is the framework for measuring the quality of the approximation, which is still important when comparing, say, Euler and other algorithms for the numerical integration of differential equations.

To understand why Taylor's invention was genuinely important in the 18th and 19th centuries, it helps to compare the technology for computing of Taylor's day to today's computing technology. Today, of course, an ordinary computer can almost instantaneously perform arithmetic to 15 digits precision. From these basic operations, software has been constructed to compute the values of many functions to a similar level of precision: `exp(0)`, `sin()` and `cos()`, `log()`, and so on. The algorithms of these functions today are different from Taylor's, but for the people developing those algorithms it was important to be able to compare their new methods to Taylor's method.

In order to facilitate the comparison of Taylor's method with that of modern computing, it helps to think about Taylor's invention as a computer, which I'll call the "polynomial computer," but which is also called "Taylor polynomials." (There is also something called "Taylor series," which are closely related but mainly of interest in pure mathematics rather than applied math, modeling, and computing.)

There were no electronic chips implementing the polynomial computer, but even in Taylor's day you could hire a computer: a skilled person who could perform the arithmetic calculations of addition, subtraction, multiplication and division. Think of this as the "hardware" of a polynomial computer. 

There's also software for the polynomial computer. Each program for the polynomial computer consisted simply of an ordered set of numbers: $a_0$, $a_1$, $a_2$, $\cdots$, and $a_n$ as well as the value $x_0$.

We can write a simulator of the polynomial computer in R. To program the simulated computer, pick the value of $x_0$ and the coefficients $a_0$ to $a_n$. Once programmed, the computer is simply a function: You give it $x$ and it gives you $f(x)$. Let's try it for a very simple polynomial: $f(x) \equiv 1 + x + x^2$. The first step is to use `poly_comp()` to define $f()$. Then we can apply $f()$ to any $x$ we wish.

Note that in $f(x) \equiv 1 - 2*x + x^2$ nowhere does $x_0$ appear. In other words, $x_0 = 0$. The coefficients are $a_0=1$, $a_1=-2$, and $a_3 = 1$.


```{r pc1-2, exercise=TRUE, exercise.cap="Demo of poly_comp()",exercise.nlines=6, eval=FALSE}
f <- poly_comp(0, 1, -2, 1) #these are the coefficients for the previous function, you should change them to answer the MC questions
slice_plot(f(x) ~ x, domain(x=c(-1,1)))
```

Using the sandbox above, re-program the polynomial computer with $x_0 = 0$ and $a_0$ through $a_4$ set to the coefficients you found earlier that match $e^x$ up to the $a_4 x^4$ term.

```{r pc1-3, echo=FALSE, results="markup"}
askMC(
  "Find $f(1)$ exactly for your 4th-order polynomial approximation to $e^x$. Which of these is it?",
  "2 8/12",
  "+2 17/24+", 
  "2 35/48", 
  "2 7/10" 
)
```

```{r pc1-4, echo=FALSE, results="markup"}
askMC(
  "Again using $f()$  for the 4th-order polynomial approximation to $e^x$, subtract $f(1)$ from $e^1$. The result will be near zero. To quantify how near, count the number of leading zeros after the decimal point. How many zeros are there?",
  1, "+2+", 3, 4,
  random_answer_order = FALSE
)
```

```{r pc1-5, echo=FALSE, results="markup"}
askMC(
  "Repeat the above calculation, but include the 5th- and 6th-order terms (that is, $a_5 x^5$ and $a_6 x^6$) when programming the polynomial computer. Subtract the new $f(1)$ from $e^1$.  How many leading zeros are there after the decimal point?",
  1, "2", "+3+", 4,
  random_answer_order = FALSE
)
```



Another mathematical question is when and whether the question mark in $\overset{?}{=}$ can be removed and equality established between $f(x)$ and a corresponding polynomial.



We tend to think of computing as a modern activity. The first electronic computers were built in the 1940s for decoding and solving ballistics problems; by 1960s computers were available to mid-sized businesses for handling accounting, inventory, and payroll; around 1980 micro-computers with mouse-based interfaces became reasonably affordable to consumers and the foundations of the internet were in place; about 1990 the World Wide Web and browsers were starting to emerge; the smart phone appeared in 2007.

But this modern history is about a certain form of computing: electronic, stored instruction, Von Neumann architecture computers. Before that there were mechanical calculators and card tabulators. And before that ...

This session is about a type of computer that started to emerge around 1700. Since it lacks an official name, we'll call it the "infinity computer," since it's based on ideas of infinitely long series and infinitesimally small differences. It's fair to say that the infinity computer was *discovered* rather than *invented*; it was put together out of technological components available by 1700 and took form as mathematicians realized the sorts of problems that could be solved by it.

A key component of the infinity computer is polynomials. These had been available for 500 years (with roots going much further back) and much of the high-school mathematics curriculum is still oriented around them. As you know, a polynomial is a function built up as a linear combination of power-law functions:

$$p(x) \equiv a_0 x^0 + a_1 x^1 + a_2 x^2 + a_3 x^3 + \cdots$$
Polynomials are "flexible" and, importantly, a polynomial function can be evaluated at any $x$ by a series of multiplications and additions, arithmetic operations that had already been mastered. 

Before Newton, polynomials were mostly used to describe shapes and generally consisted of only the first few terms: linear, quadratics, and cubics were standard forms. It was only with the advent of the infinity computer that much thought was given to the possibilities of the $\cdots$ terms.

The other key component of the infinity computer is the idea of a derivative function, introduced in the late 1600s. Already by 1700 the basic apparatus of calculating derivatives was available, e.g. the chain and product rules, symbolic forms derivatives of some basic modeling functions such as power laws and sinusoids.

The initiating idea of the infinity computer was sequences of derivatives evaluated at a single value of $x$. (We'll use $x=0$ but any point could be used.)

To illustrate, the table shows the first few derivatives of a few of our basic modeling functions, evaluated at $x=0$

$f()$ | $\partial_x f()$ | $\partial_{xx} f()$ | $\partial_{xxx} f()$ | $\partial_{xxxx} f()$ | $\cdots$
------|-------|--------|-------|--------|-----------
$\sin(x)\left.\right|_0 = 0$ | $\cos(x)\left.\right|_0 = 1$ | $-\sin(x)\left.\right|_0 = 0$ | $-\cos(x)\left.\right|_0 = -1$ | $\sin(x)\left.\right|_0 = 0$ | $\cdots$
$e^x\left.\right|_0 = 1$ | $e^x\left.\right|_0 = 1$ | $e^x\left.\right|_0 = 1$ |$e^x\left.\right|_0 = 1$ | $e^x\left.\right|_0 = 1$ | $\cdots$
$x^3\left.\right|_0 = 0$ | $3 x^2 \left.\right|_0 = 0$ | $3\cdot 2\cdot x^1\left.\right|_0 = 0$ | $3\cdot 2 \cdot 1 \cdot x^0\left.\right|_0 = 6$ | $\ \ \ \ 0$ | $\cdots$

Compare this to the first few derivatives of the polynomial $p(x)$ evaluated at $x=0$:

- $p(x = 0)\ \ \ \ \ \ \ \ =\ \ \ \  a_0$
- $\partial_{x} p(x=0)\ \ \ \ \ \ =\ \ \ \  a_1$
- $\partial_{xx} p(x=0)\ \ \ \ = \ \ \ 2\cdot a_2$
- $\partial_{xxx} p(x=0) \ \ = \ \ 3\cdot 2 \cdot a_3\ \  = \ \ 3!\, a_3$
- $\partial_{xxxx} p(x=0) = \ 4\cdot 3 \cdot 2 \cdot 1 \cdot a_4 \ \ = \ \ 4!\, a_4$
- $\cdots$

Here's a tantalizing possibility! Suppose we custom design a polynomial by picking the coefficients $a_0, a_1, a_2, \ldots$ in order to match the derivatives of the function $f(x)$. For instance, the polynomial designed to have the same derivatives as $\sin(x)$ (at $x=0$) is:

$$p_{\sin}(x) = 0 + \frac{1}{1!} x + \frac{0}{2!} x + \frac{-1}{3!}x^3! + \frac{0}{4!} x^4 + \ldots = x - x^3/3! + \ldots $$

The polynomial that matches the derivatives of $e^x$ at $x=0$ is even simpler:

$$p_{\exp}(x) = 1 + \frac{1}{1!}x + \frac{1}{2!} x^2 + \frac{1}{3!} 3^2 + \frac{1}{4!} x^4 + \cdots$$

Natural questions to ask are

$$p_{\sin}(x) \overset{?}{=} \sin(x)  \ \ \ \mbox{or} \ \ \ \ p_{\exp}(x) \overset{?}{=} e^x$$
Imagine that the answer were yes. (That turns out to be the case.) Evaluating polynomials is easy: just addition and multiplication. 
So if we can write a polynomial $p_f(x)$ that matches any (differentiable) function $f(x)$, several tasks come within our range. For instance:

1. Evaluate $f(x)$ for some $x$. Just plug in that $x$ to the polynomial, turn the arithmetic crank, and the answer appears. 
2. Integrate $f(x)$. As you remember, integration can be algebraically hard or even impossible. But integrating the terms of a polynomial, $a_n x^n$ is so easy: the answer is $\frac{a_n}{n+1} x^{n+1}$.
3. Examine carefully questions like $\lim_{x\rightarrow 0} \frac{\sin(x)}{x}$ which involve division by zero.

Generations of calculus students have been taught to program the infinity computer. That is, they have been exercises to construct the polynomial that matches $f(x)$ and to use that to solve problems (1), (2), and (3). 


EXERCISE: Write expansion for $h(x) \equiv \sqrt{x}$ at $x=1$.
