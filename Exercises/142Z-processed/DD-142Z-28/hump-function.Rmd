---
title: "The Gaussian function"
author: "Daniel Kaplan"
runtime: shiny_prerendered
---

```{r include=FALSE, context="setup"}
library(etude2)
library(mosaic)
library(mosaicCalc)
```


From the start of CalcZ we have used a small set of basic modeling functions which will by now be familiar to you:

- exponential
- logarithm: inverse to exponentials
- power-law
- sinusoid
- hump and sigmoid

This section gives a more detailed introduction to the hump function and provides a specific algebraic formula that composes an exponentials with a low-order polynomial.

Start with the low-order polynomial: $$h(x) = - \frac{(x-\mu)^2}{2 \sigma^2}$$ This is, of course a parabola with an argmax $x^\star = \mu$ and a maximum value 0. It is written this way by convention, the point of which is to give names to two features of the function:

- The **mean**, $\mu$, is the argmax of the function.
- The **variance**, $\sigma^2$, how "fat" the parabola is.

(Recall that the Greek letters $\mu$ and $\sigma$ are pronounced "mu" and "sigma" respectively.)

A reasonable person can point out that the domain of the low-order polynomial is $-\infty < x < \infty$. It's sensible to define "width" to refer to that part of the domain where the function's value is non-zero and, better, where "most" of the function is. One way to define "how much" of the function there is uses the area under the curve. The convention that you are about to see defines the width as the domain that contains the central 2/3 of the overall area.

The sandbox defines the function $h(x)$ and graphs it, with particular emphasis on the range (vertical scale!) from -1 to 0. The graph is annotated with blue and red horizontal lines with y-intercept of 0 and $-\frac{1}{2}$ respectively.

```{r hf1-1, exercise=TRUE, exercise.cap="Parameters mu and sigma", exercise.nlines = 8, eval=FALSE, warning=FALSE}
sigma <- 1
mu <- 0
h <- makeFun(  -(x-mu)^2/(2*sigma^2) ~ x, mu=mu, sigma=sigma)
slice_plot(h(x) ~ x, domain(x=sigma*c(-1.5, 1.5))) %>%
  gf_hline(yintercept = c(0,-0.5), color=c("dodgerblue", "orange3"))
```

The *width* of the parabola is based on the length of the horizontal line segment between the two branches of the parabola. Specifically, the width is defined to be *half the length of this line segment*. In order to avoid confusion, we'll use "width" in the usual English sense and a special term, **standard deviation**, to refer to half the length of the line segment. ("Standard deviation" is a good candidate for the worst name ever for a simple concept: a length. Another, equivalent term you will hear for this length is "root mean square," which is almost as bad. Still, those are the standard terms and you should be careful to use instead of non-standard alternatives.)

```{r rf1-2, echo=FALSE, results="markup"}
askMC(
  "In the above sandbox, set `sigma <- 2`. What *standard deviation* corresponds to $\\sigma = 2$?",
  "1/4", 1, "$\\sqrt{2}$", "+2+", 4,
  random_answer_order = FALSE
)
```

```{r rf1-3, echo=FALSE, results="markup"}
askMC(
  "Still holding $\\sigma = 2$, what is the *variance* of the function?",
  "1/4", 1, "$\\sqrt{2}$", "2", "+4+",
  random_answer_order = FALSE
)
```

```{r rf1-4, echo=FALSE, results="markup"}
askMC(
  "Set $\\sigma = 3$, and read off the graph what is the *standard deviation* of the function?",
  "1/3", 1, "$\\sqrt{3}$", "+3+", "9",
  random_answer_order = FALSE
)
```

```{r rf1-5, echo=FALSE, results="markup"}
askMC(
  "When $\\sigma = 3$, what is the *variance* parameter?",
  "1/3", 1, "$\\sqrt{3}$", "3", "+9+",
  random_answer_order = FALSE
)
```

```{r rf1-6, echo=FALSE, results="markup"}
askMC(
  "Pick a $\\sigma$ of your choice, try a few non-zero values for $\\mu$. Read off from the graph the standard deviation. How does the standard deviation depend on $\\mu$?",
  "The standard deviation is $\\mu + \\sigma$.",
  "The standard deviation is $\\sqrt{\\strut\\mu + \\sigma^2}$.",
  "The standard deviation is $\\ln(\\mu + e^\\sigma)$.",
  "+The standard deviation is not affected by $\\mu$.+"
)
```

```{r rf1-7, echo=FALSE, results="markup"}
askMC(
  "What is the relationship between the *variance* $\\sigma^2$ and the *standard deviation* $\\sigma$? ",
  "+The standard deviation is the square root of the variance.+",
  "The standard deviation is 1 over the variance.",
  "They are completely unconnected concepts.",
  "The variance is the square of the mean, $\\mu$."
)
```

One of the features that make hump functions useful is that they are **local**, the function is practically zero except on a domain of limited width. 
The parabola $h(x) \equiv - \frac{(x - \mu)^2}{2 \sigma^2}$ is non-zero everywhere except at $x = \mu$, so not at all local. 

To produce our hump function, we compose an exponential function $e^x$ with the polynomial $h(x)$ to get $$f(x) \equiv e^{h(x)} = \exp\left(-\frac{(x - \mu)^2}{2 \sigma^2}\right)$$ This is not yet our "official" hump function, but are getting close!

The sandbox defines $f(x)$ and graphs it. As with the previous sandbox, the graph is annotated with a blue horizontal line that touches the curve at the argmax and a red horizontal line with a y-intercept at $e^{-1/2}$.

```{r hf1-8, exercise=TRUE, exercise.cap="Parameters mu and sigma", exercise.nlines=10, eval=FALSE, warning=FALSE}
sigma <- 1
mu <- 0
f <- makeFun(  exp( -(x-mu)^2/(2*sigma^2) ) ~ x, mu=mu, sigma=sigma)
slice_plot(f(x) ~ x, domain(x=sigma*c(-3.5, 3.5))) %>%
  gf_hline(yintercept = c(0,exp(-0.5)), color=c("dodgerblue", "orange3"))
sigmoid <- antiD(g(x) ~ x, lower.bound = -Inf)
# Graph the sigmoid on the domain -10 < x < 10

```

Notice that the vertical range of the function is $0 <  f(x) \leq 1$. The argmax is $\mu$, so $f(\mu) = 1$. This seems easy and convenient, but one of the purposes of the standard hump function is to define a standard *sigmoid* function. It's the sigmoid that we want to have a range from 0 to 1. 

How to scale the hump $f(x)$ to produce a sigmoid with the range 0 to 1? Recall that a sigmoid is the anti-derivative of the corresponding hump. In the sandbox, we use `antiD()` to compute $$\int_{-\infty}^x f(u) du$$, which we called `sigmoid()`. From the graph of `sigmoid()` you can read off the scaling factor that will make the vertical range of the resulting sigmoid zero to one.

```{r hf1-9, echo=FALSE, results="markup"}
askMC(
  "Where did the variable $u$ come from in $$\\int_{-\\infty}^x f(u) du ?$$",
  "The instructors made a mistake. They will try to be more careful in the future!",
  "You can use any name for the variable of integration. The function $\\int_{-\\infty}^x f(u) du$ will be a function of $u$, not $x$.",
  "+You can use any name for the variable of integration. The function $\\int_{-\\infty}^x f(u) du$ will be a function of $x$, not $u$.+",
  "$u$ is the Latin equivalent of $\\mu$." = "Actually, not. $\\mu$ is equivalent to $m$, as in the word \"mean\"."
)
```

```{r hf1-10, echo=FALSE, results="markup"}
askMC(
  "This question will take a bit of detective work in the sandbox. Make a table of a few combinations of different values for $\\mu$ and $\\sigma$. For each combination, find the maximum value of the corresponding `sigmoid()` function. Using this data, choose the correct formula for the maximum value of the sigmoid as a function of $\\mu$ and $\\sigma$.",
  "+$\\sqrt{\\strut 2 \\pi \\sigma^2}$+",
  "$\\pi \\sigma$",
  "$\\sqrt(\\strut \\mu \\pi \\sigma)$",
  "$\\sqrt(\\strut \\pi \\sigma^2 / \\mu)$"
)
```

Putting all this together, we arrive at our "official" standard hump function, called the *Gaussian* function:

$$g(x) \equiv \frac{1}{\sqrt{\strut2\pi\sigma^2}} e^{-(x-\mu)^2 / 2 \sigma^2}$$
The Gaussian function is important to many quantitative disciplines and has a central role in statistics. The function is named after physicist and mathematician Carl Friedrick Gauss (1777-1855). But in the social sciences, it is usually called the "normal" function; that's how common it is.

In R, the Gaussian function is provided as `dnorm(x, mu, sigma)`. (The corresponding sigmoid, that is, the anti-derivative of `dnorm()` with respect to `x` is available as `pnorm(x, mu, sigma)`.

The Gaussian function is so important, that it's worth pointing out some recognizable landmarks in the algebraic expression. Knowing to look for such things is one trait that defines an expert.

1. The term $1/\sqrt{2 \pi \sigma^2}$ is not a function of $x$, it is a constant related to the *variance* $\sigma^2$. It's just a number arranged to make $$\int_{-\infty}^\infty g(x) dx = 1 .$$
2. The exponential means that, whatever the values of $\mu$ and $\sigma$, the function value $0 < g(x)$.
3. The argmax is at $x^\star = \mu$. This is also the "center" of the function, which is symmetrical around the argmax.
4. The *variance* $\sigma^2$ appears directly in the formula. In no place does $\sigma$, the *standard deviation*, appear without the exponent 2. This is a hint that the variance is more fundamental than the standard deviation. 

```{r hf1-11, echo=FALSE, results="markup"}
askMC(
"Go back to the sandbox where we defined the function `f(x)` and graphed it. The function has a distinctive shape which is almost always described in terms of a familiar word. Which one is it?",
"breadloaf-shaped",
"ghost-shaped",
"+bell-shaped+",
"sombrero-shaped" = "The Gaussian isn't sombrero shaped, but $\\partial_{xx} g(x)$ is!"
)
```

**Optional background**. Now for a bit of irony. We've taken a lot of care to define a specific form of hump function with a formula that strikes many people as complicated. It must be that all these specifics are important, right? In reality, any function with a roughly similar shape would work in pretty much the same way. We could have defined our "official" hump function in any of a number of ways, some of which would be algebraically simpler. But the specific gaussian shape is a kind of fixed point in the differential equation that we'll study next.

<!--
The Gaussian shape is preferred in much the same way as the "natural logarithm" is natural. The "natural" only makes sense in terms of the results of a specific operation, differentiation, which has a particularly simple form for $\ln(x)$. As you recall, $\partial_x \ln(x) = 1/x$. Although the derivatives of log-base 2 and log-base 10 (and any other log) are similar, they have slightly more complicated derivatives:

- $\partial_x\, \ln(x) = 1/x$
- $\partial_x\, \log_{10}(x) = 1 / \ln(10) x$
- $\partial_x\, \log_2(x) = 1 / \ln(2) x$

In a bit, you'll see that something similar is going on with the Gaussian function: it's exact and simple in a particular context in a way that other similarly shaped functions are not.

-->

