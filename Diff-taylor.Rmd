# Polynomials

A big part of the high-school algebra curriculum is about polynomials.
In some ways, this is appropriate since polynomials played an outsized
part in the historical development of mathematical theory. Indeed, the
so-called "Fundamental theorem of algebra" is about
polynomials.[^diff-taylor-1]

[^diff-taylor-1]: The fundamental theorem says that an order-n
    polynomial has n roots (including multiplicities).

For modelers, polynomials are a mixed bag. They are very widely used in modeling. Sometimes this is entirely appropriate, for instance the low-order polynomials that are the subject of Chapter \@ref(local-approximations). The problems come when high-order
polynomials are selected for modeling purposes. Building a reliable model with high-order polynomials requires a deep knowledge of mathematics, and introduces serious potential pitfalls. Modern professional modelers learn the alternatives to high-order polynomials, but newcomers often draw on their experience in high-school and give unwarranted credence to polynomials. This chapter attempts to guide you to the ways you are likely to see polynomials in your future work and to help you avoid them when better alternatives are available.

## Basics of polynomials {#polynomial-basics}

As you know, a polynomial is a ***linear combination*** of a particular class of functions: power-law functions with non-negative, integer exponents: 1, 2, 3, .... The individual functions are called ***monomials***, a word use that echoes the construction of chemical polymers out of monomers; for instance, the material *polyester* is constructed by chaining together a basic chemical unit called an [*ester*](https://en.wikipedia.org/wiki/Ester)*.*

In one variable, say $x$, the monomials are
$x^1, x^2, x^3$, and so on. (There's also $x^0$, but that's better thought of as the constant function.) An ***n-th order*** polynomial has monomials up to exponent $n$. For example, the form of a third-order polynomial is
$$a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3$$
High-order polynomials are rarely used for multiple inputs because they are verbose and unreliable. Consider the third-order polynomial form in $x$ and $y$:
$$\underbrace{b_0 + b_x x + b_y y}_\text{first-order terms} + \underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\text{second-order terms} + \underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\text{third-order terms}$$ Too many terms! Consequently, we'll limit this chapter to polynomials in one variable.

The ***domain*** of polynomials, like the power-law functions they are made from, is the entire number line: $-\infty < x < \infty$. But for the purposes of understanding the shape of high-order polynomials, it's helpful to divide the domain into three parts: a ***wriggly domain*** at the center and two ***tail domains*** to the right and left of the center.

```{r wriggly-polynomial, echo=FALSE, fig.cap="A $n$th-order polynomial can have up to $n-1$ critical points that it wriggles among. A 7-th order polynomial is shown here in which there are six local maxima or minima alternatingly."}
set.seed(101)
n <- 8
Pts <- tibble(x = runif(n, -3, 3), y = rnorm(n))
mod <- lm(y ~ poly(x, n-1), data = Pts)
fmod <- makeFun(mod)
slice_plot(fmod(x) ~ x, domain(x=c(-2.8, 1.65))) %>%
  gf_vline(xintercept = ~ -2.65, color="tan") %>%
  gf_vline(xintercept = ~ 1.6, color="tan") %>%
  gf_text(25 ~ -.5, label="Wriggly domain", color="tan", size=10) %>%
  gf_lims(x=c(-3.5,2.5)) %>%
  gf_text(-20 ~ -3.1, label="Negative tail domain", color="tan", angle=90, size=8) %>%
  gf_text(-20 ~ 2.2, label="Positive tail domain", color="tan", angle=-90, size=8)
```
Figure \@ref(fig:wriggly-polynomial) shows a 7-th order polynomial---that is, the highest-order term is $x^7$. In one of the tail domains the function value heads off to $\infty$, in the other to $-\infty$.

This is always the case with odd-order polynomials: 1, 3, 5, 7, .... For even-order polynomials, the function value in the two tail domains go in the same direction, either both to $\infty$ (Hands up!) or both to $-\infty$.  

In the wriggly domain in Figure \@ref(fig:wriggly-polynomial), there are six argmins or argmaxes. An $n$th-order polynomial can have up to $n-1$ extrema.

Note that the local polynomial approximations in Chapter \@ref(local-approximations) are at most 2-nd order and so there is at most 1 wriggle: a unique argmax. If the approximation does not include the quadratic terms ($x^2$ or $y^2$) then there is no argmax for the function. 

## High-order approximations

We've discussed first- and second-order polynomial approximations and their valuable contribution to modeling technique in Chapter \@ref(local-approximations). From now on, we'll focus only on polynomials of order three or higher. 

The potential attraction of high-order polynomials is that, with their wriggly interior, they can take on a large number of appearances. This chameleon-like behavior has historically made them the tool of choice for understanding the behavior of approximations. That theory has motivated the use of polynomials for modeling patterns in data, but, paradoxically, has shown that high-order polynomials should not be the tool of choice for modeling data. The mathematical background needed for those better tools won't be available to us until Block 5, when we explore linear algebra. 

Let's start the story before the invention of the linear-algebra tools, when polynomials were well understood. 

Polynomial functions lend themselves well to calculations, since the output from a polynomial function can be calculated using just the basic arithmetic functions: addition, subtraction, multiplication, and division. To illustrate, consider this polynomial: $$g(x) \equiv x - \frac{1}{6} x^3$$
This is a third-order polynomial, since the highest-order term is $x^3$. The coefficients have been selected for the purpose of the illustration to be easy to handle by mental arithmetic. For instance, for $g(x=1)$ is $5/6$. Similarly, $g(x=1/2) = 23/48$ and $g(x=2) = 2/3$. A person of today's generation would use an electronic calculator for more complicated inputs, but the mathematicians of Newton's time were accomplished human calculators.  It would have been well within their capabilities to calculate, using paper and pencil^[Unfortunately for these human calculators, pencils weren't invented until 1795. Prior to the introduction of this advanced, graphite-based computing technology, mathematicians had to use quill and ink.], $g(\pi/4) = 0.7046527$. 

Our example polynomial, $g(x) \equiv x - \frac{1}{6}x^3$, graphed in color in Figure \@ref(fig:small-sine),  doesn't look like any of our pattern book functions. But, for a small interval around $x=0$, it has a great similarity to the sinusoid.


```{r small-sine, echo=FALSE, fig.cap="The polynomial $g(x) \\equiv x -x^3 / 6$ is remarkably similar to $\\sin(x)$ near $x=0$."}
slice_plot(sin(x) ~ x, domain(x=c(-2.85, 2.85)), size=3, alpha=0.25) %>%
  slice_plot(x - x^3/6 ~ x, color="magenta")
```

It's clear from the graph that the approximation is excellent near $x=0$ and gets worse as $x$ gets larger. The approximation is poor for $x \approx \pm 2$. We know enough about polynomials to say that the approximation will not get better for larger $x$; the sine function has a range of $-1$ to $1$, while the left and right tails of the polynomial are heading off to $\infty$ and $-\infty$ respectively.


One way to quantify the quality of the approximation is to look at the ***error***, that is the difference between the actual sinusoid and $g(x)$, that is ${\cal E}(x) \equiv |\strut\sin(x) - g(x)|$. The point of the absolute value used in defining the error is that we're interested in how ***far*** the approximation is from the actual function and not so much in whether the approximation is below or above the actual function. Figure \@ref(fig:sin-error) shows ${\cal E}(x)$ as a function of $x$.


```{r sin-error,  echo=FALSE, fig.cap = "The error ${\\cal E}(x)$ of $x - x^3/6$ as an approximation to $\\sin(x)$. Left panel: linear scale. Right panel: log-log scale."}
P <- slice_plot(abs(sin(x) - (x - x^3/6)) ~ x, domain(x=c(0.001, 3)), npts=500) %>% 
  gf_labs(y = "Error(x)")
P
breaksx <- c(.001, .003, .005, 0.01,0.03,0.05, 0.1, 0.3, 0.5, 1, 3)
P %>%
  gf_refine(scale_y_log10(breaks=10^(-17:0)), scale_x_log10(breaks=breaksx, labels=as.character(breaksx)))

```



Figure \@ref(fig:sin-error) shows that for $x < 0.3$, the error in the polynomial approximation to $\sin(x)$ is in the 5th decimal place. For instance, $\sin(0.3) = 0.2955202$ while $g(0.3) = 0.2955000$. 



That the graph of ${\cal E}(x)$ is a straight-line on log-log scales diagnoses ${\cal E}(x)$ as a power law. That is: ${\cal E}(x) = A x^p$. As always for power-law functions, we can estimate the exponent $p$ from the slope of the graph. It's easy to see that the slope is positive, so $p$ must also be positive.

The inevitable consequence of ${\cal E}(x)$ being a power-law function with positive $p$ is that $\lim_{x\rightarrow 0} {\cal E}(x) = 0$. That is, the polynomial approximation $x - \frac{1}{6}x^3$ is *exact* as $x \rightarrow 0$.

Throughout this book, we've been using straight-line approximations to functions around an input $x_0$. The slope of the tangent-line approximation is the derivative of the function at $x_0$. We can always choose the coefficients of a polynomial approximation to be at least as good as the tangent-line approximation. For a function $f(x)$ that we want to approximate at $x=x_0$ with a polynomial, just start with the polynomial 
$$g(x) = f(x_0) + \partial_x f(x_0) [x-x_0]$$
Notice that $g(x=x_0) = f(x=x_0)$, and $\partial_x g(x=x_0) = \partial_x f(x=x_0)$: the approximation matches the function's value and its derivative at $x_0$. One way to construct a high-order polynomial approximation is to follow the same logic: structure $g(x)$ such that $g(x_0) = f(x_0)$ and $\partial_x g(x=x_0) = \partial_x f(x=x_0)$, $\partial_{xx} g(x=x_0) = \partial_{xx} f(x=x_0)$, and so on for the higher order derivatives.


The table below compares the function value and the derivatives at input $x = x_0 = 0$ for $\sin(x)$ to the polynomial approximation $g(x)\equiv x - \frac{1}{6} x^3$.

Order | $\sin(x)$ derivative | $x - \frac{1}{6}x^3$ derivative ------|----------------------|--------------------------------   0   | $\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ | $\left( 1 - \frac{1}{6}x^3\right)\left.{\Large\strut}\right|_{x=0} = 0$ 
  1   | $\cos(x) \left.{\Large\strut}\right|_{x=0} = 1$ | 
$\left(1 - \frac{3}{6} x^2\right) \left.{\Large\strut}\right|_{x=0}= 1$
  2   | $-\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ | 
$\left(- \frac{6}{6} x\right) \left.{\Large\strut}\right|_{x=0} = -1$
  3   |  $-\cos(x) \left.{\Large\strut}\right|_{x=0} = -1$ |
$- 1\left.{\Large\strut}\right|_{x=0} = -1$ 
  4   |  $\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ |
$0\left.{\Large\strut}\right|_{x=0} = 0$ 

The first four derivatives of $x - \frac{1}{6} x^3$ exactly match, at $x=0$, the first four derivatives of $\sin(x)$.

The polynomial $h(x) \equiv a_0 + a_1 [x-x_0]^1 + a_2 [x-x_0]^2 + \cdots + a_n [x-x_0]^n$, with coefficients $a_0, a_1, a_2, ...$ selected to match the function value $f(x=x_0)$ as well as the first $n$ derivatives $\partial_x f(x=x_0)$, $\partial_{xx} f(x=x_0)$, $\partial_{xxx} f(x=x_0)$, ... is called the ***Taylor polynomial*** approximation to $f(x)$ around the input $x_0$.

::: {.workedexample   data-latex=""}
Find the 4th-order Taylor polynomial approximation to $f(x) = e^x$ around $x=0$.

We know it will be a 4th order polynomial:
$$\text{Taylor}(x) \equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4$$
The exponential function is particularly nice for examples because the function value and all it's derivatives are identical: $e^x$. So $$f(x= 0) \color{magenta}{=} \partial_x f(x=0) \color{magenta}= \partial_{xx} f(x=0) \color{magenta}= \partial_{xxx} f(x=0) \color{magenta}= \partial_{xxxx} f(x=0) \color{magenta}= 1$$
The function value and derivatives of $\text{Taylor}(x)$ at $x=0$ are:
$$
\text{Taylor}(x=0) = a_0\\
\,\\
\partial_{x}\text{Taylor}(x=0) = a_1\\
\,\\
\partial_{xx}\text{Taylor}(x=0) = 2 a_2\\
\,\\
\partial_{xxx}\text{Taylor}(x=0) = 2\cdot3\cdot a_3\\
\,\\
\partial_{xxxx}\text{Taylor}(x=0) = 2\cdot3\cdot4 \cdot a_4
$$
Matching these to the exponential evaluated at $x=0$, we get
$$a_0 = 1\\
\,\\
a_1 = 1\\
\,\\
a_2 = \frac{1}{2}\\
\,\\
a_3 = \frac{1}{2 \cdot 3}\\
\,\\
a_4 = \frac{1}{2 \cdot 3 \cdot 4}
$$
So the 4th-order Taylor polynomial approximation to the exponential at $x=0$ is $$g(x) = 1 + x + \frac{1}{2} x + \frac{1}{2\cdot 3} x^2 + \frac{1}{2\cdot 3} x^3 +\frac{1}{2\cdot 3\cdot 4} x^4$$

Figure \@ref(fig:taylor-exp-4) shows the exponential function $e^x$ and its 4th-order Taylor polynomial approximation near $x=0$:

```{r taylor-exp-4, echo=FALSE, class.source="code-hide", fig.cap="The 4th-order Taylor polynomial approximation to $e^x$ arount $x=0$"}
slice_plot(exp(x) ~ x, domain(x=c(-2,2)), size=3, alpha = 0.25) %>%
  slice_plot(1 + x + x^2/2 + x^3/6 + x^4 / 24 ~ x, color="magenta")
```

The polynomial is exact at $x=0$ but the ***error*** ${\cal E}(x)$ grows with increasing distance from $x=0$:

```{r taylor-exp-5, echo=FALSE, class.source="code-hide", fig.cap="The error from a 4th-order Taylor polynomial approximation to $e^x$ around $x=0$.", out.width="50%", fig.show="hold"}
Pts <- tibble(x = seq(-2, 2, length=100),
              y = exp(x),
              y_approx = 1 + x + x^2/2 + x^3/6 + x^4/24,
              y_error = abs(y - y_approx),
              x_abs = abs(x))
gf_path(y_error ~ x, data = Pts, color=~sign(x)) %>%
  gf_labs(y="Error(x)") %>%
  gf_refine(scale_color_continuous(guide = 'none'))
gf_path(log10(y_error) ~ log10(x_abs), data = Pts, color=~sign(x)) %>%
  gf_labs(y="Log10(Error(x))") %>%
  gf_refine(scale_color_continuous(guide = 'none'))
```

From the plot of $\log_{10} {\cal E}(x)$ versus $\log_{10} | x |$ in Figure \@ref(fig:taylor-exp-5)you can see that the error grows from zero at $x=0$ as a power-law function. Measuring the exponent of the power-law from the slope of the graph on log-log axes give ${\cal E}(|x|) = a |x|^5$. This is typical of Taylor polynomials: for a polynomial of degree $n$, the error will grow as a power-law with exponent $n+1$. This means that the higher is $n$, the faster $\lim_{x\rightarrow x_0}{\cal E}(x) \rightarrow 0$. On the other hand, since ${\cal E}_x$ is a power law function, as $x$ gets further from $x_0$ the error grows as as $\left(x-x_0\right)^{n+1}$
:::

## l'Hopital's rule

One task for which the polynomial computer is extremely well suited is
the resolution of singularities. A singularity is an input for which the
function involves division by zero, for instance:

$$g(x) \equiv \frac{\sin(x)}{x}\ \ \ \text{at}\ \ \ x=0$$

Since division by zero is undefined, there's no way to do a numerical
computation at $x=0$. Even computer arithmetic is set up to recognize
this:

```{r echo=TRUE}
g <- makeFun(sin(x)/x ~ x)
g(0)
```

The output `NaN` stands for "not a number." It is as if the computer is
throwing up its hands and saying, "I don't know what to do with this."

Actually, the computer doesn't get so frustrated at all such
division-by-zero problems, for instance

```{r echo=TRUE}
h <- makeFun(1/x ~ x)
h(0)
```

The output `Inf` is also not a number, but here the computer is willing
to say that whatever 1/0 might be, it's very large: infinity.

Why does the computer make a distinction between the kind of
divide-by-zero in $\sin(x)/x$ and the kind in $1/x$. The answer is that
the first function involves a numerator, $\sin(x)$ that will also be
zero when $x=0$. It's the zero-over-zero that prompts the `NaN`
response:

```{r echo=TRUE}
0 / 0
```

The numerator must literally be zero. Being very close to zero doesn't
cut it.

```{r echo=TRUE}
0.000000000000000000000000000000000000000000000001 / 0
```

The polynomial computer provides another approach to sorting out what
$\sin(x)/x$ and similar functions might be at $x=0$. And the key thing
is the word "approach". The sandbox carries out the $\sin(x)/x$
calculation for $x$ very small but not zero. See what you get.

```{r zoz1-1, exercise=TRUE, exercise.cap="Divide by tiny", exercise.nlines=5, eval=FALSE}
x <- 0.00000001
sin(x) / x
```

Although $\sin(x) / x$ is not defined at $x=0$, it is defined
*everywhere* else. Recall that the idea of a **limit** is to find a
value to stand in for the undefined $\sin(0)/0$ by making $x$ very small
and seeing what you get. If you get something sensible for very small
$x$, and get the same thing for *even smaller* $x$, then we have a
reasonable claim for what value to insert for $\sin(x)/x$.


LHOPITAL EXERCISES HERE


Conventionally, the relationship
$$\lim_{x\rightarrow x_0} \frac{u(x)}{v(x)} = \lim_{x\rightarrow x_0} \frac{\partial_x u(x)}{\partial_x v(x)}$$
is called "L'Hopital's Rule" after the author of the very first Calculus
textbook where the rule was first published. Here's the title page from
the second edition of 1716.

```{r echo=FALSE, out.width="50%", fig.align="center"}
knitr::include_graphics("www/lhopital-cover.png")
```

::: {.intheworld data-latex=""} 
[Brooke Taylor](https://en.wikipedia.org/wiki/Brook_Taylor) (1685-1731), a near contemporary of Newton, published his work on approximating polynomials in 1715. Wikipedia reports: "[T]he importance of [this] remained unrecognized until 1772, when Joseph-Louis Lagrange realized its usefulness and termed it 'the main [theoretical] foundation of differential calculus'."[Source](https://en.wikipedia.org/wiki/Brook_Taylor#/media/File:Taylor_Brook_Goupy_NPG.jpg)

```{r brook-taylor, echo=FALSE, fig.cap="Brook Taylor", out.width = "40%", fig.align="center"}
knitr::include_graphics(normalizePath("www/Brook_Taylor.jpg"))
```

Taylor's work preceded by about a century the development of techniques for working with data. One of the pioneers in these new techniques was Carl Friedrich Gauss (1777-1855), after whom the gaussian function is named. Gauss's techniques are the foundation of an incredibly important statistical method that is ubiquitous today: ***least squares***. Least squares provides an entirely different way to find the coefficients on approximating polynomials (and an infinite variety of other function forms). The R/mosaic `fitModel()` function for polishing parameter estimates is based on least squares. In Block 5, we'll explore least squares and the mathematics underlying the calculations of least-squares estimates of parameters.

Due to the importance of Taylor polynomials in the development of calculus, many students assume their use extends to constructing models from data. They also assume that third- and higher-order monomials are a good basis for modeling data. Both these assumptions are wrong. Least squares is the proper foundation for working with data.
:::



`r insert_calcZ_exercise("26.05", "k8H2vs", "Exercises/Diff/bee-throw-mug.Rmd")`



`r insert_calcZ_exercise("XX.XX", "uebqnO", "Exercises/Diff/fir-shut-coat.Rmd")`


`r insert_calcZ_exercise("XX.XX", "682lsB", "Exercises/Diff/local-shift.Rmd")`


`r insert_calcZ_exercise("XX.XX", "ecdVKx", "Exercises/Diff/frog-throw-screen.Rmd")`

`r insert_calcZ_exercise("XX.XX", "KmDiXI", "Exercises/Diff/girl-wake-bottle.Rmd")`

`r insert_calcZ_exercise("XX.XX", "IlNSF0", "Exercises/Diff/fawn-hear-kayak.Rmd")`

`r insert_calcZ_exercise("XX.XX", "cQT2v6", "Exercises/Diff/dolphin-mean-linen.Rmd")`

`r insert_calcZ_exercise("XX.XX", "2i2QSR", "Exercises/Diff/pine-burn-plate.Rmd")`

`r insert_calcZ_exercise("XX.XX", "13a7wI", "Exercises/Diff/rhinosaurus-toss-gloves.Rmd")`



<!--
## Fitting polynomials

Taylor polynomials provide a means to approximate continuous and smooth functions around a center $x_0$. So long as $x$ is very close to $x_0$, the approximation will be excellent. But Taylor polynomials aren't a solution to every problem. Consider the piecewise continuous function, $ramp(x-1)$, in Figure \@ref(fig:ramp-Taylor).

```{r ramp-Taylor, echo=FALSE, fig.cap="A piecewise continuous ramp function (gray) together with its Taylor polynomial (magenta) centered on $x_0 = 0$."}
ramp <- makeFun(ifelse(x-1 < 0, 0, x-1) ~ x)
slice_plot(ramp(x) ~ x, size = 3, alpha = 0.25, domain(x=c(-2, 4))) %>%
  slice_plot(0 ~ x, color="magenta") %>%
  gf_text(0.25 ~ 3, label="Taylor poly.",color = "magenta")

```
The value of $ramp(x-1) \left.\Large\right|_{x=0}$ is zero, as is the value of the first, second, third, and every other derivative. Whatever we choose for the order $n$ of the Taylor polynomial, it will be $\text{Taylor(x) = 0}$. That's an excellent approximation to $ramp(x-1)$ around $x=0$! But it misses the point of $ramp()$ entirely.

Or consider the problem that introduced this chapter: finding an arithmetic process to evaluate $\sin(x)$. As it happens, we only need to be able to evaluate $\sin(x)$ on the interval $0 \leq x \leq \pi/2$.^[If $x$ is outside this range, add or subtract a multiple $k \in [\ldots, -2, -1, 0, 1, 2, \ldots]$ of $\pi$ so  $0 \leq x - k \pi \leq \pi$. Then, if $\pi/2 \leq (x - k \pi)$, calculate $\sin(\pi - (x - k \pi))$ 

FIT TO PART OF SINE

```{r}
Pts <- tibble(x = seq(0, pi/2, length=1000), y = sin(x))
mod <- lm(y ~ x + I(x*x) + I(x*x*x) - 1, data = Pts)
mod
fmod <- makeFun(mod)
slice_plot(sin(x) ~ x, domain(x=c(0, pi/2)), size=3, alpha=0.25) %>%
  slice_plot(fmod(x) ~ x, color="magenta")
slice_plot(sin(x) - fmod(x) ~ x, domain(x=c(0, pi/2))) %>%
  slice_plot(sin(x) - (x - x^3/6) ~ x, color="green")
```

-->
