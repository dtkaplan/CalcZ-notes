---
editor_options: 
  markdown: 
    wrap: 72
---

# Polynomials

A big part of the high-school algebra curriculum is about polynomials.
In some ways, this is appropriate since polynomials played an outsized
part in the historical development of mathematical theory. Indeed, the
so-called "Fundamental theorem of algebra" is about
polynomials.[^diff-taylor-1]

[^diff-taylor-1]: The fundamental theorem says that an order-n
    polynomial has n roots (including multiplicities).

For modelers, polynomials are a mixed bag. They are very widely used in modeling. Sometimes this is entirely appropriate, for instance the low-order polynomials that are the subject of Chapter \@ref(local-approximations). The problems come when high-order
polynomials are selected for modeling purposes. Building a reliable model with high-order polynomials requires a deep knowledge of mathematics, and introduces serious potential pitfalls. Modern professional modelers learn the alternatives to high-order polynomials, but newcomers often draw on their experience in high-school and give unwarranted credence to polynomials. This chapter attempts to guide you to the ways you are likely to see polynomials in your future work and to help you avoid them when better alternatives are available.

## Basics of polynomials {#polynomial-basics}

As you know, a polynomial is a ***linear combination*** of a particular class of functions: power-law functions with non-negative, integer exponents: 1, 2, 3, .... The individual functions are called ***monomials***, a word use that echoes the construction of chemical polymers out of monomers; for instance, the material *polyester* is constructed by chaining together a basic chemical unit called an [*ester*](https://en.wikipedia.org/wiki/Ester)*.*

In one variable, say $x$, the monomials are
$x^1, x^2, x^3$, and so on. (There's also $x^0$, but that's better thought of as the constant function.) An ***n-th order*** polynomial has monomials up to exponent $n$. For example, the form of a third-order polynomial is
$$a_0 + a_1 x^1 + a_2 x^2 + a_3 x^3$$
High-order polynomials are rarely used for multiple inputs because they are verbose and unreliable. Consider the third-order polynomial form in $x$ and $y$:
$$\underbrace{b_0 + b_x x + b_y y}_\text{first-order terms} + \underbrace{b_{xy} x y + b_{xx} x^2 + b_{yy} y^2}_\text{second-order terms} + \underbrace{b_{xxy} x^2 y + b_{xyy} x y^2 + b_{xxx} x^3 + b_{yyy} y^3}_\text{third-order terms}$$ Too many terms! Consequently, we'll limit this chapter to polynomials in one variable.

The ***domain*** of polynomials, like the power-law functions they are made from, is the entire number line: $-\infty < x < \infty$. But for the purposes of understanding the shape of high-order polynomials, it's helpful to divide the domain into three parts: a ***wriggly domain*** at the center and two ***tail domains*** to the right and left of the center.

```{r wriggly-polynomial, echo=FALSE, fig.cap="A $n$th-order polynomial can have up to $n-1$ critical points that it wriggles among. A 7-th order polynomial is shown here in which there are six local maxima or minima alternatingly."}
set.seed(101)
n <- 8
Pts <- tibble(x = runif(n, -3, 3), y = rnorm(n))
mod <- lm(y ~ poly(x, n-1), data = Pts)
fmod <- makeFun(mod)
slice_plot(fmod(x) ~ x, domain(x=c(-2.8, 1.65))) %>%
  gf_vline(xintercept = ~ -2.65, color="tan") %>%
  gf_vline(xintercept = ~ 1.6, color="tan") %>%
  gf_text(25 ~ -.5, label="Wriggly domain", color="tan", size=10) %>%
  gf_lims(x=c(-3.5,2.5)) %>%
  gf_text(-20 ~ -3.1, label="Negative tail domain", color="tan", angle=90, size=8) %>%
  gf_text(-20 ~ 2.2, label="Positive tail domain", color="tan", angle=-90, size=8)
```
Figure \@ref(fig:wriggly-polynomial) shows a 7-th order polynomial---that is, the highest-order term is $x^7$. In one of the tail domains the function value heads off to $\infty$, in the other to $-\infty$.

This is always the case with odd-order polynomials: 1, 3, 5, 7, .... For even-order polynomials, the function value in the two tail domains go in the same direction, either both to $\infty$ (Hands up!) or both to $-\infty$.  

In the wriggly domain in Figure \@ref(fig:wriggly-polynomial), there are six argmins or argmaxes. An $n$th-order polynomial can have up to $n-1$ extrema.

Note that the local polynomial approximations in Chapter \@ref(local-approximations) are at most 2-nd order and so there is at most 1 wriggle: a unique argmax. If the approximation does not include the quadratic terms ($x^2$ or $y^2$) then there is no argmax for the function. 

## High-order approximations

We've discussed first- and second-order polynomial approximations and their valuable contribution to modeling technique in Chapter \@ref(local-approximations). From now on, we'll focus only on polynomials of order three or higher. 

The potential attraction of high-order polynomials is that, with their wriggly interior, they can take on a large number of appearances. This chameleon-like behavior has historically made them the tool of choice for understanding the behavior of approximations. That theory has motivated the use of polynomials for modeling patterns in data, but, paradoxically, has shown that high-order polynomials should not be the tool of choice for modeling data. The mathematical background needed for those better tools won't be available to us until Block 5, when we explore linear algebra. 

Let's start the story before the invention of the linear-algebra tools, when polynomials were well understood. 

Polynomial functions lend themselves well to calculations, since the output from a polynomial function can be calculated using just the basic arithmetic functions: addition, subtraction, multiplication, and division. To illustrate, consider this polynomial: $$g(x) \equiv x - \frac{1}{6} x^3$$
This is a third-order polynomial, since the highest-order term is $x^3$. The coefficients have been selected for the purpose of the illustration to be easy to handle by mental arithmetic. For instance, for $g(x=1)$ is $5/6$. Similarly, $g(x=1/2) = 23/48$ and $g(x=2) = 2/3$. A person of today's generation would use an electronic calculator for more complicated inputs, but the mathematicians of Newton's time were accomplished human calculators.  It would have been well within their capabilities to calculate, using paper and pencil^[Unfortunately for these human calculators, pencils weren't invented until 1795. Prior to the introduction of this advanced, graphite-based computing technology, mathematicians had to use quill and ink.], $g(\pi/4) = 0.7046527$. 

Our example polynomial, $g(x) \equiv x - \frac{1}{6}x^3$, graphed in color in Figure \@ref(fig:small-sine),  doesn't look like any of our pattern book functions. But, for a small interval around $x=0$, it has a great similarity to the sinusoid.

```{r small-sine, echo=FALSE, fig.cap="The polynomial $g(x) \\equiv x -x^3 / 6$ is remarkably similar to $\\sin(x)$ near $x=0$."}
slice_plot(sin(x) ~ x, domain(x=c(-2.85, 2.85)), size=3, alpha=0.25) %>%
  slice_plot(x - x^3/6 ~ x, color="magenta")
```

It's clear from the graph that the approximation is excellent near $x=0$ and gets worse as $x$ gets larger. The approximation is poor for $x \approx \pm 2$. We know enough about polynomials to say that the approximation will not get better for larger $x$; the sine function has a range of $-1$ to $1$, while the left and right tails of the polynomial are heading off to $\infty$ and $-\infty$ respectively.

One way to quantify the quality of the approximation is to look at the ***error***, that is the difference between the actual sinusoid and $g(x)$, that is ${\cal E}(x) \equiv |\strut\sin(x) - g(x)|$. The point of the absolute value used in defining the error is that we're interested in how ***far*** the approximation is from the actual function and not so much in whether the approximation is below or above the actual function. Figure \@ref(fig:sin-error) shows ${\cal E}(x)$ as a function of $x$.

```{r sin-error, echo=FALSE, fig.cap = "The error ${\\cal E}(x)$ of $x - x^3/6$ as an approximation to $\\sin(x)$. Left panel: linear scale. Right panel: log-log scale."}
P <- slice_plot(abs(sin(x) - (x - x^3/6)) ~ x, domain(x=c(0.001, 3)), npts=500) %>% 
  gf_labs(y = "Error(x)")
P
breaksx <- c(.001, .003, .005, 0.01,0.03,0.05, 0.1, 0.3, 0.5, 1, 3)
P %>%
  gf_refine(scale_y_log10(breaks=10^(-17:0)), scale_x_log10(breaks=breaksx, labels=as.character(breaksx)))

```

Figure \@ref(fig:sin-error) shows that for $x < 0.3$, the error in the polynomial approximation to $\sin(x)$ is in the 5th decimal place. For instance, $\sin(0.3) = 0.2955202$ while $g(0.3) = 0.2955000$. 

That the graph of ${\cal E}(x)$ is a straight-line on log-log scales diagnoses ${\cal E}(x)$ as a power law. That is: ${\cal E}(x) = A x^p$. As always for power-law functions, we can estimate the exponent $p$ from the slope of the graph. It's easy to see that the slope is positive, so $p$ must also be positive.

The inevitable consequence of ${\cal E}(x)$ being a power-law function with positive $p$ is that $\lim_{x\rightarrow 0} {\cal E}(x) = 0$. That is, the polynomial approximation $x - \frac{1}{6}x^3$ is *exact* as $x \rightarrow 0$.

Throughout this book, we've been using straight-line approximations to functions around an input $x_0$. The slope of the tangent-line approximation is the derivative of the function at $x_0$. We can always choose the coefficients of a polynomial approximation to be at least as good as the tangent-line approximation. For a function $f(x)$ that we want to approximate at $x=x_0$ with a polynomial, just start with the polynomial 
$$g(x) = f(x_0) + \partial_x f(x_0) [x-x_0]$$
Notice that $g(x=x_0) = f(x=x_0)$, and $\partial_x g(x=x_0) = \partial_x f(x=x_0)$: the approximation matches the function's value and its derivative at $x_0$. One way to construct a high-order polynomial approximation is to follow the same logic: structure $g(x)$ such that $g(x_0) = f(x_0)$ and $\partial_x g(x=x_0) = \partial_x f(x=x_0)$, $\partial_{xx} g(x=x_0) = \partial_{xx} f(x=x_0)$, and so on for the higher order derivatives.

$$\newcommand{\bigstrut}{{\rule[-.3\baselineskip]{0pt}{1.5\baselineskip}}$$

The table below compares the function value and the derivatives at input $x = x_0 = 0$ for $\sin(x)$ to the polynomial approximation $g(x)\equiv x - \frac{1}{6} x^3$.

Order | $\sin(x)$ derivative | $x - \frac{1}{6}x^3$ derivative ------|----------------------|--------------------------------   0   | $\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ | $\left( 1 - \frac{1}{6}x^3\right)\left.{\Large\strut}\right|_{x=0} = 0$ 
  1   | $\cos(x) \left.{\Large\strut}\right|_{x=0} = 1$ | 
$\left(1 - \frac{3}{6} x^2\right) \left.{\Large\strut}\right|_{x=0}= 1$
  2   | $-\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ | 
$\left(- \frac{6}{6} x\right) \left.{\Large\strut}\right|_{x=0} = -1$
  3   |  $-\cos(x) \left.{\Large\strut}\right|_{x=0} = -1$ |
$- 1\left.{\Large\strut}\right|_{x=0} = -1$ 
  4   |  $\sin(x) \left.{\Large\strut}\right|_{x=0} = 0$ |
$0\left.{\Large\strut}\right|_{x=0} = 0$ 

The first four derivatives of $x - \frac{1}{6} x^3$ exactly match, at $x=0$, the first four derivatives of $\sin(x)$.

The polynomial $h(x) \equiv a_0 + a_1 [x-x_0]^1 + a_2 [x-x_0]^2 + \cdots + a_n [x-x_0]^n$, with coefficients $a_0, a_1, a_2, ...$ selected to match the function value $f(x=x_0)$ as well as the first $n$ derivatives $\partial_x f(x=x_0)$, $\partial_{xx} f(x=x_0)$, $\partial_{xxx} f(x=x_0)$, ... is called the ***Taylor polynomial*** approximation to $f(x)$ around the input $x_0$.

::: {.example}
Find the 4th-order Taylor polynomial approximation to $f(x) = e^x$ around $x=0$.

We know it will be a 4th order polynomial:
$$\text{Taylor}(x) \equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4$$
The exponential function is particularly nice for examples because the function value and all it's derivatives are identical: $e^x$. So $$f(x= 0) \color{magenta}{=} \partial_x f(x=0) \color{magenta}= \partial_{xx} f(x=0) \color{magenta}= \partial_{xxx} f(x=0) \color{magenta}= \partial_{xxxx} f(x=0) \color{magenta}= 1$$
The function value and derivatives of $\text{Taylor}(x)$ at $x=0$ are:
$$
\text{Taylor}(x=0) = a_0\\
\,\\
\partial_{x}\text{Taylor}(x=0) = a_1\\
\,\\
\partial_{xx}\text{Taylor}(x=0) = 2 a_2\\
\,\\
\partial_{xxx}\text{Taylor}(x=0) = 2\cdot3\cdot a_3\\
\,\\
\partial_{xxxx}\text{Taylor}(x=0) = 2\cdot3\cdot4 \cdot a_4
$$
Matching these to the exponential evaluated at $x=0$, we get
$$a_0 = 1\\
\,\\
a_1 = 1\\
\,\\
a_2 = \frac{1}{2}\\
\,\\
a_3 = \frac{1}{2 \cdot 3}\\
\,\\
a_4 = \frac{1}{2 \cdot 3 \cdot 4}
$$
So the 4th-order Taylor polynomial approximation to the exponential at $x=0$ is $$g(x) = 1 + x + \frac{1}{2} x + \frac{1}{2\cdot 3} x^2 + \frac{1}{2\cdot 3} x^3 +\frac{1}{2\cdot 3\cdot 4} x^4$$

Figure \@ref(fig:taylor-exp-4) shows the exponential function $e^x$ and its 4th-order Taylor polynomial approximation near $x=0$:

```{r taylor-exp-4, echo=FALSE, class.source="code-hide", fig.cap="The 4th-order Taylor polynomial approximation to $e^x$ arount $x=0$"}
slice_plot(exp(x) ~ x, domain(x=c(-2,2)), size=3, alpha = 0.25) %>%
  slice_plot(1 + x + x^2/2 + x^3/6 + x^4 / 24 ~ x, color="magenta")
```

The polynomial is exact at $x=0$ but the ***error*** ${\cal E}(x)$ grows with increasing distance from $x=0$:

```{r taylor-exp-5, echo=FALSE, class.source="code-hide", fig.cap="The error from a 4th-order Taylor polynomial approximation to $e^x$ around $x=0$.", out.width="50%", fig.show="hold"}
Pts <- tibble(x = seq(-2, 2, length=100),
              y = exp(x),
              y_approx = 1 + x + x^2/2 + x^3/6 + x^4/24,
              y_error = abs(y - y_approx),
              x_abs = abs(x))
gf_path(y_error ~ x, data = Pts, color=~sign(x)) %>%
  gf_labs(y="Error(x)") %>%
  gf_refine(scale_color_continuous(guide = 'none'))
gf_path(log10(y_error) ~ log10(x_abs), data = Pts, color=~sign(x)) %>%
  gf_labs(y="Log10(Error(x))") %>%
  gf_refine(scale_color_continuous(guide = 'none'))
```

From the plot of $\log_{10} {\cal E}(x)$ versus $\log_{10} | x |$ in Figure \@ref(fig:taylor-exp-5)you can see that the error grows from zero at $x=0$ as a power-law function. Measuring the exponent of the power-law from the slope of the graph on log-log axes give ${\cal E}(|x|) = a |x|^5$. This is typical of Taylor polynomials: for a polynomial of degree $n$, the error will grow as a power-law with exponent $n+1$. This means that the higher is $n$, the faster $\lim_{x\rightarrow x_0}{\cal E}(x) \rightarrow 0$. On the other hand, since ${\cal E}_x$ is a power law function, as $x$ gets further from $x_0$ the error grows as as $\left(x-x_0\right)^{n+1}$
:::


`r insert_calcZ_exercise("26.05", "k8H2vs", "Exercises/Diff/bee-throw-mug.Rmd")`



`r insert_calcZ_exercise("XX.XX", "uebqnO", "Exercises/Diff/fir-shut-coat.Rmd")`


`r insert_calcZ_exercise("XX.XX", "682lsB", "Exercises/Diff/local-shift.Rmd")`


`r insert_calcZ_exercise("XX.XX", "ecdVKx", "Exercises/Diff/frog-throw-screen.Rmd")`




```{r pc1-3, echo=FALSE, results="markup"}
askMC(
  "Find $f(1)$ exactly for your 4th-order polynomial approximation to $e^x$. Which of these is it?",
  "2 8/12",
  "+2 17/24+", 
  "2 35/48", 
  "2 7/10" 
)
```

```{r pc1-4, echo=FALSE, results="markup"}
askMC(
  "Again using $f()$  for the 4th-order polynomial approximation to $e^x$, subtract $f(1)$ from $e^1$. The result will be near zero. To quantify how near, count the number of leading zeros after the decimal point. How many zeros are there?",
  1, "+2+", 3, 4,
  random_answer_order = FALSE
)
```

```{r pc1-5, echo=FALSE, results="markup"}
askMC(
  "Repeat the above calculation, but include the 5th- and 6th-order terms (that is, $a_5 x^5$ and $a_6 x^6$) when programming the polynomial computer. Subtract the new $f(1)$ from $e^1$.  How many leading zeros are there after the decimal point?",
  1, "2", "+3+", 4,
  random_answer_order = FALSE
)
```


`r insert_calcZ_exercise("XX.XX", "KmDiXI", "Exercises/Diff/girl-wake-bottle.Rmd")`

`r insert_calcZ_exercise("XX.XX", "IlNSF0", "Exercises/Diff/fawn-hear-kayak.Rmd")`

## l'Hopital's rule

One task for which the polynomial computer is extremely well suited is
the resolution of singularities. A singularity is an input for which the
function involves division by zero, for instance:

$$g(x) \equiv \frac{\sin(x)}{x}\ \ \ \mbox{at}\ \ \ x=0$$

Since division by zero is undefined, there's no way to do a numerical
computation at $x=0$. Even computer arithmetic is set up to recognize
this:

```{r echo=TRUE}
g <- makeFun(sin(x)/x ~ x)
g(0)
```

The output `NaN` stands for "not a number." It is as if the computer is
throwing up its hands and saying, "I don't know what to do with this."

Actually, the computer doesn't get so frustrated at all such
division-by-zero problems, for instance

```{r echo=TRUE}
h <- makeFun(1/x ~ x)
h(0)
```

The output `Inf` is also not a number, but here the computer is willing
to say that whatever 1/0 might be, it's very large: infinity.

Why does the computer make a distinction between the kind of
divide-by-zero in $\sin(x)/x$ and the kind in $1/x$. The answer is that
the first function involves a numerator, $\sin(x)$ that will also be
zero when $x=0$. It's the zero-over-zero that prompts the `NaN`
response:

```{r echo=TRUE}
0 / 0
```

The numerator must literally be zero. Being very close to zero doesn't
cut it.

```{r echo=TRUE}
0.000000000000000000000000000000000000000000000001 / 0
```

The polynomial computer provides another approach to sorting out what
$\sin(x)/x$ and similar functions might be at $x=0$. And the key thing
is the word "approach". The sandbox carries out the $\sin(x)/x$
calculation for $x$ very small but not zero. See what you get.

```{r zoz1-1, exercise=TRUE, exercise.cap="Divide by tiny", exercise.nlines=5, eval=FALSE}
x <- 0.00000001
sin(x) / x
```

Although $\sin(x) / x$ is not defined at $x=0$, it is defined
*everywhere* else. Recall that the idea of a **limit** is to find a
value to stand in for the undefined $\sin(0)/0$ by making $x$ very small
and seeing what you get. If you get something sensible for very small
$x$, and get the same thing for *even smaller* $x$, then we have a
reasonable claim for what value to insert for $\sin(x)/x$.

```{r zoz1-2, echo=FALSE, results="markup"}
askMC(
  "Using the sandbox above, add more zeros to $x$ to make it even smaller. You can stop when you get tired. Does $\\sin(x)/x$ evaluate to something sensible for such tiny $x$? If so, what value?",
  "0", 
  "1/2",
  "+1+",
  "answer varies with $x$ as $x$ gets smaller",
  random_answer_order=FALSE
)
```

Saying, "so small that I got tired typing the zeros" is not a convincing
definition of "small" to a mathematician. For example,
0.0000000000000001 parsec (a unit of length) seems small but it is
equivalent to about 10 feet---no so small. Mathematicians want you to
take "small" to the limit, an arbitrarily large number of zeros, and
when you're done with that, even more zeros.

Fortunately, R and other computer languages have a scientific notation
that allows you just to name the number of zeros you want after the
decimal point. For instance `1e-2` is $0.01$---one zero. Similarly
`1e-20` is $0.00000000000000000001$, nineteen zeros.

```{r zoz1-3, echo=FALSE, results="markup"}
askMC(
  "Use the previous sandbox, but this time use scientific notation so that you can look at $x$ as small as 1e-31 (30 zeros) or even smaller. Starting at `x = 1e-31`, calculate `sin(x)/x`. Then double the number of zeros, keep on doubling the number of zeros. The result will continue to be 1 ... until it eventually becomes `NaN`. How many zeros are there in the `x` that produces `NaN` as the answer to `sin(x)/x`?",
  127,
  191,
  "+323+", 
  379, 
  1281,
  random_answer_order = FALSE
)
```

What's happening here has more to do with the nature of computers than
the nature of numbers. Computers (in the manner they are ordinarily
programmed) use packets of bits to represent numbers, and the chips have
been engineered to make those bit packets respond to arithmetic
operations as if they were the numbers they represent. A typical
computer number, like 0.001, uses 64 bits in a special, standard format.
Since there is a finite number of bits, there is a largest possible
non-`Inf` number and a smallest possible non-zero number. According to
the IEEE standard for "floating-point" arithmetic the largest non-`Inf`
number is around `1e300` and the smallest non-zero number is around
`1e-320`. This failure to behave like genuine mathematical numbers is
called "overflow" (for large numbers which turn into `Inf`) and
"underflow" (for small numbers which turn into `0`).

```{r zoz1-4, echo=FALSE, results="markup"}
askMC(
  "Play around with numbers in the format `1e300`, `1e301` and so on until you find the smallest `1e???` that prints as `Inf`. Similarly, try numbers in the format `1e-320` and `1e-321` until you find the largest one that prints out as exactly zero. What are those two numbers?",
  "`1e305` and `1e-322`",
  "`1e306` and `1e-323`",
  "+`1e308` and `1e-324`+",
  "`1e309` and `1e-327`",
  random_answer_order = FALSE
)
```

The polynomial computer doesn't have any problem with overflow or
underflow. The key to success is to write the Taylor polynomial for
functions such as $\sin(x)$ or $x$ or $x^2$ near $x_0 = 0$. Such
polynomials will always look like:

$$f(x) = a_1 x^1 + a_2 x^2 + a_3 x^3 + \cdots$$

What's special here is that the `a_0` term does not need to be included
in the polynomial, since $f(0) = 0$.

```{r zoz1-5, echo=FALSE, results="markup"}
askMC(
  "One of these functions has a Taylor polynomial at $x_0 = 0$ the *does need* a non-zero $a_0$ term. The other's don't. Which function needs the non-zero $a_0$ term?",
  "`sin()`",
  "`tan()`",
  "`atan()`",
  "+`acos()`+"
)
```

These zero divided by zero problems (like $\sin(x) / x$) always involve
a ratio of two functions ($\sin(x)$ and $x$ here) that don't need the
$a_0$ term in their Taylor series around $x_0 = 0$. That makes them just
a little bit simpler.

What's more important than simpler is that, for the expansion of such
functions to study the limit at $x \rightarrow 0$, we only need the
\*\*first terms with a non-zero coefficient\* $a_k$ to represent the
function with complete accuracy.

Why? Consider the 2nd-order Taylor polynomial $a_1 x + a_2 x^2$. If we
are to be able to safely disregard the $a_2$ term it is because that
term, for small $x$ is much, much smaller than the $a_1 x$ term. And we
can always choose non-zero $x$ to make this so.

For instance, suppose our polynomial were $x + 100 x^2$. For $x=0.1$,
the first and second terms are the same size; we need them both for
accuracy. For $x=0.01$, the second term is 1/100 the size of the first
term, maybe we don't need the second term so much. You can always make
$x$ so small that anyone will be satisfied that the second term is
utterly negligible compared to the first.

Here's the method:

Suppose you have a function $f(x) \equiv u(x)/v(x)$ where
$$\lim_{x\rightarrow 0} u(x) = 0\ \ \ \mbox{and}\ \ \ \lim_{x\rightarrow 0} v(x) = 0$$
Given this, $f(0)$ is not defined. But we can ask whether there is a
sensible value that can be plugged in in place of $f(0)$ that will cause
the modified $f()$ to be continuous at $x=0$.

Step 1: Write the Taylor polynomial expansion around $x-0 = 0$ for both
$u(x)$ and $v(x)$. If both expansions have a non-zero first coefficient,
you can stop there. Now we have:

$$u(x) \approx a_1 x\\
v(x) \approx b_1 x$$ where $a_1 = \partial_x u(0)$ and
$b_1 = \partial_x v(0)$.

Step 2: Divide the polynomial (really just linear!) expansion of $u()$
by the expansion of $v()$ to get

$$\lim_{x\rightarrow 0}\frac{u(x)}{v(x)} = \lim_{x\rightarrow 0} \frac{a_1 x}{b_1 x} = \frac{a_1}{b_1}$$

That's the answer, $a_1/b_1$, at least when $b_1 \neq 0$. We'll come
back to that case later.

```{r zoz1-6, echo=FALSE, results="markup"}
askMC(
  "For $\\lim_{x\\rightarrow 0} \\sin(x) / x$, what are $a_1$ and $b_1$?",
  "+$a_1 = 1$ and $b_1 = 1$+",
  "$a_1 = \\pi$ and $b_1 = \\pi$",
  "$a_1 = -1$ and $b_1 = -1$",
  "$a_1 = 0$ and $b_1 = -1$"
)
```

Sometimes the singularity is at some non-zero $x$. For instance,
$$h(x) \equiv \frac{x^2 - 16}{x - 4}$$ The divide-by-zero comes into
play when $x=4$. So is there a sensible value to plug in for $h(4)$ to
replace the singularity.

Here, write your Taylor polynomials around $x_0 = 4$, the location of
the singularity. We'll get:

$$x^2 - 16 = a_1 (x-4) + a_2 (x-4)^2 + \cdots\\ 
x - 4 = b_1 (x-4)$$ Using Taylor's formula for coefficients we'll get
$$a_1 = \partial_x (x^2 - 16)\left.\right|_{x=4} = 2x\left.\right|_{x=4} = 8\\
b_1 = \partial_x (x - 4) = 1
$$

Consequently, $\lim_{x\rightarrow 4} \frac{x^2 - 16}{x - 4} = 8$

We've been discussing ratios of functions where the ratio cannot be
calculated at the singularity using simply the limits of the functions
approaching that singularity. (For instance
$\lim_{x\rightarrow 0} \sin(x) = 0$ and $\lim_{x\rightarrow 0} x = 0$,
but knowing this does not tell us what
$\lim_{x\rightarrow 0} \frac{\sin(x)}{x}$ will be. These are called
"indeterminate forms." As you've seen, if we know more about the
functions than merely their individual limits, we can sometimes resolve
the indeterminacy. Here we're doing that by writing each function as a
low-order polynomial.

The indeterminate form $\lim_{x\rightarrow 0} \frac{\sin(x)}{x}$ might
be said to have the "shape" 0/0. But 0/0 is just a notation about the
limits of the two individual functions. There are indeterminate forms
with other shapes:
$$\frac{0}{0}\ \ \ \ \ \ \frac{\infty}{\infty}\ \ \ \ \ \ 0\, {\infty\ \ \ \  \ 0^0\ \ \ \ \ \ \infty^0\ \ \ \ \ \ 1^\infty\ \ \ \ \ \ \infty - \infty}$$
Keep in mind that something like $0 \infty$ is not a multiplication
problem but rather shorthand for $u(x) v(x)$ where
$\lim_{x\rightarrow x_0} u(x) = 0$ and
$\lim_{x\rightarrow x_0} v(x) \rightarrow \infty$.

There is a variety of algebraic tricks to try to transform these
different shapes of indeterminate forms into a ratio of functions, each
of which goes to zero at the relevant $x_0$. Once that's done, you can
apply the method described above.

Indeterminate forms have been a bonanza for the author of calculus book
exercises, who can write an large number of examples, many of which have
never been seen in the wild.

One shortcut that that works in practice is to make a graph of the
indeterminate form near the singularity. If the limit as $x$ approaches
the singularity is a finite number, you can read off the result from the
graph.

In the sandbox below, the function $g(x)\equiv x \ln(x)$ is set up. This
function has a singularity at $x=0$. Examine the plot and determine
where the function value is going as you walk along the graph to the
singularity.

```{r zoz1-7, exercise=TRUE, exercise.cap="Just plot the function", exercise.nlines=5, eval=FALSE}
g <- makeFun(x * log(x) ~ x)
slice_plot(g(x) ~ x, domain(x=c(0, 0.5)))
```

You may have to zoom in on the domain to get a clear read of the
function value at $x=0$ singularity.

```{r zoz1-8, echo=FALSE, results="markup"}
askMC(
  "From the graph, determine $\\lim_{x\\rightarrow 0} x \\ln(x()$. Choose the correct answer.",
  -0.2, "+0+", 0.1, 0.5,
  random_answer_order = FALSE
)
```

Conventionally, the relationship
$$\lim_{x\rightarrow x_0} \frac{u(x)}{v(x)} = \lim_{x\rightarrow x_0} \frac{\partial_x u(x)}{\partial_x v(x)}$$
is called "L'Hopital's Rule" after the author of the very first Calculus
textbook where the rule was first published. Here's the title page from
the second edition of 1716.

```{r echo=FALSE, out.width="50%", fig.align="center"}
knitr::include_graphics("www/lhopital-cover.png")
```

::: {.intheworld}
[Brooke Taylor](https://en.wikipedia.org/wiki/Brook_Taylor) (1685-1731), a near contemporary of Newton, published his work on approximating polynomials in 1715. Wikipedia reports: "[T]he importance of [this] remained unrecognized until 1772, when Joseph-Louis Lagrange realized its usefulness and termed it 'the main [theoretical] foundation of differential calculus'."

```{r brook-taylor, echo=FALSE, fig.cap="Brook Taylor [Source](https://en.wikipedia.org/wiki/Brook_Taylor#/media/File:Taylor_Brook_Goupy_NPG.jpg)", out.width = "40%", fig.align="center"}
knitr::include_graphics(normalizePath("www/Brook_Taylor.jpg"))
```

Taylor's work preceded by about a century the development of techniques for working with data. One of the pioneers in these new techniques was Carl Friedrich Gauss (1777-1855), after whom the gaussian function is named. Gauss's techniques are the foundation of an incredibly important statistical method that is ubiquitous today: ***least squares***. Least squares provides an entirely different way to find the coefficients on approximating polynomials (and an infinite variety of other function forms). The R/mosaic `fitModel()` function for polishing parameter estimates is based on least squares. In Block 5, we'll explore least squares and the mathematics underlying the calculations of least-squares estimates of parameters.

Due to the importance of Taylor polynomials in the development of calculus, many students assume their use extends to constructing models from data. They also assume that third- and higher-order monomials are a good basis for modeling data. Both these assumptions are wrong. Least squares is the proper foundation for working with data.

:::

<!--
## Fitting polynomials

Taylor polynomials provide a means to approximate continuous and smooth functions around a center $x_0$. So long as $x$ is very close to $x_0$, the approximation will be excellent. But Taylor polynomials aren't a solution to every problem. Consider the piecewise continuous function, $ramp(x-1)$, in Figure \@ref(fig:ramp-Taylor).

```{r ramp-Taylor, echo=FALSE, fig.cap="A piecewise continuous ramp function (gray) together with its Taylor polynomial (magenta) centered on $x_0 = 0$."}
ramp <- makeFun(ifelse(x-1 < 0, 0, x-1) ~ x)
slice_plot(ramp(x) ~ x, size = 3, alpha = 0.25, domain(x=c(-2, 4))) %>%
  slice_plot(0 ~ x, color="magenta") %>%
  gf_text(0.25 ~ 3, label="Taylor poly.",color = "magenta")

```
The value of $ramp(x-1) \left.\Large\right|_{x=0}$ is zero, as is the value of the first, second, third, and every other derivative. Whatever we choose for the order $n$ of the Taylor polynomial, it will be $\text{Taylor(x) = 0}$. That's an excellent approximation to $ramp(x-1)$ around $x=0$! But it misses the point of $ramp()$ entirely.

Or consider the problem that introduced this chapter: finding an arithmetic process to evaluate $\sin(x)$. As it happens, we only need to be able to evaluate $\sin(x)$ on the interval $0 \leq x \leq \pi/2$.^[If $x$ is outside this range, add or subtract a multiple $k \in [\ldots, -2, -1, 0, 1, 2, \ldots]$ of $\pi$ so  $0 \leq x - k \pi \leq \pi$. Then, if $\pi/2 \leq (x - k \pi)$, calculate $\sin(\pi - (x - k \pi))$ 

FIT TO PART OF SINE

```{r}
Pts <- tibble(x = seq(0, pi/2, length=1000), y = sin(x))
mod <- lm(y ~ x + I(x*x) + I(x*x*x) - 1, data = Pts)
mod
fmod <- makeFun(mod)
slice_plot(sin(x) ~ x, domain(x=c(0, pi/2)), size=3, alpha=0.25) %>%
  slice_plot(fmod(x) ~ x, color="magenta")
slice_plot(sin(x) - fmod(x) ~ x, domain(x=c(0, pi/2))) %>%
  slice_plot(sin(x) - (x - x^3/6) ~ x, color="green")
```

-->
