# Change and accumulation {#change-accumulation}

Every 10 years, starting in 1790, the US Census Bureau carries out a constitutionally mandated census: a count of the current population. The overall count as a function of year is shown in Figure \@ref(fig:pop-graph). [[Source](https://en.wikipedia.org/wiki/Demographic_history_of_the_United_States)
]



```{r echo=FALSE, message=FALSE, warning=FALSE}
USPop <- readr::read_csv("www/census-totals.csv")
Tmp <- USPop %>% 
  mutate(population=prettyNum(population, big.mark=",")) %>%
  select(-growth_rate)
Table <- if (knitr::is_html_output()) {
  DT::datatable(Tmp) 
} else {
  knitr::kable(Tmp)
}
USPop <- USPop %>% 
  mutate(population = population/1e6)
```

In the 230 years spanned by the census data, the US population has grown 100-fold, from about 4 million in 1790 to about 330,000,000 in 2020.

```{r pop-graph, echo=FALSE, fig.cap="US population since 1790.", fig.margin=TRUE}
popfun <- spliner(population ~ year, data = USPop)
USPop <- USPop %>%
  mutate(yearly_growth_rate = 100*((1+growth_rate/100)^(1/10) - 1))
mod <- lm(population ~ poly(year, 2), data = USPop)
modf <- makeFun(mod)
gf_point(population ~ year, data = USPop) %>%
  gf_line(population ~ year) %>%
  gf_labs(y="Population (millions)") %>%
  gf_lims(x = c(1790, 2100), y=c(0,500))
```




It's always tempting to look for simple patterns in such data. Perhaps the US population has been growing exponentially. A semi-log plot of the same data suggests that the growth is only very roughly exponential. (See Figure \@ref(fig:pop-graph-log).) A truly exponential process would present as a curve with a constant derivative, but the derivative of the function in the graphed is decreasing over the centuries. 


```{r pop-graph-log, echo=FALSE, fig.cap="Population since 1790.", fig.margin=TRUE}
P <- gf_point(population ~ year, data = USPop) %>%
  gf_line(population ~ year) %>%
  gf_labs(y="Population (millions)") %>%
  gf_refine(scale_y_log10())
```

Insofar as the slope over the semi-log graph is informative, it amounts to this quantity:
$$\partial_t \ln(\text{pop}(t)) = \frac{\partial_t\, \text{pop}(t)}{\text{pop}}$$
This is the *per-capita* rate of growth, that is, the rate of change in the population divided by the population. Conventionally, this fraction is presented as a percentage: percentage growth in the population per year, as in Figure \@ref(fig:pop-growth).

```{r pop-growth, echo=FALSE, fig.cap="Annual growth rate of the US population (percent)"}
growth_mod <- lm(yearly_growth_rate ~ year, data = USPop)
growth_fun <- makeFun(growth_mod)
gf_point(yearly_growth_rate ~ year, data = USPop) %>%
  gf_lims(x = c(1790, 2100), y=c(-.5,4)) %>%
  gf_lm() %>%
  gf_labs(y = "Growth rate (% per year)") 
```

The dots in the graph are a direct calculation from the census data.  There's a lot of fluctuation, but an overall trend stands out: the population growth rate has been declining since the mid-to late 1800s. The deviations from the trend are telling and correspond to historical events. There's a relatively low growth rate seen from 1860 to 1870: that's the effect of the US Civil War. The Great depression is seen in the very low growth from 1930 to 1940. Baby Boom: look at the growth from 1950-1960. The bump from 1990 to 2000? Not coincidentally, the 1990 Immigration Act substantially increased the yearly rate of immigration. 
<<<<<<< HEAD

If the trend in the growth rate continues, the US will reach zero net growth about 2070, then continue with negative growth. Of course, negative growth is just decline. A simple prediction from Figure \@ref(fig:pop-growth) is that the argmax of the US population will be around 2070.

How large will the population be when it reaches its maximum? 

In Block 2, we dealt with situations where we know the function $f(t)$ and want to find the rate of change $\partial_t f(t)$. Here, we know the rate of change of the population and we need to figure out the population itself, in other words to figure out from a known $\partial_t f(t)$ what is the (as yet) unknown function $f(t)$.  

The process of figuring out $f(t) \longrightarrow \partial_t f(t)$ is, of course, called ***differentiation***. The opposite process, $\partial_t f(t) \longrightarrow f(t)$ is called ***anti-differentiation***. 


If the trend in the growth rate continues, the US will reach zero net growth about 2070, then continue with negative growth. Of course, negative growth is just decline. A simple prediction from Figure \@ref(fig:pop-growth) is that the argmax of the US population will be around 2070.

How large will the population be when it reaches its maximum? 

In Block 2, we dealt with situations where we know the function $f(t)$ and want to find the rate of change $\partial_t f(t)$. Here, we know the rate of change of the population and we need to figure out the population itself, in other words to figure out from a known $\partial_t f(t)$ what is the (as yet) unknown function $f(t)$.  

The process of figuring out $f(t) \longrightarrow \partial_t f(t)$ is, of course, called ***differentiation***. The opposite process, $\partial_t f(t) \longrightarrow f(t)$ is called ***anti-differentiation***. 

In this block we'll explore the methods for calculating anti-derivatives and some of the settings in which anti-derivative problems arrive. 


::: {.intheworld data-latex=""}
The predictions from the accumulate-population-growth model are shown as a $\color{magenta}{\text{magenta}}$ line in Figure \@ref(fig:pop-prediction-bad).

```{r pop-prediction-bad, echo=FALSE, warning=FALSE, fig.cap="Predicted US population based on the historical linear decline in per-capita growth."}
growth_mod <- lm(yearly_growth_rate ~ year, data = USPop)
growth_fun <- makeFun(growth_mod)
accum <- tibble(year = 2021:2130,
                pop = 331 * cumprod(1 + growth_fun(2021:2130)/100))

gf_point(population ~ year, data = USPop) %>%
  gf_labs(y="Population (millions)") %>%
  gf_lims(x = c(1790, 2130), y=c(0,450)) %>%
  gf_line(pop ~ year, data = accum, color="magenta") %>%
  gf_point(404.4 ~ 2060, color="dodgerblue", size=1)
```

According to the accumulation model, the population peaks in 2075 at 390 million. We'll be back down to the present population level in about 100 years. 

Professional demographers make much more sophisticated models using detailed data from many sources. The demographers at the US Census Bureau predict that the population will reach a maximum of 404 million in 2060, shown by the little blue dot in Figure \@ref(fig:pop-prediction-bad).  That's not too different from what we got by analyzing just the raw census numbers. `r mark(3135)`
:::

## Differentiation and anti-differentiation

It will help to keep in mind a fundamental principle of calculus, even if the mental image of this principle remains blurry: 

> ***Accumulation*** is the reverse operation to finding ***rate of change***, and *vice versa*.

To make this principle even easier to remember, let's restate it using the mathematical word used to say precisely what we mean by "accumulation":

> ***Anti-differentiation*** is the reverse operation to ***differentation***, and *vice versa*.

The mathematical notation for differentiation is simple: $\partial_t f(t)$. For anti-differentiation the notation is typographically very different. The anti-derivative of a function $f(t)$ is written: `r mark(3150)`

$$\large \color{blue}{\int} f(\color{blue}{t}) \color{blue}{dt}$$
The math notation consists of several components, each of which has something to say. The components colored blue are part of the general notation. You can change the name of the input variable from $\color{blue}{t}$ to whatever you like, but you'll have to change the $\color{blue}{dt}$ accordingly. 

Our understanding of differentiation gives us a nice head start in understanding differentiation. Recall the derivatives of the pattern-book functions.

$f(x)$   | $\partial_x f(x)$
---------|-------------------
$e^{x}$  | $e^x$
$\ln{x}$ | $1/x$
$\sin{x}$ | $\cos{x}$
$\pnorm(x)$ | $\dnorm(x)$
$x^2$    | $2x$
$\vdots$ | $\vdots$

Transforming this into a table of anti-derivatives is merely a matter of re-labeling the columns:

$\int g(x)dx$   | $g(x)$
---------|-------------------
$e^{x}$  | $e^x$
$\ln{x}$ | $1/x$
$\sin{x}$ | $\cos{x}$
$\pnorm(x)$ | $\dnorm(x)$
$x^2$    | $2x$
$x^p$    | $p x^{p-1}$
$\vdots$ | $\vdots$

But unlike differentiation, anti-differentiation has no easy equivalents of the product rule or the chain rule (for compositions of functions). 




## Euler

Block 2 introduced the derivative of a continuous function by looking at discrete differences. Given a function $g(t)$, we quantified the rate of change using the differencing operator. We called this the
$$\diff{t} g(t) \equiv \frac{g(t+h) - g(t)}{h}$$
As written above, $\diff{t}$ is properly called the ***finite-difference operator***, since no suggestion is made that $h$ is anything but a small number. We moved from $\diff{t}$ to $\partial_t$ by considering the ***limit*** as $h\rightarrow 0$ This move was fraught because of the concern about dividing by zero, but in the end we found simple algebraic expressions for the derivatives of the pattern-book functions as well as a few rules for handling the basic ways of combining functions using linear combinations, products, and function combinations. These rules were the sum rule, product rule, and chain rule respectively. `r mark(3140)`

In studying accumulation, we'll follow much the same path. The major difference is that our starting point is knowing a function like $\partial_t f(t)$: a derivative. From there will will construct a $f(t)$ from which $\partial_t f(t)$ *could have been derived, had we known it in the first place*. The idea is that sometimes information comes to us in the form of a rate of change and we need to figure out a function that could have generated that rate of change.  `r mark(3145)`

 `r mark(3155)`

The R/mosaic notation for the anti-derivative has exactly the same format as the derivative: 

    antiD(f(t) ~ t)


## Visualizing anti-differentiation {#anti-diff-viz}

Section \@ref(slope-fun-visualization) introduced a non-standard visualization of the slope function. We can build on that to show how the function $f(x)$ can be reconstructed from $\partial_x f(x)$. 

Figure \@ref(fig:euler-viz-1) shows a slope function visualization of some function $f(x)$

```{r euler-viz-1, echo=FALSE, fig.cap="A slope function $\\partial_x f(x)$ from which we are going to reconstruct the parent function $f(x)$. Each of the sloped segments has been given a label for later reference."}
Segs <- create_segments(sin(x) ~ x, domain(x=c(-pi,pi)), nsegs=30)
Segs$num <- c(letters, LETTERS)[1:nrow(Segs)]
Segs_orig <- Segs
gf_segment(y + yend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_text(0.25 ~ start, label=~num, size=3) %>%
    gf_labs(title="sin(x) slope-function visualization") %>%
    gf_lims(y=c(-1,1))

```
We'll reconstruct $f(x)$ one segment at a time. Recall that each of the segments shows a linear approximation to $f(x)$ at the input marked by the green dot. But in constructing the slope function, we threw away the information about the vertical placement of the segment. Now we have to recover that discarded information, as well as we can. `r mark(3160)`

The big clue for the reconstruction is that the function $f(x)$ was ***continuous***. But the piecewise function graphed in Figure \@ref(fig:euler-viz-1) is discontinuous; the endpoints of adjacent segments don't meet each other. `r mark(3165)`

That's easy to fix: we'll just move segment (b) vertically so that it becomes continuous with segment (a). 

```{r euler-viz-2, echo=FALSE, fig.cap="Moving segment (b) to become continuous with segment (a)."}
Segs$y[2] <- Segs$y[2] -0.199
Segs$yend[2] <- Segs$yend[2] - 0.199 
gf_segment(y + yend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_text(0.25 ~ start, label=~num, size=3) %>%
    gf_labs(title="sin(x) slope-function visualization. Segment (b) moved.") %>%
    gf_lims(y=c(-1,1))
```

Now that (a) and (b) are joined, we can join (c) to that:

```{r euler-viz-3, echo=FALSE, fig.cap="Moving segment (c) to become continuous with segments (a) and (b)."}
Segs$y[3] <- Segs$y[3] -0.4
Segs$yend[3] <- Segs$yend[3] - 0.4
gf_segment(y + yend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_text(0.25 ~ start, label=~num, size=3) %>%
    gf_labs(title="sin(x) slope-function visualization") %>%
    gf_lims(y=c(-1,1))
```

Continue this process one segment at a time to reconstruct $f(x)$.

```{r euler-viz-4, echo=FALSE, fig.cap="After joining all the segments together, the picture of $f()$ is complete."}
gf_segment(yf + yfend ~ x + xend, data = Segs_orig, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_text(0.25 ~ start, label=~num, size=3) %>%
    gf_lims(y=c(-1,1))
```
This process reconstructs the ***shape*** of $f(x)$ from $\partial_x f(x)$. But there is still something missing. We never touched segment (a). Its vertical location was arbitrary. So we have to qualify our claim to have reconstructed $f(x)$. What we've reconstructed is some function $\widehat{f}(x)$ whose derivative is $\partial_x f(x)$. There are other such functions; any function $\widehat{f}(x) + C$ can make a legitimate claim to being the anti-derivative of $\partial_t f(x)$.  We'll return to $C$ in later chapters, but for now we'll just name it: the ***constant of integration***. `r mark(3170)`

::: {.workedexample}
In the population example that started this chapter, we constructed a model for $\partial_t f(t)$, where $f(t) = \ln(P(t))$. We can plot $\partial_t f(t)$ using an ordinary graph as we did in Figure \@ref(fig:pop-growth), but let's use the slope-function representation instead. `r mark(3175)`

```{r pop-slope-fun, echo=FALSE, fig.cap="The slope-function visualization corresponding to the plot of $\\partial_t f(t)$ in Figure @@ref(fig:pop-growth)."}
G <- antiD(growth_fun(t) ~ t, lower.bound = 2020)
Segs <- create_segments(G(t) ~ t, domain(t=c(2020-1.5, 2100)), nsegs=30)
gf_segment(y + yend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_labs(x="Year", y="Percent growth per year")
```

To construct the original function $f(t)$, just connect the segments in Figure \@ref(fig:pop-slope-fun). 

```{r accum-pop-growth, echo=FALSE, fig.cap="Accumulating the annual growth in Figure @@ref(fig:pop-slope-fun)."}
gf_segment(yf + yfend ~ x + xend, data = Segs, 
           color=~slope,size=1.3, lineend="round") %>%
  gf_point(0 ~ start, size=1, color="green") %>%
  gf_labs(x="Year", y="Growth accumulated versus year")
```

The accumulated annual growth graph shows that by around 2075, the accumulated growth will be about 17% above the population of 2020. 

:::

::: {.forinstructor}
You may wonder why I'm introducing anti-differentiation with the "slope-function visualization" rather than the "area under a curve." One reason is that the slope-function visualization is closer to the logic of Euler's method, so in addition to showing how differentiation can be undone, we get a free introduction to Euler.  The other reasons are more fundamental to pedagogy and the challenges students have relating differentiation and anti-differentiation when presented as slope-and-area. I'll start with some history ... `r mark(3180)`

Pyrrhus was a Greek king who invaded Italy in 280 B.C. and fought the Romans. His first battle, at Heraclea, was a famous victory, as was his second battle a year later at Asculum. History records Pyrrhus saying, after his second victory, “If we are victorious in one more battle with the Romans, we shall be utterly ruined.” The victories were so costly that the army could not be sustained. `r mark(3185)`

Such ***pyrrhic victories*** occur throughout history: the British at Bunker Hill (1775),  Napolean at Borodino (1812), and Lee at Chancellorsville (1863).

A pyrrhic victory in mathematics pedagogy: the visualization of integration as the "area under a curve." This account of integration is utterly dominant among graduates of calculus courses. It is a brilliant and successful way to give the abstract operation of anti-differentiation an easily remembered visage. But it creates costs that are simply not worth bearing.  `r mark(3190)`

i. There are not so many genuine applications for finding areas under curves. Students, seeing them as emblematic of calculus, are often dis-motivated by the lack of connection between the rhetoric of the importance of calculus and the seeming unimportance of the primary application.
ii. It's extremely difficult to make a connection between the many genuine applications of anti-differentiation and the mental image of area.
iii. Because area is most easily shown as a fixed, delimited region, students are introduced to ***integration*** before they see ***anti-differentiation***. It can be hard for students to make the transition between "slope at a point" or "tangent line" and a slope ***function***. This cost has to be paid all over again when moving from integration to anti-differentiation.
iv. The Yin and Yang of calculus are differentiation and anti-differentation. Students are successfully taught that if $f(x)$ is a function, the derivative is the slope of that function and the integral is the area under the function. This creates an unhelpful illusion that derivatives and anti-derivatives are related through the function, that there is an intermediary between them. Translating this image to the metaphor of family generations, the picture looks like $f(x)$ is the **parent** of $\partial_x f(x)$ and $\int f(x)dx$ is the **parent** of $f(x)$. In other words, the incorrect image is encouraged that an anti-derivative is the **grandparent** of the derivative.  In fact the anti-derivative and the derivative have a parent-child relationship.
vi. At best, for many people, the relationship between the slope function and the area function is hard to see and sometimes mysterious. (Of course it's hard to see: the slope function is the ***second derivative*** of the area function.) This  `r mark(3195)`

For these reasons, I encourage instructors to avoid defining the calculus operation as "area under the curve." Make use of areas when they are part of a genuine application of calculus. Save the Riemann Sum for courses in analysis. For demonstrating anti-differentiation, the starting point should be a function which we know to be the derivative of the sought-after function. Teach Euler as connecting together short segments of slopes. Emphasis the connections between differentiation and anti-differentiation from the start: both are relationships between one function and another function, just as every person is both a mother and a child. The anti-derivative is the mother of the child, the derivative is the child of the mother. `r mark(3200)`
:::



## Symbolic anti-derivatives

The Euler method involves a ***finite*** $h$, which is just to say that $h$ must be ***non-zero***. Otherwise, $f(t_0 + h)$ would be exactly the same as $f(t_0)$. For some functions, however, it's possible to construct the anti-derivative without needing to deal with $h$ at all.  `r mark(3205)`

Recall that anti-differentiation undoes differentiation, and vice versa. In the previous Block, we found the symbolic derivatives of the basic modeling functions and general methods for differentiating functions constructed by linear combination, products, and function composition. Using the techniques from Block 2, tables can be constructed of functions and their derivatives, looking like this: `r mark(3210)`



Recall that every smooth, continuous function has a derivative defined everywhere in the function's domain. Similarly, every function has an anti-derivative, and even discontinuous, un-smooth functions have nicely behaved anti-derivatives. In this sense anti-differentiation is easy. It's only the algebra of anti-differentiation that can be hard or often literally impossible. `r mark(3215)`



::: {.objectives}
```{r echo=FALSE, results="markup"}
state_objective("Int-2b", "Utilize your knowledge of derivatives rules to backwards solve for anti-derivatives.")
state_objective("Int-2f", "Understand that the output of anti-differentiation is a function known as a general solution and more information is necessary to find the particular solution [DTK: This is language I associate with differential equations. How about: Find the constant of integration.]")
state_objective("Int-2c", "Understand the notation of an indefinite integral to include a. the integral symbol; b. variable of integration (\"with respect to\" variable); c. the constant of integration")
state_objective("Int-2d", "Understand the relationship between a (child) function  and its (parent) integral.")
```
:::


::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Int-5c", "Know common scientific application relationships between base functions and anti-derivatives: a. Acceleration, Velocity, Position; b. Force, Work; c. Area, Volume; d. Cash flow, assets")
```
:::




    
::: {.todo}

Calculation of luminance using light intensity at different wavelengths integrated over the luminance function. https://en.wikipedia.org/wiki/Luminous_efficiency_function
:::

::: {.todo}
Pick up on the Lorenz curve `{.intheworld  data-latex=""}` in Blocks 1 and 2. Integrate to find the Gini coefficient. Show that the Gini coefficient is the same for very different types of inequality and that therefore it's not such a good measure. How about the integral over the poorest 25% or 50% of society." `r mark(3220)`
:::

-----


## Numerical anti-derivatives

We will devote about a third of this block on accumulation to algebraic techniques for calculating anti-derivatives. You will see these techniques in use in some of your future classes and work in science and engineering.  `r mark(3225)`

It's the nature of things that some people master the algebraic techniques and many do not. But it's easy to make mistakes. Even more fundamentally, there are many accumulation problems where the functions to be integrated do not have an algebraic form for the anti-derivative. In such cases, professionals use numerical techniques such as the Euler method. `r mark(3230)`

In order to give you a simple way to construct the anti-derivative of (just about) any function, while minimizing the amount of computer programming, we have packaged up anti-differentiation techniques into one, easy to use R function. This is `antiD()`. `r mark(3235)`

The `antiD()` function has the same interface as `D()` or `makeFun()`: the argument is a tilde expression of the sort `sqrt(x*sin(3*x)) ~ x`. The result returned from `antiD()` is a new R function that takes as its argument the "with respect to" variable. The sandbox provides a space to play with `antiD()` so that you feel comfortable using it. `r mark(3240)`

```{r eval=FALSE}
antiD(x^-2 ~ x)

f <- makeFun(sqrt(x*sin(3*x)) ~ x)
antiD(f(x) ~ x)
```

As you can see from the output of the sandbox, `antiD()` returns an R `function()`. The variable on the right of the tilde expression in the argument becomes the first of the arguments to that function. There is also a `C` argument: the constant of integration. `r mark(3245)`

`antiD()` knows a few of the algebraic integration techniques, roughly at the level of the basic modeling functions part of the course. When `antiD()` identifies the tilde expression as something it can handle, it returns a function whose body is the algebraic formula for the anti-derivative (although sometimes written in a cumbersome way).

When `antiD()` does not recognize its argument as a basic modeling function, the result is still an R function with the "with respect to" variable and `C` as arguments. But the body of the function is  unintelligible to a human reader (except perhaps for the `numerical_integration()`). The method of numerical integration is more sophisticated than Euler, and is highly precise and reliable. `r mark(3250)`

We're going to use `antiD()` in this daily digital simply because we want to focus on the process of differential modeling. The integrals you encounter will sometimes be ones you know how to handle algebraically. It's a good idea to do such integrals by hand and then compare to the results of `antiD()` to check your work. `r mark(3255)`

## Exercises


`r insert_calcZ_exercise("XX.XX", "JKLes","Exercises/Accum/u-on-the-bottom.Rmd")`


`r insert_calcZ_exercise("XX.XX", "NYPDR", "Exercises/Accum/horse-takes-pillow.Rmd")`


`r insert_calcZ_exercise("XX.XX", "GHNDR", "Exercises/Accum/integrals-of-the-day.Rmd")`

`r insert_calcZ_exercise("XX.XX", "KEsll","Exercises/Accum/what-is.Rmd")`

`r insert_calcZ_exercise("XX.XX", "9k3s","Exercises/Accum/MMAC-1.Rmd")`

`r insert_calcZ_exercise("XX.XX", "Rr2ds","Exercises/Accum/ups-and-downs.Rmd")`


**Example**: Find the numerical value of this definite integral.

$$\int^{7}_{3} e^{x^{2}} dx$$
**Example Solution in R**: 
```{r}
F<-antiD(exp(x^2)~x)
F(7)-F(3)
```


**Problem 1**: Find the numerical value of this definite integral.

$$\int^{5}_{2} x^{1.5} dx$$

Recall that for a definite integral of function $f()$, you find the anti-derivative $F(x) \equiv \int f(x) dx$ and evaluate it at the limits of integration. Here that will be $F(5) - F(2)$.



```{r tbf1-solution}
f <- antiD( x^1.5 ~ x )
f(5) - f(2)
```


```{r tbf-a, echo=FALSE, results="markup"}
askMC(
  "**Problem 1**: What's the numerical value of $$\\int_2^5 x^{1.5} dx  ?$$",
  0.58,6.32,"+20.10+",27.29,53.60,107.9,1486.8,
  random_answer_order = FALSE,
  id = knitr::opts_current$get()$label
)
```


```{r tbf-b, echo=FALSE, results="markup"}
askMC(
  "**Problem 2**: What's the numerical value of $$\\int^{10}_{0} \\sin( x^2 ) dx ?$$",
  "+0.58+",6.32,20.10,27.29,53.60,107.9,1486.8,
  random_answer_order = FALSE,
  id = knitr::opts_current$get()$label
)
```





```{r tbf-c, echo=FALSE, results="markup"}
askMC(
  "**Problem 3**: What's the numerical value of $$\\int^{4}_{1} e^{2x} dx ?$$",
  0.58,6.32,20.10,27.29,53.60,107.9,"+1486.8+",
  random_answer_order = FALSE,
  id = knitr::opts_current$get()$label
)
```

`r insert_calcZ_exercise("XX.XX", "iLeSB", "Exercises/Accum/sailing-over-time.Rmd")`


`r insert_calcZ_exercise("XX.XX", "Hd5zsE", "Exercises/Accum/panda-tear-cotton.Rmd")`

`r insert_calcZ_exercise("XX.XX", "YLELSE", "Exercises/Accum/falcon-tell-mug.Rmd")`


`r insert_calcZ_exercise("XX.XX", "ady5Fh", "Exercises/Accum/monkey-light-stove.Rmd")`

`r insert_calcZ_exercise("XX.XX", "JEslw", "Exercises/Accum/chain-of-differentiation.Rmd")`

::: {.todo}
This needs to be reconfigured to match the new pattern-book function/basic modeling function dicotomy.
:::

`r insert_calcZ_exercise("XX.XX", "EOSLE", "Exercises/Accum/basic-modeling-functions.Rmd")`


