# Change and accumulation {#change-accumulation}

Consider this table of **population** versus **year**, which records the overall results of the every-10-year US Census since 1790

```{r pop-table, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="US Population as counted by the US Census Bureau: 1790-2020"}
USPop <- readr::read_csv("www/census-totals.csv")
DT::datatable(USPop %>% mutate(population=prettyNum(population, big.mark=",")))
USPop <- USPop %>% 
  mutate(population = population/1e6)
```

These are ***discrete-time*** data, but nobody would dispute that the population is a continuous function of time and that we are entitled to graph it as in Figure \@ref(fig:pop-graph).

```{r pop-graph, echo=FALSE, fig.cap="US population since 1790. [Source](https://en.wikipedia.org/wiki/Demographic_history_of_the_United_States)"}
popfun <- spliner(population ~ year, data = USPop)
USPop <- USPop %>%
  mutate(yearly_growth_rate = 100*((1+growth_rate/100)^(1/10) - 1))
mod <- lm(population ~ poly(year, 2), data = USPop)
modf <- makeFun(mod)
gf_point(population ~ year, data = USPop) %>%
  gf_line(population ~ year) %>%
  gf_labs(y="Population (millions)") %>%
  gf_lims(x = c(1790, 2100), y=c(0,500))
```
Many of the student readers of this book will have children who will be about 70 years old in the year 2100. Use Figure \@ref(fig:pop-growth) to make a prediction of the population in 2100.

From the graph itself, you might just sketch out what you think is the trend. Or, more formally, and based on the ideas introduced in Block 1, you might seek an exponential or power-law function, fit it to the data, and extrapolate out to year 2100. The next figure does exactly that, but you'll have to click on "Show model prediction" to see the results.

<details>
<summary>Show model prediction</summary>
```{r pop-prediction-bad, echo=FALSE, warning=FALSE, fig.cap="Predicted US population using an exponential function (red) and a power-law function (green)"}
powermod <- fitModel(population ~ A * ((year-1750)/100)^b, data = USPop, start=list(b=2))
expmod <- fitModel(population ~ A*exp(k*(year-1750)/100), data = USPop, start=list(k=0.2))
fpower <- makeFun(powermod)
fexp <- makeFun(expmod)
growth_mod <- lm(yearly_growth_rate ~ year, data = USPop)
growth_fun <- makeFun(growth_mod)
accum <- tibble(year = 2021:2100,
                pop = 331 * cumprod(1 + growth_fun(2021:2100)/100))

gf_point(population ~ year, data = USPop) %>%
  gf_labs(y="Population (millions)") %>%
  gf_lims(x = c(1790, 2100), y=c(0,700)) %>%
  slice_plot(fexp(year) ~ year, domain(year=c(1790, 2100)), color="red",
             label_text = "Exponential", label_x=0.9, label_color="red") %>%
  slice_plot(fpower(year) ~ year, domain(year=c(1790, 2100)), color="green",
             label_text = "\nPower-law\np=2.56", label_x=0.92, label_color="green") %>%
  gf_line(pop ~ year, data = accum, color="gray") %>%
  gf_point(404.4 ~ 2060, color="blue", size=1)
```

The power-law function with power 2.56 is an excellent match to the historical data up through the present. But ...
</details>

There are occasions where the modeler has no alternative to curve fitting. However, it's best when the modeler knows as much as possible about the mechanisms of the process being modeled and can somehow incorporate those processes into the model.

With population, you know an awful lot about the mechanisms involved: birth, death, and immigration. As for births .... this is a personal matter. What I mean is that it's appropriate to look at the mechanism in terms of births per person. And if we're interested in the yearly growth of the population, what's relevant is the rate of births per person per year. That's a complicated rate, but when you multiply it by the population it turns into births per year, which is exactly right for studying population. The trend in births per person per year has been downward since 1900.  Immigration has fluctuated over the decades. That's going to be hard to predict. And death ... Old age is still the primary risk factor for death. The population is getting older, so deaths per year may be going up.

Births, deaths, and immigration are the components of the population rate of growth per year. The statements in the previous paragraph suggest that the population rate of growth per year is going down. The census data don't break down population change into its components. Still, we can check for patterns over the decades, as in Figure \@ref(fig:pop-growth).

```{r pop-growth, echo=FALSE, fig.cap="Annual growth rate of the US population (%)"}
growth_mod <- lm(yearly_growth_rate ~ year, data = USPop)
growth_fun <- makeFun(growth_mod)
gf_point(yearly_growth_rate ~ year, data = USPop) %>%
  gf_lims(x = c(1790, 2100), y=c(-.5,4)) %>%
  gf_lm() %>%
  gf_labs(y = "Growth rate (% per year)") 
```

There's a lot of fluctuation, but an overall trend stands out: the population growth rate has been declining since the mid-to late 1800s. The deviations from the trend are telling. There's a relatively low growth rate seen in the 1870 census: that's the effect of the US Civil War. The Great depression is seen in the very low growth from 1930 to 1940. Baby Boom: look at the growth from 1950-1960. The bump from 1990 to 2000? Not coincidentally, the 1990 Immigration Act substantially increased the yearly rate of immigration. 

The extrapolation of the historical pattern in annual growth rate has a zero crossing at about 2075. As you know from Block 2, a zero crossing of the rate of change corresponds to a local maximum. A reasonable prediction is therefore that the US population will max out in the second half of the 21st century and decline thereafter.

What will that maximum population be? The derivative tells us only about the argmax, not the max. What we need to do to make a prediction of the future population is to ***accumulate*** the yearly change in the population on top of the known, current population. In other words, rather than going from the population vs time to the rate of change in population versus time, we need to go the other way. This process of knowing a derivative $\partial_x f()$ and finding the unknown function $f()$ from which it was derived is called ***anti-differentiation***. Just as the name suggests, anti-differentiation is the opposite of differentiation. But how to do it?

::: {.intheworld}
The predictions from the accumulate-population-growth model are shown as a thin gray line in Figure \@ref(fig:pop-prediction-bad) along with the exponential and power law models fit directly to the population vs year data. According to the accumulation model, the population peaks in 2075 at 390 million. Professional demographers make much more sophisticated models using data from many sources. The demographers at the US Census Bureau predict that the population will reach a maximum of 404 million in 2060, shown by the little blue dot in Figure \@ref(fig:pop-prediction-bad).
:::

## Differentiation and anti-differentiation

Block 2 introduced the derivative of a continuous function by looking at discrete differences. Given a function $g(t)$, we quantified the rate of change using the differencing operator. We called this the
$$\diff{t} g(t) \equiv \frac{g(t+h) - g(t)}{h}$$
As written above, $\diff{t}$ is properly called the ***finite-difference operator***, since no suggestion is made that $h$ is anything but a small number. We moved from $\diff{t}$ to $\partial_t$ by considering the ***limit*** as $h\rightarrow 0$ This move was fraught because of the concern about dividing by zero, but in the end we found simple algebraic expressions for the derivatives of the naked modeling functions as well as a few rules for handling the basic ways of combining functions using linear combinations, products, and function combinations. These rules were the sum rule, product rule, and chain rule respectively.

In studying accumulation, we'll follow much the same path. The major difference is that our starting point is knowing a function like $\partial_t f(t)$: a derivative. From there will will construct a $f(t)$ from which $\partial_t f(t)$ *could have been derived, had we known it in the first place*. The idea is that sometimes information comes to us in the form of a rate of change and we need to figure out a function that could have generated that rate of change. 

It will help to keep in mind a fundamental principle of calculus, even if the mental image of this principle remains blurry: 

> ***Accumulation*** is the reverse operation to ***differentiation***, and *vice versa*.

To make this principle even easier to remember, let's restate it using the mathematical word used to say precisely what we mean by "accumulation":

> ***Anti-differentiation*** is the reverse operation to ***differentation***, and *vice versa*.

The mathematical notation for differentiation is simple: $\partial_t f(t)$. For anti-differentiation the notation is typographically very different. The anti-derivative of a function $f(t)$ is written:

$$\huge \color{blue}{\int} f(\color{blue}{t}) \color{blue}{dt}$$
The math notation consists of several components, each of which has something to say. You'll get familiar with these components as we move along. 

The R/mosaic notation for the anti-derivative has exactly the same format as the derivative: 

    antiD(f(t) ~ t)


## Accumulating population change

Let's return to the prediction of future population to show how it was done. Recall that we started with the total population of the US as estimated by the Census Bureau every ten years from 1790 on.^[The Census estimate was a particular and somewhat peculiar way of counting people. For instance, Native Americans were excluded from the count. So the census numbers give a false impression of a small population growing on an empty continent.] 

Those numbers are monotonically increasing, so a projection into the future based just on those numbers is bound to give an ever-increasing value. Augmenting the data with our understanding of the mechanisms of population growth, and bringing in additional information from other sources---families are getting smaller on average, people are living longer---we decided to process the census numbers to give us a better representation of growth: the annual per-capita population growth. 

Writing "annual per-capita population growth" in explicit units gives "change in population per year per person." We observed from the historical record that this quantity is decreasing over time in a way that's reasonably extrapolated into the future by a straight-line function. 

That part of the quantity which is "change in population per year" has the units of the derivative with respect to time of population: $\partial_t P(t)$. In other words, the information that we're taking as informative for predicting future population growth, the extrapolation of the growth rate, has the form of a derivative. We're going to transform that derivative into the the population function itself $P(t)$. This is an absolutely typical use of anti-differentiation. 

The annual growth rate in population $\partial_t P(t)$ can be approximated from the once-per-decade data by finite differencing:
$$\diff{t} P(\text{year}) = \frac{P(\text{year} + 10) - P(\text{year})}{10}$$
The quantity graphed in Figure \@ref(fig:pop-growth) is a little different, it is 
the population growth rate ***per capital***, that is, population growth rate divided by the population. In terms of derivatives, the quantity we are using for the prediction is the function drawn as a blue line in Figure \@ref(fig:pop-growth) we can directly estimate from the data $P(t)$ is 
$$\frac{\partial_t P(t)}{P(t)} = 0.0071 - 0.000120 (t - 2020)$$

Someone who has absolutely mastered the rules of differentiation in Block 2 might recognize that $$\partial_t \ln(P(t)) = \frac{\partial_t P(t)}{P(t)}$$
In other words, our modeling of the data has told us the derivative not of $P(t)$ but of $\ln(P(t))$. Let's accumulate that.

The growth rate model is $$\text{Growth rate model:}\ \ \ \partial_t \ln(P(t)) = 0.0071 - 0.000120(t-2020)$$

We'll start in 2020, when we know $P(t) = 331$ million, so $\ln(P(2020)) = 19.618$. According to the growth rate model, for the year to 2021, the growth rate will be  $0.0071 - 0.000120(2021-2020)n= 0.00698$$ per year. This means that $\ln(P(2021)) = 19.619 + 0.00698 = 19.6247$. 

Year $t$ | $P(t)$ | $\ln(P(t))$ | $\partial_t \ln(P(t))$ (from model)
---------|--------|-------------|----------------------------------
2020     | 331M   | 19.618      | 0.00698
2021     |        | 19.625      | 0.00686 
2022     |        | 19.632      | 0.00674 
2023     |        | 19.639      | 0.00662 
2024     |        | 19.645      | 0.00650 
2025     |        | 19.652      | 0.00638 
2026     |        | 19.658      | 0.00626 
2027     |        | 19.664      | 0.00614 
2028     |        | 19.670      | 0.00602 
2029     | 351M   | 19.677      | 0.00590


Performing the accumulation is simply accounting. Our model has told us the 4th column of the table for each year. We the first row of the table: $P(2020) = 331$M and therefore $\ln(P(2020)) = 19.618$. To get the $\ln(P(2021)$ row, add the number in the fourth column of previous row. Continue on from each row to the next. Once we have accumulated $\ln(P(t))$, we can convert it through exponentiation to $P(t)$.

## Discrete-time Euler Step

Remembering that accumulation is the opposite of differentiation, let's invert differentiation. The setting is that we know $\partial_t f(t)$ but do not yet know $f(t)$. Recall the familiar form of derivative, approximated by 
$$\color{blue}{\partial_t f(t_0)} \equiv \frac{f(t_0+\color{blue}{h}) - f(t_0)}{\color{blue}{h}}$$
We've put the quantities we know in <span style="color: blue;">blue</span>. We know the derivative $\color{blue}{\partial_t f(t)}$ and we have selected some $\color{blue}{h}$. (In the population example, we set $\color{blue}{h}$ to be 1 year.) Re-arranging the above formula gives: 
$$f(t_0+h) = \color{blue}{f(t_0)} + \color{blue}{h} \times \color{blue}{\partial_t f(t_0)}$$
Notice that we're saying we know $\color{blue}{f(t_0)}$. This is just like saying that we knew the population in year 2020; we know where we start.

The formula allows us to find $f()$ at time $h$ in the future. That is, we compute the unknown $f(t_0 + h)$ from what we already know.

This projection into the future using the (known) rate of change is called an ***Euler step***. The idea of inverting differentiation is to take one Euler step after another, constructing the future values one after the other 
$f(t_0 + h)$, $f(t_0 + 2h)$, $f(t_0 + 3h), \cdots$. After each step, we know something more about $f()$ which we can use in taking the next step. For instance, once $f(t_0 + h)$ has been calculated, we get $f(t_0 + 2h)$ by applying the Euler step formula:

$$f(t_0 + 2h) = \color{blue}{f(t_0 + h)} + \color{blue}{h} \times \color{blue}{\partial_t f(t_0 + h)}$$
Notice that we've written $\color{blue}{f(t_0 + h)}$ in blue. Although we didn't know $f(t_0 + h)$ at the very start of the process, we figured it out by the first Euler step and it's ready for use in the second step.

## Symbolic anti-derivatives

The Euler method involves a ***finite*** $h$, which is just to say that $h$ must be ***non-zero***. Otherwise, $f(t_0 + h)$ would be exactly the same as $f(t_0)$. For some functions, however, it's possible to construct the anti-derivative without needing to deal with $h$ at all. 

Recall that anti-differentiation undoes differentiation, and vice versa. In the previous Block, we found the symbolic derivatives of the basic modeling functions and general methods for differentiating functions constructed by linear combination, products, and function composition. Using the techniques from Block 2, tables can be constructed of functions and their derivatives, looking like this:

$f(x)$   | $\partial_x f(x)$
---------|-------------------
$e^{x}$  | $e^x$
$\ln{x}$ | $1/x$
$\sin{x}$ | $\cos{x}$
$\pnorm(x)$ | $\dnorm(x)$
$x^2$    | $2x$
$\vdots$ | $\vdots$

Transforming this into a table of anti-derivatives is merely a matter of re-labeling the columns:

$\int g(x)dx$   | $g(x)$
---------|-------------------
$e^{x}$  | $e^x$
$\ln{x}$ | $1/x$
$\sin{x}$ | $\cos{x}$
$\pnorm(x)$ | $\dnorm(x)$
$x^2$    | $2x$
$x^p$    | $p x^{p-1}$
$\vdots$ | $\vdots$

But unlike differentiation, anti-differentiation has no easy equivalents of the product rule or the chain rule (for compositions of functions). 

Recall that every smooth, continuous function has a derivative defined everywhere in the function's domain. Similarly, every function has an anti-derivative, and even discontinuous, un-smooth functions have nicely behaved anti-derivatives. In this sense anti-differentiation is easy. It's only the algebra of anti-differentiation that can be hard or often literally impossible.

::: {.fortheinstructor}
Pyrrhus was a Greek king who invaded Italy in 280 B.C. and fought the Romans. His first battle, at Heraclea, was a famous victory, as was his second battle a year later at Asculum. History records Pyrrhus saying, after his second victory, “If we are victorious in one more battle with the Romans, we shall be utterly ruined.” The victories were so costly that the army could not be sustained.

Such ***pyrrhic victories*** occur throughout history: the British at Bunker Hill (1775),  Napolean at Borodino (1812), and Lee at Chancellorsville (1863).

A pyrrhic victory in mathematics pedagogy: the visualization of integration as the "area under a curve." This account of integration is utterly dominant among graduates of calculus courses. It is a brilliant and successful way to give the abstract operation of anti-differentiation an easily remembered visage. But it creates costs that are simply not worth bearing. 

i. There are not so many genuine applications for finding areas under curves. Students, seeing them as emblematic of calculus, are often dis-motivated by the lack of connection between the rhetoric of the importance of calculus and the seeming unimportance of the primary application.
ii. It's extremely difficult to make a connection between the many genuine applications of anti-differentiation and the mental image of area.
iii. Because area is most easily shown as a fixed, delimited region, students are introduced to ***integration*** before they see ***anti-differentiation***. It can be hard for students to make the transition between "slope at a point" or "tangent line" and a slope ***function***. This cost has to be paid all over again when moving from integration to anti-differentiation.
iv. The Yin and Yang of calculus are differentiation and anti-differentation. Students are successfully taught that if $f(x)$ is a function, the derivative is the slope of that function and the integral is the area under the function. This creates an unhelpful illusion that derivatives and anti-derivatives are related through the function, that there is an intermediary between them. Translating this image to the metaphor of family generations, the picture looks like $f(x)$ is the **parent** of $\partial_x f(x)$ and $\int f(x)dx$ is the **parent** of $f(x)$. In other words, the incorrect image is encouraged that an anti-derivative is the **grandparent** of the derivative.  In fact the anti-derivative and the derivative have a parent-child relationship.
vi. At best, for many people, the relationship between the slope function and the area function is hard to see and sometimes mysterious. (Of course it's hard to see: the slope function is the ***second derivative*** of the area function.) This 

For these reasons, I encourage instructors to avoid defining the calculus operation as "area under the curve." Make use of areas when they are part of a genuine application of calculus. Save the Riemann Sum for courses in analysis. For demonstrating anti-differentiation, the starting point should be a function which we know to be the derivative of the sought-after function. Teach Euler as connecting together short segments of slopes. Emphasis the connections between differentiation and anti-differentiation from the start: both are relationships between one function and another function, just as every person is both a mother and a child. The anti-derivative is the mother of the child, the derivative is the child of the mother.
:::

::: {.objectives}
```{r echo=FALSE, results="markup"}
state_objective("Int-2b", "Utilize your knowledge of derivatives rules to backwards solve for anti-derivatives.")
state_objective("Int-2f", "Understand that the output of anti-differentiation is a function known as a general solution and more information is necessary to find the particular solution [DTK: This is language I associate with differential equations. How about: Find the constant of integration.]")
state_objective("Int-2c", "Understand the notation of an indefinite integral to include a. the integral symbol; b. variable of integration (\"with respect to\" variable); c. the constant of integration")
state_objective("Int-2d", "Understand the relationship between a (child) function  and its (parent) integral.")
```
:::

# LATER: Modeling with Accumulation

::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Int-5c", "Know common scientific application relationships between base functions and anti-derivatives: a. Acceleration, Velocity, Position; b. Force, Work; c. Area, Volume; d. Cash flow, assets")
```
:::

Include the mortgage problem


    
::: {.intheworld}

Calculation of luminance using light intensity at different wavelengths integrated over the luminance function. https://en.wikipedia.org/wiki/Luminous_efficiency_function
:::

::: {.todo}
Pick up on the Lorenz curve `{.intheworld}` in Blocks 1 and 2. Integrate to find the Gini coefficient. Show that the Gini coefficient is the same for very different types of inequality and that therefore it's not such a good measure. How about the integral over the poorest 25% or 50% of society."
:::
