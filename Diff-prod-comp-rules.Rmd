# Differentiation by algebra {#prod-comp-rules}

## Memorizing derivatives

Generations of calculus students have had to memorize the algebraic form  of some simple derivatives and *rules* for constructing the derivative of complicated algebraic expressions based on the simple forms.

Often, courses in many fields that have calculus as a pre-requisite make use of these simple forms and rules. So it's helpful to commit the most common ones to memory. 

**Naked modeling functions**

* The derivative of a **constant function** is the zero function.
    - $f(x) \equiv 7 \ \ \implies \ \ \partial_x f(x) \equiv 0$
    - $g(x) \equiv a \ \ \implies \ \ \partial_x g(x) \equiv 0$
    - $h(x,y) \equiv y \ \ \implies \ \ \partial_x h(x) \equiv 0$
* The derivative of the **identity function** is the 1  function.
    - $f(x) \equiv x \ \ \implies \ \ \partial_x f(x) \equiv 1$
    - $g(t) \equiv t \ \ \implies \ \ \partial_t g(t) \equiv 1$
    - BUT ... $h(x, t) \equiv t \ \ \implies \ \ \partial_x h(x, t) \equiv 0$. Make sure to pay attention to the "with respect to" argument,  that is, the subscript on the $\partial$.
* The derivative of an **exponential function** is an exponential function. 
    - $f(t) \equiv e^t \ \ \implies\ \ \partial_t f(t) = e^t$.
    - BUT ... watch out that the with-respect-to argument matches the argument in the exponent, for instance, $f(x, t) \equiv e^t  \ \ \implies\ \ \partial_x f(x, t) = 0$.
* Sinusoids have derivatives that are sinusoids.
    - $f(x) \equiv \sin(x)\ \ \implies\  \ \partial_x  f(x) = \cos(x)$
    - $f(x) \equiv \cos(x)\ \ \implies\  \ \partial_x  f(x) = -\sin(x)$. Note the minus sign!    
    
* The derivative of a **power-law function**  is a power-law  function.
    - $f(x) \equiv x^2 \ \ \implies \ \ \partial_x f(x) \equiv 2  x$
    - $f(x) \equiv x^3 \ \ \implies \ \ \partial_x f(x) \equiv 3  x^2$
    - $f(x) \equiv x^{1/2}\ \ \implies \ \ \partial_x f(x) \equiv -\frac{1}{2} x^{-1/2}$
    - $f(x) \equiv x^{1.43}\  \ \implies \ \ \ \partial_x f(x) \equiv 1.43 x^{0.43}$
    - In general, the rule is $f(x) \equiv x^n  \ \ \implies \partial_x f(x) \equiv n x^{n-1}$ but  only when $n \neq 0$. When $n = 0$ the function would be $x^0 = 1$ which is simply a constant so the derivative is zero.
* The derivative of the **natural logarithm function** $\ln(x)$  is  the reciprocal $1/x$.
    - $f(x) \equiv \ln(x)\ \ \implies \  \ \partial_x f(x) \equiv x^{-1}$
    
**Basic modeling functions**

Each of the basic modeling functions $f(x)$ is a naked function with a scaled input with the general form $f\left(A (x-x_0)\right)$. We'll call $A$ the ***scaling factor*** and $x_0$ the ***shift***. You differentiate the basic modeling functions in two steps:

1. Identify the derivative of the naked modeling function $\partial_x f(x)$. Let's call this derivative $f'(x)$. For instance, when $f(x) \equiv e^x$, then $f'(x) = e^x$. Or, when $f(x) \equiv \ln(x)$, then $f'(x) = 1/x$.
2. Replace the simple $x$ argument in (1) with the scaled input $A(x-x_0)$, then multiply by the scaling factor. The result will look like $A f'\!\left(A(x-x_0)\right)$
The derivative $\partial f\left(A (x-x_0)\right) = 

Naked function | Basic function | $\partial_x$ Basic function
---------------|----------------|-------------------
$e^x$          | $e^{k(x-x_0)}$ | $k e^{k(x-x_0)}$
$x^p$          | $\left(A(x-x_0)\right)^p$ | $A\, p\, \left(\strut A(x-x_0)\right)^{p-1}$
$\ln(x)$       | $\ln\left(k (x-x_0)\right)$ | $\frac{k}{x-x_0}$
$\sin(x)$      | $\sin\left(\frac{2\pi}{P}(x-x_0)\right)$ | $\frac{2\pi}{P}\cos\left(\frac{2\pi}{P}(x-x_0)\right)$

At first glance, the pattern for $\sin()$ may seem different, but it's really not. When we write $\sin\left(\frac{2\pi}{P}(t-t_0)\right)$ the scaling factor is $\frac{2\pi}{P}$.


## Exponentials and logarithms (optional)
    
    
 with the same base. The relationship is particularly simple when the base is $e$.
    - $f(x) \equiv e^x\ \ \implies \partial_x f(x) \equiv e^x$
    - $g(x) \equiv 2^x\ \ \implies \partial_x g(x) \equiv \ln(2)\cdot 2^x$
    - $h(x) \equiv 10^x\ \ \implies \partial_x h(x) \equiv \ln(10)\cdot 10^x$
    - Note that  $\ln()$ is the so-called "natural logarithm," which is the logarithm to the base $e \approx 2.71828182845905...$
    

    - $g(x) \equiv \log_{10}(x)\ \ \implies \  \ \partial_x g(x) \equiv [\ln(10) x]^{-1}  \approx  [2.30259  x]^{-1}$
    - $h(x) \equiv \log_2(x)\ \ \implies \  \ \partial_x h(x) \equiv [\ln(2) x]^{-1}  \approx  [0.69315  x]^{-1}$
    
## Combining functions

In Chapter \@ref(fun-assembling) we introduced three major methods for putting two or more basic modeling functions together in order to make a new function.

i. Linear combinations: e.g. $a f(x) + b g(x)$
i. Products: e.g. $f(x) g(x)$
i. Composition:, e.g. $f(g(x))$ (which is usually a different function than $g(f(x))$.)

In all three cases, you start with two functions $f(x)$ and $g(x)$. Whatever those functions be, you'll need to find their individual derivative. Normally, we write those derivatives as $\partial_x f(x)$ and $\partial_x g(x)$. But for seeing the patterns of the derivatives of the three cases, it helps to use a much more compact (though less general) notation: $\partial_x f(x) = f'(x)$ and similarly for $g'(x)$. With this notation, the derivatives of linear combinations, products, and compositions is a matter of applying a formula, traditionally called a ***rule***.

i. Linear combinations

$$\partial_x \left[\strut a f(x) + b g(x)\right] = a f'(x) + b g'(x)$$
ii. Products: 
$$\partial_x \left[\strut f(x) g(x)\right] = f'(x) g(x) + f(x) g'(x)$$
iii. Composition:
$$\partial_x \left[\strut f(g(x))\right] = f'(g(x)) g'(x)\\
\ \\
\partial_x \left[\strut g(f(x))\right] = g'(f(x)) f'(x)$$

The derivative of a linear combinations is particularly simple: the derivative of a linear combination of functions is the same linear combination of the derivatives of the functions, that is, if $$h(x) \equiv a f(x) + b g(x)\ \ \implies \ \ \partial_x h(x) = a \,\partial_x f(x) + b\, \partial_x g(x)$$

::: {.workedexample}
Consider the function $h(t) \equiv A e^{kt} + B$. This is a linear combination of two functions, which we can call $f(t) \equiv e^{kt}$ and $g(t) \equiv 1$.

Of course, $f(t)$ and $g(t)$ are basic modeling functions so we have memorized their derivatives: $f'(t) = k e^{kt}$ and $g'(t) = 0$. 

Putting this together gives $$\partial_t h(t) = A\, k\, e^{kt} + B\times 0= A\, k\, e^{kt}$$
:::

The derivative of a ***polynomial*** follows the linear combination rule. That's because polynomials are a linear combination of monomials, $x^0$, $x^1$, $x^2$, and so on.

The consequence is that the derivative of a polynomial is another polynomial, with each term being reduced by one order.

- $\partial_x x^0 = 0$
- $\partial_x x^1 = x^0 = 1$
- $\partial_x x^2 = 2 x^1 = 2x$
- and so on.

Example:  $f(x) \equiv a + b x +  c  x^2\  \  \implies\ \ \partial_x f(x) \equiv b  + 2 c x$

## Composition of functions




The chain rule, which considers the  derivative  of a function $h(x)$ which  is the *composition* of two functions: $h(x) \equiv f(g(x))$. 

We'll call $f()$  the "exterior function" and  $g()$ the interior  function. When we write the exterior function on its own, we'll call  the input $y$.  The interior  function will be written in  the ordinary way. For example:

Composition                     | Exterior function | Interior  function
:------------------------------:|:-----------------:|:-----------------:
$h(x) \equiv \sin(a x)$         | $\sin(y)$         | $y(x) \equiv  ax$
$h(t) \equiv e^{k x}$           | $e^y$             | $y(x)  \equiv  kx$
$h(x) \equiv \sqrt{1  + x^2}$   | $\sqrt{y}$        | $y(x) \equiv 1  + x^2$

To apply the chain rule, follow these  steps:

1. Identify the exterior function $f(y)$ and the interior function $y(x) \equiv g(x)$.
2. Calculate the derivative of the interior function: $\partial_x y(x)$
3. Calculate the derivative  of the exterior function with respect  to  $y$,  that  is,  $\partial_y f(y)$ 
4. Write down $\partial_y f(y)$ but, everywhere that a $y$ appears, replace it with  $g(x)$.
5. Multiply the expression in (4) by $\partial_x g(x)$.  The result is  the derivative  of the composition with respect to $x$.

Example:  $\partial_x \sin(a x)$.

1. The exterior function is $\sin(y)$ and the interior function is  $y(x) \equiv ax$
2. Derivative  of interior  function:  $\partial_x y(x)  \equiv a$.
3. Derivative  of exterior function:  $\partial_y \sin(y) \equiv \cos(y)$
4. Replace $y$ in (3) with $y(x) \equiv ax$,  giving  $\cos(ax)$.
5. Multiply the result in (4)  by  $\partial_x y(x)$,  giving the final  result $a \cos(ax)$.  

Example: $\partial_x \sqrt{1 + x^2}$

1. The exterior function is $\sqrt{y}$, the interior function is $y(x) \equiv 1 + x^2$.
2. The derivative  of the  interior function  is $\partial_x  y(x)  \equiv 2 x$.
3. The derivative of the exterior function  is  $-1/\sqrt{y}$
4. Replace  $y$ in  (3) with the interior function, giving  $-1/\sqrt{1 + x^2}$
5. Multiply the result in (4) by $\partial_x y(x)$, giving the final result $-2x/\sqrt{1 + x^2}$


::: {.workedexample}

We'll start with the familiar linear approximation for $f()$, namely

$$f({\cal X} + {\cal H}) = f({\cal X}) + {\cal H}f'({\cal X})$$
We have used funny symbols $\cal X$ and $\cal H$ here, but that doesn't make any fundamental difference. Since $\cal X$ can be any value whatsoever, let's make it the output of $g()$ for an input $x$: that is, ${\cal X} = g(x)$. If we were to increase the input to $x+h$, the output would change: $g(x+h) = g(x) + h g'(x)$. Since the output of $g()$ with input $x+h$ is ${\cal X} + {\cal H}$, that is, $g(x+h) = {\cal X} + {\cal H}$. Since we already know that ${\cal X} = g(x)$ we can solve for ${\cal H}$, namely
$${\cal H} = h g'(x)$$
With the definitions of ${\cal X}$ and ${\cal H}$ in terms of $g(x)$ and $h$, we find
$$f(g(x+h)) = f(g(x)) + h g'(x) f'(g(x))$$ Re-arranging, we get $$\frac{f(g(x+h)) - f(g(x))}{h} = f'(g(x)) g'(x)$$ the formula for the chain rule.
:::




## Product of functions


::: {.todo}
See 141 DD 31 for drill problems
:::

## Checking your work

::: {.todo}
1. When you get the result for a derivative, plot it out along with the finite-difference approximation. If they are close, you're good. 

2. Apply the `antiD()` operator to your derivative and graphically confirm that there is a constant difference between the output of `antiD()` and the original function.
:::

## Working with algebraic derivatives

::: {.todo}
These need to be re-organized, with some of the content broken out into the chapter.
:::


```{exercise, name="optim-violet"}
```
<details>`r knitr::knit_child("Exercises/Diff/optim-violet.Rmd")`</details>

```{exercise, name="optim-pink"}
```
<details>`r knitr::knit_child("Exercises/Diff/optim-pink.Rmd")`</details>


```{exercise, name="optim-red"}
```
<details>`r knitr::knit_child("Exercises/Diff/optim-red.Rmd")`</details>

```{exercise, name="optim-blue"}
```
<details>`r knitr::knit_child("Exercises/Diff/optim-blue.Rmd")`</details>

```{exercise, name="optim-purple"}
This is probably misplaced. and the `plot_lens()` function would have to be provided.
```
<details>`r knitr::knit_child("Exercises/Diff/optim-purple.Rmd")`</details>


