# Computing derivatives {#computing-derivs}

To differentiate a function $g(x)$ means simply to produce the corresponding function $\partial_t g(x)$. This is often called "finding the derivative," language that resonates with the high-school algebra task of "finding $x$." Rather than conjuring an image of search high and low for a missing function, it's more realistic to say, "compute the derivative." `r mark(2150)`

In this chapter we'll introduce two ways of computing a derivative. For simplicity we will write $x$ for the with-respect-to-variable, although in practice you might be using $t$ or $z$ or something else. `r mark(2155)`

* Symbolic differentiation, which uses a set of re-writing rules
* Finite-differencing, which is based directly on the differencing operator ${\cal D_x}$

In the days when functions were always presented using formulas, symbolic differentiation was usually the only method taught. Nowadays, when functions are just as likely to be described using data and an algorithm, finite-differencing provides the practical approach. `r mark(2160)`

## A function from a function

Recall that the goal of differentiation is to make a function out of an already known function. We'll call the already known function $g(x)$. In Chapter \@ref(change-relationships) we've outlined the properties that the new function should have and gave a nice naming convention, $\partial_x g(x)$ that shows where the new function comes from. In this section we'll put that aside and focus on the question of what it means to "make a function." `r mark(2165)`

When mathematics is done with paper and pencil, "making a function" is a matter of writing a formula, such as $x^2 \sin(x) + 3$ and sometimes giving a name to the formula, e.g. $h(x) \equiv x^2 \sin(x) + 3$. We are essentially writing something down that will make sense when viewed by another person trained in the conventions of mathematical notation. `r mark(2170)`

For a computer, on the other hand, a function is a definite kind of thing. We  "make a function" by creating that kind of thing and, usually, giving it a name. We use (or "evaluate") a function by using a definite syntax, which in R involves the use of parentheses, for instance *name*`(`*input*`)`. `r mark(2175)`

The computer language itself provides specific means to define a new function. In R/mosaic, you first construct a tilde expression naming the function inputs (right side of the tilde) and specifying the algorithm of that function (left side of the tilde), as with this formula:
```{r}
f_description <- x^2 * sin(x) + 3 ~ x   # a tilde expression
```

On its own, `f_description` cannot be used like a function because it was constructed as something else: a tilde expression. Trying to use `f_description` in the way one uses a function produces an error.
```
f_description(2)
 Error in f_description(2): could not find function "f_description"
```

In between the tilde expression and the final result---a function---is software that translates from tilde-expressions into functions:
```{r}
f <- makeFun(f_description)
```

The new creation, `f()` can now be used like any other function, e.g.
```{r}
f(2)
```

Down deep inside, `makeFun()` uses a more basic function-creation syntax which looks like this
```{r}
function(x) {x^2 * sin(x) + 3}
```

You can see all the same information that was in the tilde description, just arranged differently.

Almost every computer language provides something like `function`. There workings are advanced technology and essentially impossible to describe in much the same way as the workings of a transistor or a COVID vaccine are known only to specialists. `r mark(2180)`

In the same spirit as `makeFun()`, which translates a tilde-expression into the corresponding function, in R/mosaic you have `D()` which takes a tilde expression and translates it into the derivative of the function described. For example:
```{r}
D(f_description)
```

`r insert_calcZ_exercise("XX.XX", "kelwx", "Exercises/Diff/tilde-function.Rmd")`


## Finite differencing

You can use the definition of the slope function $${\cal D}_x f(x) = \frac{f(x+0.1) - f(x)}{0.1}$$
to create an approximation to the derivative of any function. Like this:

```{r}
g <- makeFun(sin(2*x)*(pnorm(x/3)-0.5) ~ x)
dg <- makeFun((g(x+0.1) - g(x))/0.1 ~ x)
```

Whenever you calculate a derivative function, you should check against mistakes or other sources of error. For instance, whenever the derivative is zero, the original function should have an instantaneous slope of zero. Figure \@ref(fig:check-dg) shows a suitable plot for supporting this sort of check.
```{r check-dg, fig.cap="A check that zero-crossings (blue) of the derivative function (red) correspond to inputs where the original function is flat (black)." }
zeros_of_dg <- findZeros(dg(x) ~ x, xlim=c(-5,5))
slice_plot(g(x) ~ x, domain(x=c(-5,5)), npts=500) %>%
  slice_plot(dg(x) ~ x, color="red", npts=500) %>%
  gf_hline(yintercept = ~ 0, color = "orange", size=2, alpha=0.2) %>%
  gf_vline(xintercept = ~ x, data=zeros_of_dg, color="blue")
```

Look very closely at Figure \@ref(fig:check-dg), particularly at the places where the blue vertical markers cross the function $g(x)$ (black). They should cross exactly at the flat zone, but they are a little shifted to the left. (You might have to zoom in on the plot to see the offset between the vertical blue marker and the local maximum of the function.) That's the sense in which the finite-difference approach gives an approximation. The small left-shift stems from the use of 0.1 in the definition of the zero function. Use a smaller value, say 0.01 or 0.001, and you won't be able to see the shift at all. `r mark(2185)`

::: {.takenote}
In modeling work, there's nothing wrong with an approximation so long as it is good enough for your purposes. We picked the value 0.1 for our definition of the slope function because it works very well with the pattern-book functions. Here, "very well" means you can't easily see in the graph any deviation compared the the exact derivative. `r mark(2190)`

When a calculation can be done exactly (without outrageous effort) it certainly makes sense to use the exact method. However:

1. It's useful to have an easy, approximate method always at hand. This lets you check the results of other methods against the possibility of some blunder or mis-conception. The slope function approach to differentiation is certainly easy, and if you think the approximation isn't good enough, then instead of 0.1 use something smaller. (Chapter \@ref(evanescent-h) discusses how small is **too** small.)
2. The computer makes it practical to employ the slope function as a useful approximation to the derivative. There are many other mathematical methods that the computer has made feasible, for instance the methods of ***machine learning***. These methods create functions that sometimes cannot be handled by the traditional ("exact") methods of differentiation.  `r mark(2195)`
:::


`r insert_calcZ_exercise("XX.XX", "K05LrF", "Exercises/Diff/fly-speak-canoe.Rmd")`

`r insert_calcZ_exercise("XX.XX", "n78LrF", "Exercises/Diff/wolf-talk-kayak.Rmd")`


## The slope-function operator

Take a look at the statement we used to construct the slope function of `g()`:
```r
dg <- makeFun((g(x+0.1) - g(x))/0.1 ~ x)
```

There's almost nothing about this statement that has anything to do with the specifics of how we defined `g()`; we could have used any $g()$. The "almost" in the previous sentence is about the choice of 0.1, which isn't guaranteed to be small enough. `r mark(2200)`

It would be convenient to have an ***operator*** that automates the process of constructing a slope function. This is a programming task and in that sense beyond the scope of this course. Still, it's a good idea to get in the habit of reading programming code. So here goes ... creating a `slopeFun()` operator: `r mark(2205)`

1. Remember the `function(){}` syntax for creating an operator. (If I were speaking to experienced programmers, I would have said "function" instead of "operator." )
2. We're going to use a ***tilde expression*** as the input to `slopeFun()`. This is how the other R/mosaic operators work. That will be easier to the user and will also give us access to those other operators if we need them in writing `slopeFun()`.
3. The object returned by the `slopeFun()` operator will be, of course, a ***function***. We've been using `makeFun()` to make our mathematical functions, so expect to see that in the code for `slopeFun()`.
4. There's that nuisance about using 0.1 and whether that is small enough. So let's use an `h` argument that we can change when needed.

```{r}
slopeFun <- function(tilde, h=0.1) { #two arguments, one with a default value
   g <- makeFun(tilde)  # Turn the tilde expression into a function
   
   makeFun((g(x + h) - g(x))/h ~ x, h=h) # just like before, with h instead of 0.1
}
```

Figure \@ref(fig:slope-quick-check) shows the results of a quick check of whether the function works and whether `h=0.1` is small enough.

```{r slope-quick-check, fig.cap="Checking whether the homemade `slopeFun()` operator works."}
dx_sin <- slopeFun(sin(x) ~ x)
slice_plot(dx_sin(x, h = 0.00000000001) ~ x, domain(x=c(-5,5)), size=2) %>%
  slice_plot(dx_sin(x, h=0.001) ~ x, color="orange3")
```

::: {.scaffolding}
You can decide for yourself whether the red and black curves in Figure \@ref(fig:slope-quick-check) are similar enough for your purposes. Even better, do some exploring yourself in a `r sandbox_link()`. You'll have to copy and paste into your sandbox both the code defining `slopeFun()` and the graphics statements. Start by replacing `dx_sin(x, h=0.1)` by `dx_sin(x, h=0.0001)` `r mark(2210)`

`r insert_calcZ_exercise("XX.XX", "l2ksw", "Exercises/Diff/h-too-small.Rmd")`


In practice, you won't need to use `slopeFun()`. You'll use `D()` instead, which gives the "exact" results whenever it can.

## Symbolic differentiation

Symbolic differentiation is the process of taking a formula and translating it to a new formula according to certain patterns or ***rules***. Each rule is ultimately derived from the definition of the slope function and the differencing operator. `r mark(2215)`

As you recall, the ***differencing operator*** $\diff{x}$ turns a function into its slope function
$$\diff{x} f(x) \equiv \frac{f(x+h) - f(x)}{h}$$

Let's look at one where we already know the result: The straight line function $\line(x) \equiv a x + b$ has a slope function that is constant: $\diff{x}\line(x) = a$


$$\diff{x}\line(x) = \frac{a (x+h) + b - (a x + b)}{h} = \frac{ah}{h} = a$$
The derivative is the slope function with $h$ made as small as possible. It's tempting to think of this as $h = 0$, but that would imply dividing by zero in the differencing operator. Being cautious about this, we write that differentiation is differencing with $h \rightarrow 0$, or `r mark(2220)`

$$\partial_x \line(x) \equiv \lim_{h\rightarrow 0} \frac{\line(x+h) - \line(x)}{h} =\\
\ \\
= \lim_{h\rightarrow 0} \frac{a h}{h} = a$$

This derivation is unarguably correct for any non-zero $h$. 

This short derivation gives us a basic differentiation rule which we can divide into 3 special cases. 

- **Line rule**: $\partial_x ax + b = a$
    i. $\partial_x ax = a$
    ii. $\partial_x b = 0$
    iii. $\partial_x x = 1$
    
Remember that $\partial_x f(x)$ for any $f(x)$ is always a function. The functions associated with the line rule are all ***constant functions***, meaning the output doesn't depend on the input.

Only for the $\line()$ function and its three special cases is the derivative a constant function. And $\line()$ is the only function for which the $h$ in the differencing operator disappears on its own. For instance, consider $g(x) \equiv x^2$:  `r mark(2225)`

$$\partial_x [x^2] = \lim_{h\rightarrow 0}\frac{(x+h)^2 - x^2}{h} =\\
\ \\
=\lim_{h\rightarrow 0}\frac{(x^2 + 2 x h + h^2) - x^2}{h} =\\
\ \\
= \lim_{h\rightarrow 0}\frac{2 x h + h^2}{h} =\\
\ \\
=\lim_{h\rightarrow 0} [2x + h]$$
It's accepted that the limit of a sum is the sum of the limits. Also, the limit of something not involving $h$ is just that thing: for instance $\lim_{h\rightarrow 0}2x = 2x$.

$$\partial_x [x^2] = 2x + \lim_{h\rightarrow 0}h = 2x$$


We'll write this as another differentiation rule.

- **Quadratic rule**: $\partial_x [x^2] = 2x$

::: {.todo}
[When introducing product rule ...] point out that $\partial_x x^2 \neq (\partial_x x)(\partial_x x) = 1$
:::



Let's take on $h(x) \equiv e^x$:

$$\partial_x e^x = \lim_{h\rightarrow 0}\frac{e^{x+h} - e^x}{h} = e^x \lim_{h\rightarrow 0}\left[\frac{e^h - 1}{h}\right]$$
At a glance, it can be hard to know what to make of $\lim_{h\rightarrow 0} (e^h-1)/h$. Setting $h=0$ in the denominator is perfectly legitimate and gives $e^0 - 1 = 0$. But that still leaves the $h$ in the numerator. Still, for any non-zero $h$, the division is legitimate, so let's see what happens as $h \longarrow 0$:
```{r}
f <- makeFun((exp(h) - 1)/h ~ h)
f(0.1)
f(0.01)
f(0.001)
f(0.0001)
f(0.0000001)
f(0.0000000001)
```

Setting $h$ exactly to zero, however, won't work: it produces `NaN`.
```{r}
f(0)
```

Since $\lim_{h\rightarrow 0} (e^h-1)/h = 1$, we have

* **Exponentiation rule**: $\partial_x e^x = e^x$

::: {.todo}
[Under taylor series, show that $\frac{e^h - 1}{h} \rightarrow 0$.]
:::

Still another example: the reciprocal function, written equivalently as $1/x$ or $x^{-1}$

$$\partial x^{-1} = \lim_{h\rightarrow 0}\frac{1/(x+h) - 1/x}{h} =\\
\ \\
= \lim_{h\rightarrow 0}\frac{x - x+h}{x(x+h)h} = -\lim_{h\rightarrow 0}\frac{h}{x(x+h)h} =\\
\ \\
= -\lim_{h\rightarrow 0}\frac{1}{x^2 + hx}$$
So long as $x \neq 0$, there is no divide-by-zero problem, but let's see what the computer thinks:

```{r}
g <- makeFun(-1/(x^2 + h*x) ~ h, x=10)
g(0.1)
g(0.01)
g(0.001)
g(0.0001)
g(0.0000001)
g(0.0000000001)
g(0)
```

Setting $h$ to zero in the last expression gives another differentiation rule:

* **Reciprocal rule**: $\partial_x \frac{1}{x} = -\frac{1}{x^2}$

We'll save for later the derivation of the derivatives of the other pattern-book functions, but note that the gaussian function is **defined** to be the derivative of the sigmoidal function.

Name | $f(x)$ | $\partial_x f(x)$   | demonstrated already
-----|---------------|------------------
exponential | $e^x$     | $e^x$ | yes
logarithm (natural)  | $\ln(x)$  | $1/x$  | not yet
sinusoid    | $\sin(x)$ | $\cos(x)$ | not yet
square | $x^2$  | $2x$ | yes
proportional | $x$   | $1$ | yes
constant | 1  | 0 | yes
reciprocal | $1/x$ or $x^{-1}$   | $-1/x^2$ | yes
gaussian (hump)     |  $\dnorm(x)$           |  $-x\, \dnorm(x)$ | not yet
sigmoid  |  $\pnorm(x)$            |  $\dnorm(x)$ | by definition

::: {.todo}
When doing the basic modeling functions, show that the derivative of the cosine is -sin. Use $\partial_x sin(x) = sin(x + \pi/2)$ and apply the scaling rule to that.

Also, derive $\partial_x x^p = \partial_x e^{p\ln(x)} = e^{p\ln(x)} \times \frac{p}{x} = \frac{p}{x} x^p = p\, x^{p-1}$
:::

`r insert_calcZ_exercise("XX.XX", "ykels", "Exercises/Diff/finch-trim-kayak.Rmd")`
 
