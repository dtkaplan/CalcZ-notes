# Taken out of Diff- series



## Functions and perception

<!--
As you know, a function takes one or more inputs and returns a value as output. The functions we examine in CalcZ take *quantities* as inputs and return a *quantity* as an output.
The algorithm that forms the body of the function describes arithmetic and other calculations that can turn the inputs into the output.

On the other hand, we can also use ***tables*** as functions. With a table, you specify the input, look up that input in one of the colums of the table which brings you to the right row. Then read out from that row the value in another column to be the output. The quantitative operation needed for table lookup is simple comparison. The floor/corridor/door metaphor describes table lookup as well as function evaluation.

In the previous block, we constructed functions to represent the patterns seen in data. In one example, we constructed a function $g(t) = A + B e^{-k t}$ to represent the temperature of water cooling in a mug as a function of time. In another example, we summarized the pattern of rising and falling tides.

It's common sense that data is stored in tables. But we could easily represent any smooth mathematical function, such as our basic modeling functions, as a table look-up problem. Indeed, in the era before computers, many mathematical functions were used in exactly this manner: a printed table in which a person could search for a match to the input and retrieve a value for the output.

::: {.todo}
[Picture of some nice old table.]
:::

In the computer era, we still routinely represent functions this way: data stored in computer files. For instance, an MP3 file is not much more than a sequence of numbers that record a complicated function of time: the air pressure variations of sound. Similarly, digital images record functions of $x$ and $y$ over a limited domain. Given $x$ and $y$ as input, you can look up the output by going to the right pixel.

-->

We humans perceive the world using sight and sound and our other senses. Both sight and sound are highly complex biophysically, but the *process* can be broken down trivially into a relationship: *reality $\longrightarrow$ perception*.

Perception takes place in your mind and brain, extended by sensors such as the retina of the eye and cochlea of the ear. There is considerable scientific understanding of how the retina and cochlea work, but perception itself is still somewhat mysterious and involves the interaction of sensor input with our memory and other cognitive processes.

Still, for both hearing and sight, we can analyze the *reality $\longrightarrow$ perception* process by adding an intermediate layer:

- For sight: *reality $\longrightarrow$ image $\longrightarrow$ perception*
- For hearing: *reality $\longrightarrow$ sound $\longrightarrow$ perception*

We know a tremendous amount about sound and image. For instance, we can reliably and effectively create or synthesize realistic sounds and images. Indeed, we can study the *image $\longrightarrow$ perception* process in isolation. And, as so often happens in science, we study the process by creating a mathematical representation of it.

This starts with the mathematical representation of image and sound. Since this is a calculus course, you won't be surprised when we propose that good mathematical representations are functions:

- Sound: a function of one variable, $t$ for time.
- Image: a function of two variables: the $x$ and $y$ coordinates of an image.

A sound or an image are often represented as multiple functions, for example the left and right channel of stereo sound, or the red, green, and blue planes of a digital image. But a single channel or plane is able to represent sounds or images with considerable fidelity. For simplicity, we'll stick to that: sound(t) and image(x,y).

What about the 3rd dimension? The retina is effectively a two-dimensional sensor. The third dimension comes in through the *reality $\longrightarrow$ image* part of the overall process.

Consider the image in Figure \@ref{fig:sand-1}. It is a picture of some indentations in a small area of sand, about two inches wide in the middle of a hiking trail. The dots are individual grains of sand.

```{r sand-1, echo=FALSE, out.width="50%", fig.align="center"}
knitr::include_graphics("www/sand-furrows.png")
```

Can you see three almost parallel furrows? How about the small crater in the upper left?

You can see the individual grains of sand because they contrast sharply with their neighbors.


You can think of the surface of the sand as a function of $x$ and $y$. It's lower in some places and higher in others. But, in fact, you can't see the height of an individual point in the photograph. In the right light, you wouldn't notice the furrows at all. But the way the picture is lighted, raking sunlight from the left, the *reality $\longrightarrow image* process translates the surface into broad regions of brightness and shadow. In the light regions, the surface slants toward the sun. In the shadows, the surface slants away from the sun.

What you're mainly seeing in the photo is the ***slant*** or ***slope*** of the surface. The raking light has transformed *elevation* as a function of $x$ and $y$ into *slant* as a function of $x$ and $y$ and then encoded the slant as brightness. Ironically, it would be much less effective to present the surface height directly as an image

The moral here is that sometimes the data in a function is not in the right form for us to extract useful information. But by transforming that data to represent contrast or difference or slope, the information can be revealed.

This Block is about transforming functions to show difference and slope. Such transformation, accomplished by mathematics rather than the raking light of the sun, can take a pattern that we're presented with and turn it into another pattern that can tell us what we want to know.

## Slope of an image

```{r}
set.seed(137)
diam <- 8
f <- rfun(~ x+ y)
contour_plot(f(x, y) ~ x + y, domain(x=c(-diam, diam), y=c(-diam,diam)), contours_at = -100, fill_alpha=1, contour_color=NA, n_fill=500, fill_scale=ggplot2::scale_fill_grey())
fx <- D(f(x, y) ~ x)
contour_plot(fx(x, y) ~ x + y, domain(x=c(-diam,diam), y=c(-diam, diam)), contours_at = -100, fill_alpha=1, contour_color=NA, n_fill=500, fill_scale=ggplot2::scale_fill_grey())
```



::: {.todo}
Move this earthquake example to a point where you can do parametric plots.

:::



## Use case: Risk of earthquakes

For software designers, a ***use case*** is a description of how a person will use the software to accomplish a particular goal. It's fair to wonder what are the use cases of differentiation. It's early days in your study of calculus, so some of the use cases are beyond your reach. But knowing about first and second derivatives, argmaxes and maximums, curvature, and linear and quadratic approximations gives you access to one of the most important general patterns in scientific work: the measurement of ***precision***.

As you know, precision refers to how well you know a quantity and is often expressed using the plus-or-minus notation. To illustrate, consider the earthquake risk situation in the Cascadia Subduction Zone that includes western Oregon and Washington states. The last devastating earthquake was on January 26, 1700, a date approximated by local oral tradition and derived from written tsunami records across the Pacific Ocean in Japan.

Geologic features allow approximate dating of previous large earthquakes in the region, specifically one about 700 years previous to the latest and on about 2000 years before that. 
<!-- https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/97RG00222 section 6.3 -->

Your task as a modeler is to estimate the probability of another high-magnitude earthquake occuring in the next 100 years.

A standard model for the time interval between consecutive earthquakes is $p(y) \equiv k e^{-ky}$, where $p(t)$ is the probability of an earthquake happening exactly $y$ years after the last. We can use data on previous earthquakes to find an approximate value for $k$. Once we know $k$, we can calculate the probability that the next earthquake will occur between 2025 and 2125 as $e^{-325 k} - e^{-425 k}$ where 325 is the time interval from 1700 to 2025 and 426 is the interval between 1700 and 2125. (You'll see where the formula comes from in Block 3.)

We have information on two complete earthquake cycles, one lasting 2000 years and the other 700. A standard way to estimate the parameter $k$ is called "maximum likelihood." This amounts to assuming a value for $k$ and then, with that value, calculating the likelihood of having seen inter-quake intervals of 2000 and of 700 years. This likelihood is
$$p(2000) \times p(700) = k^2 e^{-k (2000 + 700)}$$ A graph of the likelihood as a function of $k$, using a log vertical scale gives:

```{r}
slice_plot(log(k^2 * exp(-k*2700)) ~ k, domain(k=c(0.0001, 0.003)), npts=500)
```
Making a parametric plot

```{r}
Quakes <- tibble::tibble(
  k = seq(0.0001, 0.0025, length=1000),
  LL = log(k^2 * exp(-k*2700)),
  prob = exp(-325*k) - exp(-425*k)         
)
gf_point(LL ~ k, data=Quakes)
gf_point(LL ~ prob, data = Quakes)
gf_point(prob ~ k, data = Quakes)
```




## Differencing

Our basic tool for showing difference and slope is a remarkably simple operator that takes a function as input. For a function with one input, the operator ${\cal D}()$ is defined as 
$${\cal D}(f) \equiv \frac{f(x + 0.1) - f(x)}{0.1}$$
Notice that the ${\cal D}()$ operator returns a **function**. The output is a linear combination of the input function $f()$ and a shifted version of $f()$.

For a function with two inputs, there are two versions of ${\cal D}$:
$${\cal D}_x(f) \equiv \frac{f(x+0.1, y) - f(x, y)}{0.1}$$

$${\cal D}_y(f) \equiv \frac{f(x, y+0.1) - f(x, y)}{0.1}$$


::: {.workedexample}
Suppose that $f(x)\equiv x^2$. What is the function ${\cal D}(f)$?

$$
{\cal D}(f) \equiv \frac{f(x+0.1) - f(x)}{0.1}\\ =
10\left((x+0.1)^2 - x^2\right)\\
=10 x^2 + 2x + 0.1 - 10x^2\\ = 2 x + 0.1$$
:::

::: {.workedexample}
Suppose that $f(x) \equiv 2 x + y$. Find the function ${\cal D_x(f)}$.

The linear combination will be 
$$\frac{1}{0.1}\left(\left[2 (x + 0.1) + y\right] - \left[2 x + y\right]\right) =\\ \\
20 x + 2 + 10y - \left[20 x - 10y\right] = 2$$
:::

```{r}
f <- makeFun(2 *x * y ~ x + y)
D(f(x, y) ~ x)
```

Figure \@ref(fig:three-rates) shows the fitted rate of change and average rate of change in the interval $10 \leq t \leq 125$ as well as the instantaneous rate of change at $t_0 = 50$. They are different from one another.

```{r three-rates,  echo=FALSE, warning=FALSE, fig.cap="Comparing the average rate of change (red) over the interval $10 \\leq t \\leq 125$, the instantaneous rate of change at $x_0 =25$, and the fitted linear model (orange). The exponential model is shown in gray."}
set.seed(101)
Stans_data <- CoolingWater %>% sample_n(20)
gf_point(temp ~ time, data = Stans_data, color="orange") %>%
  gf_lm(color="orange") %>%
  slice_plot(water(time) ~ time, domain(time=c(0, 200)), color="gray", size=2, alpha = 0.5) %>%
  slice_plot(tangent(time) ~ time, color = "sienna", 
             domain(time=c(0,80)))  %>%
   gf_point(y ~ t, data = Pts, color="orange3", alpha=0.25, size=4, inherit=FALSE) %>%
  gf_line(y ~ t, data = Pts, color="orange3") %>%
  gf_point(25 ~ t, data = Pts, color="dodgerblue", alpha=0.25, size=4) %>%
  gf_point(y ~ t, data=Pt,
           color="sienna", alpha = 0.25, size=4) %>%
  gf_point(25 ~ t, data = Pt, color="tomato", alpha=0.25, size=4) %>%
  gf_segment(25 + y ~ t+t, data=Pt, color="tomato", linetype=2) %>%
  gf_segment(25 + y ~ t+t, data=Pts, color="dodgerblue", linetype=2) %>%
  gf_lims(y=c(25, NA)) %>%
  I()
```



Symbolic differentiation is a **mechanical** process. Some people are good at working like a machine. And some people aren't. It helps to be very organized and to have a really good short-term memory. For decades, computers have been programmed to calculate symbolic differentiation. What's required is the systematic and reliable application of a small set of rules. Computer's excel at this.

A set of about a dozen rules will handle all the formulas we'll encounter in this ook or that are commonly used for modeling. In this chapter, we're going to derive only a handful of the rules---those related to the straight-line function. Then we will introduce, without motivation, the rules for the pattern-book and basic modeling functions. In Chapter \@ref(evanescent-h) we'll motivate the rules for the pattern-book functions. In Chapters XXX we'll develop the three rules that correspond to the three forms of creating new functions out of old: linear combinations (the ***sum*** rule), function multiplication (the ***product rule***) and function composition (the ***chain rule***).

Recall that for a function $f(x)$, the derivative $\partial_x f(x)$ corresponds to the ***instantaneous slope*** of $f(x)$. There is a small set of functions for which the instantaneous slope is the same for all $x$. These are the straight-line function $g(x) \equiv a x + b$ and the constant function $h(x) \equiv b$. 

The rule for the straight-line function is
$$\partial_x \left[a x + b\right] = a$$
The rule for the constant function is 
$$ \partial_x \left[b\right] =0$$ There are two special cases of the straight-line function which are really just a restatement of the above two rules but which are worth emphasizing:
$$\partial_x \left[a x\right] = a \ \ \ \text{and}\ \ \ 
\partial_x \left[x\right] = 1$$

Note that the right-hand side of all these differentiation rules based on the straight-line function **does not involve $x$**. This amounts to saying that the derivative function for the straight-line function does not vary with $x$. Equivalently, you can say that the derivative function for the straight-line function is a constant function.

The ***with-respect-to input***---the input whose name is given as a subscript to $\partial$  is the key to these patterns. Whenever $\partial_x$ is applied to any formula that **does not contain** $x$, the derivative is zero. You see that in $\partial_x\left[ b \right] = 0$. Similarly $\partial_x \left[ b \right]=0$ and even very complicated things not involving $x$ have derivatives that are zero, for instance: 
$$\partial_x \left[\sin\left(2\pi \sqrt{\cos(y^2 + 5 e^z)}\right)\right] = 0$$
For notation, let's refer to any formula that does not involve the with-respect-to input as $$\text{Anything without x dependence:}\ \ \require{cancel}\xcancel{\strut\mathbf x}$$

* No $x$ rule: $\ \partial_x \left[a\right] = 0\ \ \text{where}\ \ a = \xcancel{\strut\mathbf x}$

* Simple x rule: $\partial_x \left[x\right] = 1$

* Scaling $x$ rule: $\partial_x \left[ a x\right] = a\ \ \text{where}\ \ a =  \xcancel{\strut\mathbf x}$


```{exercise, name="basic rules"}
A few drill problems on the above three rules.
```

```{exercise, name="other than x"}
A few drill problems on the above three rules but with other variables
```

## Symbolic derivatives of modeling functions

For a human training to be a symbolic differentiator, it's best simply to memorize the patterns for derivatives of the other pattern-book functions. They are:

$f(x)$      | $\partial_x f(x)$
------------|------------------
$e^x$       | $e^x$
$\sin(x)$   | $\cos(x)$
$\ln(x)$    | $1/x$
$x^p$       | $p\, x^{p-1}$
sigmoid, that is, pnorm(x)    | hump, that is, dnorm(x)
hump, that is dnorm(x)        | $-x \text{dnorm}(x)$

Note that some of the pattern-book functions have derivatives which are simply pattern-book functions

Name $f(x)$|           $f(x)$  | $\partial f(x)$ | name $\partial_x f(x)$
:---------:|:----------------:|:----------------:|:------:
exponential| $e^x$              | $e^x$          | exponential
logarithm  | $\ln(x)$           | $1/x$          | power-law
sigmoid    | $\text{pnorm(x)}$ |$\text{dnorm(x)}$| hump


Another couple of pattern-book functions involve division or division by $x$ and multiplication by a scalar. (The second one, $\partial_x \text{hump}(x)$, was not in the original short list)

Name $f(x)$|           $f(x)$  | $\partial f(x)$ | name $\partial_x f(x)$
:---------:|:----------------:|:----------------:|:------:
power-law  |    $x^p$  | $\frac{p}{x} x^p = px^{p-1}$  | power-law
hump       | $\text{dnorm(x)}$  | $- x\, \text{dnorm(x)}$| "hiccough"

Traditionally, the derivative of the sinusoid is presented as a pair of kissing cousins:

Name $f(x)$|           $f(x)$  | $\partial f(x)$ | name $\partial_x f(x)$
:---------:|:----------------:|:----------------:|:------:
sinusoid   | $\sin(x)$  | $\cos(x) = \sin(x+\pi/2)$ | sinusoid
sinusoid   | $\cos(x)$  | $-\sin(x) = \cos(x + \pi/2) =  \sin(x+\pi)$ | sinusoid

::: {.takenote}
Memorizing the derivatives of the short list of pattern-book functions will make it much easier to follow the rest of the section. Do that now.
:::

Recall that the ***basic modeling functions*** are just the pattern-book functions which have been tailored to match the type of input. The tailoring can be a simple a scaling function, such as $a\, x$ or $k\, x$  or $\frac{2\pi}{P} x$. As well, the tailoring can involve both a scaling and a ***shift***, for instance $a x + b$ or $a (x-x_0)$
Let's give a name to the tailoring function and recognize the several varients:

Variations of tailoring$(x)$ |    | $\partial_x \text{tailoring}(x)$
-----------------------|---------|
$ax$  | scaling | $a$
$kx$  | scaling | $k$
$\frac{2\pi}{P} x$ | scaling | $\frac{2\pi}{P}$
$ax + b$ | scaling and shift | $a$
$a(x-x_0)$ shift and scale   | $a$
$\frac{x - \text{mn}}{\text{sd}}$ | shift and scale | $\frac{1}{\text{sd}}$}


Every basic modeling function is a composition of the corresponding pattern-book function with the tailoring function:

pattern-book    | tailored   | typically written
---------|-----------|------------------
`exp(x)` | `exp(tailoring(x))` | $e^{kx}$
`sin(x)` | `sin(tailoring(x))` | $\sin\left(\frac{2\pi}{P} x\right)$
`pnorm(x)` | `pnorm(tailoring(x))` | `pnorm(x, mn, sd)`
`dnorm(x)` | `dnorm(tailoring(x))` | `dnorm(x, mn, sd)`

To differentiate the basic modeling functions, follow exactly the same rules as for the pattern-book functions, but instead of $x$, write the tailored form of $x$. And then, multiply the whole thing by the derivative with respect to $x$ of the tailoring function.

$$\partial_x \left[ f(\text{tailoring}(x))\right] = [\partial_x]\left(\strut\text{tailoring}(x)\right) \times 
\partial_x \text{tailoring}(x)$$

Some examples:

$f(x)$    |   $\partial_x f(x)$
----------|----------------------
$e^{kx}$  | $k \times e^{kx}$
$\sin\left(\frac{2\pi}{P} x\right)$ | $\frac{2 \pi}{P} \times \cos\left(\frac{2\pi}{P} x\right)$
$\text{pnorm}(x, \text{mn}, \text{sd})$ | $\frac{1}{\text{sd}} \times \text{dnorm}(x, \text{mn}, \text{sd})$


<!--

MOVE TO LATER ON



Come to think of it ... this is where computers shine. So let's start with how to use the computer for differentiation. Then we can talk about the rules that the computer is using.

I'm going to assume that you are comfortable with using `makeFun()` in the R/mosaic software. 

> **To compute derivatives, use `D()` instead of `makeFun()`. Everything else is exactly the same. 

For example, this is a typical use of `makeFun()`:
```{r}
f <- makeFun(a*x^2 + b*x + c ~ x, a=2, b=4, c=-3)
```
We're calling the newly created function `f()` and we can use it in the ordinary ways:
```{r}
f(2)    #evaluate it at a specific input
g <- makeFun(f(x)*sin(x) ~ x) # Build a new function with it
slice_plot(f(x) ~ x, domain(x=c(-3,3))) # Plot it
findZeros(f(x) ~ x)  # Find the zero crossings
```
Same with `D()`:

```{r}
dx_f <- D(a*x^2 + b*x + c ~ x, a=2, b=4, c=-3)
```
We're calling the derivative of `f()` by the name `dx_f()`. We could, as usual, have used any name, but it's nice to have a convention that makes it easier to keep track of things.
```{r}
dx_f(2)    #evaluate it at a specific input
g <- makeFun(dx_f(x)*sin(x) ~ x) # Build a new function with it
slice_plot(dx_f(x) ~ x, domain(x=c(-3,3))) # Plot it
findZeros(dx_f(x) ~ x)  # Find the zero crossings
```
::: {.tip}
As a matter of good computing style ... 

Since we had already defined `f()`, in computing its derivative a good practice is to use `f()` itself rather than copying over the tilde formula for use in `D()`. The function created by `D()` will inherit all the same default parameters used in `f()`.

```{r}
D(f(x) ~ x)
```

Notice: No $h$!

:::

## Symbolic differentiation rules

Each rule consists of a pair of patterns that can be organized into two columns of a table. If the formula matches a pattern on the left, then re-write it into the pattern on the right.

First, we need a way to denote the patterns involved in differentiation and to relate them to the way we write formulas. We already have most of what's needed in the patterns of the basic modeling functions and in the patterns of linear combination, function composition, and function multiplication. 

Let's start with the straight-line function and it's derivative. Here's the first line of our pattern table, which we'll write using $x$ as the ***with-respect-to input***.

pattern name  | Original       |  Derivative w.r.t. $x$
--------------|----------------|-------------
straight-line | $a x + b$      | $a$

There are many forms that are equivalent to the straight-line formula, for instance $3 x + 2$ or $4(x-6)$ or $a(x-x_0)$, $b + x a$, $(4 + 1)x - 7$. Whenever you encounter such a form, re-arrange it into $a x + b$.


* **Example 1**: Re-arrange $7-(4+1)x$ as $a x + b$.    
    
    Use arithmetic reduction to simplify to $7-5x$ then put it in the standard order, getting $a=-5$ and $b=7$. The derivative is therefore $-5$.

* **Example 2**: Re-arrange $3(x-2)$ as $a x + b$.

    Distribute the multiplication by 3 to get $3x - 6$. This has $a=3$ and $b=-6$. There derivative is therefore $3$.

* **Example 3**: Re-arrange $m (x - x_0)$ as $a x + b$.

    Distribute the multiplication by $m$ to get $mx - m x_0$. Recognizing that $m x_0$ does not involve the input $x$, get $a=m$ and $b= -m x_0$. The derivative is therefore $p$.

* **Example 4**: Re-arrange $3 x + 4 - 2 x +1$ as $ax + b$. 

    Bring together the terms involving $x$ and use arithmetic to simplify. Similarly, bring together the terms not involving $x$ and use arithmetic to simplify. 
$$3 x + 4 - 2x + 1 = (3 x - 2 x) + (4 + 1) =  1 x + 5$$ so $a=1$ and $b=5$. The derivative is therefore $1$.

* **Example 5**: Re-arrange $-x$ as $ax + b$. 

    We have to be inventive and recognize that $-x$ has the format $-1 x + 0$. So $a=-1$ and $b=0$. There derivative is therefore $-1$. 

* **Example 6**: Re-arrange $2 x_0$ as $ax + b$. Being inventive in the same way to recognize $a = 0$ and $b=2 x_0$. The derivative is therefore 0.

* **Example 7**: Re-arrange $\frac{2 \pi}{P} (x-x_0)$ as $a x + b$. 

What's potentially confusing here is complicated looking $\frac{2 \pi}{P}$. You can see that this doesn't involve the input $x$ in any way. For the moment, let's replace $\frac{2 \pi}{P}$ with something simpler: $c$. Then the formula is $c(x-x_0)$ which is in the form of Example 3. The derivative of that is simply $c$. And $c$ really stands for $\frac{2 \pi}{P}$ in the original, so the derivative of the original is $\frac{2 \pi}{P}$

Counter Example: Re-arrange $c x^2$ into $a x + b$. There's no way! $c x^2$ is a different kind of pattern. There's not yet a row in the pattern table to tell us how to handle $x^2$.

The straight-line rule is implemented as `axb_rule()`. You give `axb_rule()` the formula (in R notation). The result is the derivative according to that rule. Importantly, if the rule does not apply, then the result is `FALSE`:

```{r eval=FALSE}
sum_rule((a+b)*x + c*x^2)
```



## Re-writing the basic modeling functions

We're going to extend the pattern table to include the basic modeling functions. Recall that every basic modeling function is a composition of a pattern-book function with an $ax + b$ scaling function.

pattern name  | Original             |  Derivative w.r.t. $x$
--------------|----------------------|-------------
straight-line | $a x + b$            | $a$
exponential   | $\exp(a x + b)$      | $a \exp(ax + b)$
sinusoid      | $\sin(a x + b)$      | $a \cos(ax + b)$
power-law     | $\text{pow}(ax+b,p)$ | $a\,p\, \text{pow}(a x + b, p\!\!-\!\!1)$
logarithm     | $\log(a x + b)$      | $a\, \text{pow}(ax + b, -1)$
sigmoid       | $\text{pnorm}(a x + b, mn, sd)$ | $a\, \text{dnorm}(a x + b, mn, sd)$

::: {.why}

I wrote most of the basic modeling functions in their traditional mathematical notation except for $e^x$ and $x^p$. In the table, I wrote those as $\exp(x)$ and $\text{power}(x, p)$. This is similar in form to $\text{pnorm}(x, mn, sd)$ and $\text{dnorm}(x, mn, sd)$.

The reason to use the $\exp()$ and $\text{pow}()$ notation is to make it easier to see the commonality of the patterns. Each of the rows of the table, except the first, has the form $f(a x + b)$ or $f(a x + b, p)$ or $f(a x + b, mn, sd)$. Only the first argument to $f()$ involves an $x$-expression. If there are additional arguments, they never involve $x$. In other words, $p$, $mn$, and $sd$ are ***constants***.

If you prefer the traditional notation, the relevant rows of the pattern table are:

pattern name  | Original             |  Derivative w.r.t. $x$
--------------|----------------------|-------------
exponential   | $e^{a x + b}$      | $a e^{ax + b}$
power-law     | $(ax+b)^p$ | $p\, a\, (ax + b)^{p-1}$

:::


In Chapters XX, XXX, and XXXX we'll give the re-writing rules for linear combination of functions, function composition, and function multiplication. 


```{exercise}
Drill on these forms
```

```{exercise}
Move this to the chapter on assembling functions. Drill on spotting linear combinations, f(g(x)) is composition, $\sin(x^2)$ is composition, ...
```

## Pattern recognition

 To demonstrate this, and to show why computers can be "taught" to do symbolic differentiation, we'll have you practice with an R function that identifies the two kinds of patterns in our pattern table.

```{r echo=FALSE}

```

The first question when symbolically differentiating a function is whether the formula for the function is of the form $ax + b$. Our function for this is `axb_rule()`. The output of this function will be `FALSE` if the formula is not of that form. But if there is a match, `axb_rule()` will give you the formula for the derivative.

```{r eval=FALSE}
axb_rule(a*x + b)
axb_rule(2*x)
axb_rule(x)
axb_rule(-x)
axb_rule(1/x)
axb_rule(x^2)
```
The function `bmf_rule()` identifies if the formula is one of the basic modeling functions.

```{r eval=FALSE}
bmf_rule(a*x + b)
bmf_rule(sin(x))
bmf_rule(sin(3*x + 2))
bmf_rule(sqrt(x))
bmf_rule(sqrt(a*x + b))
bmf_rule(sqrt(x^2))
bmf_rule(sin(sqrt(x)))
bmf_rule(x^3)
bmf_rule((a*x + b)^4)
bmf_rule((a *x^2)^ 4)
bmf_rule(pnorm(x, mn, sd))
```
```{exercise}
```
Explain why the result is zero and not simply `a`.
```{r eval=FALSE}
bmf_rule(ax + b)
```

-->


# h and derivatives {#h-and-derivatives}

We already established, by numerical experiment, the result of differentiating the pattern-books functions. To summarize,

```{r child="D-naked-functions.Rmd"}
```

A mathematician might prefer to replace the word "established" in the first sentence of this table with a weaker word "motivated" or "proposed." This is entirely fair. Indeed, let's do another experiment that will cause us to wonder just how solid are the conclusions presented in the table above.

Recall that we can easily define the slope function for any $f(x)$, for example the slope function of $\sin(x)$:
```{r}
Dsin <- makeFun((sin(x+h) - sin(x))/h  ~ x)
```
In justifying the entry for $\sin()$ in the table, we plotted `Dsin()` using small `h`, for instance, `h=0.000001`. It would be comforting to continue the experiment with even smaller `h`. Doing so, we discover a problem.


```{r Dsin-small-h, echo=FALSE, fig.cap="Graphing the computer's slope function of $\\sin(x)$ for small enough $h$ produces a result inconsistent with the table for the pattern-book functions. Instead of producing a $\\cos(x)$, we get a ragged function."}
slice_plot(Dsin(t, h=0.000000000000001) ~ t, domain(t=c(-5,2*pi)))
```
Things get even worse for smaller $h$ still, as you can confirm for yourself using a computing sandbox.

It turns out that the reason for this behavior is the way in which computer arithmetic has been engineered. To demonstrate the non-mathematical behavior of computer arithmetic, consider what happens when we add and subtract using 1 and a number that is small compared to 1.

```{r}
options(digits=20)
0.000000000000000000000001
1 + 0.000000000000000000000001
1 - 0.000000000000000000000001
1 + 0.000000000000000000000001 - 1
```

Using technical computing successfully at a professional level requires some understanding of the ways in which computer arithmetic differs from mathematical arithmetic, And the R/mosaic `D()` operator has been constructed with these considerations in mind. But our purpose here is not to push the computer beyond it's arithmetic limits but to demonstrate that the differentiation table for the pattern-book functions is correct.

## The $h$ framework

In the end, all of the work we're going to do with $h$ will have a simple result: confirming the facts presented in the differentiation table. In a court of law, that confirmation could be established by appeal to established authority. For instance, look in any calculus textbook and you'll see the same facts as in the table. (And if you find an exception, you can be sure it's a typographical error!)

For most people, mathematical proof is not much different from appealing to established authority. Not everyone is skilled at following the deductive steps of a proof and almost everyone has been tricked into accepting a step that is not logically valid. And everyone makes mistakes. 

For the benefit of such people, rather than proving the facts in the differentiation table, we're going to reconstruct, hopefully in a fun way, the  framework developed by the mathematics community over roughly two centuries that enabled mathematicians to satisfy themselves that the methods used by the pioneers could be justified beyond doubt.

The basic problem, which everyone always agreed on, is that it's not proper to set $h$ to zero. The dispute is about how to handle $h$ in such a way that it can be held non-zero and yet give results where $h$ has evaporated as if it were never there. 

To illustrate how this can be done, consider the algebra of the slope function for $g(x) \equiv x$:

$${\cal D}_x g(x) \equiv \frac{g(x+h) - g(x)}{h} = \frac{(x+h) - x}{h} = \frac{h}{h} = 1$$ 
The above steps are completely justified for any $h$ whatsoever, so long as $h \neq 0$. So we have a formula ${\cal D}_x\, x = 1$ which will be correct no matter whether $h$ is big or small.



In contrast, consider the slope function of $f(x) \equiv x^2$:
$${\cal D}_x f(x) \equiv \frac{f(x+h) - f(x)}{h} = \frac{(x+h)^2 -x^2}{h} = \\
\ \\
= \frac{x^2 + 2hx + h^2 - x^2}{h} = \frac{2 h x + h^2}{h} = 2 x + h$$
Again, we have a formula which is correct for any $h$ whatsoever, so long as $h$ is not zero. Unfortunately, $h$ is still present in the formula. In differentiating $x^2$ we want to make $h$ go away.


I like to think of $h$ as a kind of *tire iron*, a small tool used to stretch the bead of a bicycle tire in order to pull it over the wheel rim. 

```{r echo=FALSE, out.width="30%", fig.cap="A tire iron in use", fig.align="center"}
knitr::include_graphics("www/tire-iron.png")
```

Once the tire iron has done its job, its removed and you would never know that it was ever there (except that the tire is now successfully mounted on the wheel).

But this is calculus, not bicycle mechanics. How do we know that removing the tire iron isn't damaging the mathematical wheel? 

Still in the spirit of having fun, let's try a more serious metaphor... imagining $h$ is actually a central character in a calculus play. The character $h$ is in the middle of the story but *never appears in the play*, like the missing character Godot in the famous play *[Waiting for Godot](https://en.wikipedia.org/wiki/Waiting_for_Godot#Godot)*. 

We said that $h$ in the slope function ${\cal D}_x x^2 = 2 x + h$, so long as $h$ is small, plays both a central role and has hardly any effect. An economizing director re-writes the play to take $h$ out of it, setting $h=0$ in the formula $2 x + h$: a non-speaking, offstage role.

We've already seen using legitimate algebra that $${\cal D}_x g(x) = 2 x + h$$ Re-writing by replacing $h$ with 0 streamlines the play, turning ${\cal D}_x x^2 = 2x +h$ from a dialog involving both $x$ and $h$ into a monologue with $h$ absent: $$\partial_x x^2 = 2 x$$ Simple.

And yet ... the director gets a letter from the Bit Players Union. 

> *We observe that you have eliminated the role of $h$ in the final production version of $\partial_x g(x)$. This is a violation of Union regulations. Recall that the basis for $\partial_x g(x)$ is the slope function ${\cal D}_x g(x)$. The slope function is defined as a ratio: $${\cal D}_x g(x) \equiv \frac{g(x+h) - g(x)}{h}$$ Eliminating $h$ entirely by replacing her with zero is a **division by zero error** forbidden by Article 3.16§B¶2 of the Unified Laws of Arithmetic. We ask that you comply with this Article by re-instating the role of $h$ in all evaluations of ${\cal D} g(x)$.*

Reading this, the director calls her lawyer. Is there a loophole for removing $h$ without breaking the mathematical prohibition on dividing by zero? 

The loophole involves a bit of legalistic cover, something like a corporation. As you may know, a corporation is a legal structure that makes if feasible for people to invest without being subject to unlimited liability. You bought stock in a company that later accidentally caused a catastrophe? The company will go out of business and your stock will be worthless. But you are legally obliged to fix the damage: you liability is limited. Such companies identify themselves as such with the legal suffix "Inc." and they are beholden to the state in certain ways, such as the requirement to pay taxes on profits.

In calculus, the equivalent of "Inc" is $\lim_{h \rightarrow 0}$. By prepending this to a calculation, you are allowed to carry out arithmetic operations such as dividing by $h$ without concern about liability for dividing by zero. You can perform any algebraic operations so long as they are legitimate when $h \neq 0$. For instance, it is taken as entirely correct to say:

$$\lim_{h\rightarrow 0}\frac{h}{h} = 1$$
Another privilege for the users of $\lim_{h\rightarrow 0}$ is that, at the end of the algebraic derivation, they are entitled to replace $h$ with zero so long as no divide by zero is required. The endpoint of the above is 1, where $h$ doesn't even appear. But consider
$$\lim_{h \rightarrow 0} \frac{(x + h)^2 - x^2}{h} = \\
\ \\
= \lim{h \rightarrow 0} 2 x + h$$

In the last step, an $h$ appears. But we are entitled to take the result of the derivation, $2 x + h$, and replace the $h$ by zero. Doing so doesn't entail any illegitimate operation such as dividing by zero.

At it's most basic, you're entitled to state $$\lim_{h\rightarrow 0} h = 0$$ which, to be honest, looks like nothing more than common sense.

## Approximations to pattern-book functions

Let's take a close look at two of our pattern-book functions: $e^x$ and $\sin(x)$. By "close" I mean very near to $x=0$. You already know that $e^0 = 1$ and $\sin(0) = 0$. But consider "very small" $x$, in the spirit of $\lim_{x\rightarrow 0}$. 

The fundamental approximations are these:

$$e^x \approx 1 + x\\
\ \\
\sin(x) \approx x\\
\ \\
\cos(x) \approx 1 - x^2$$

Figure \@ref(fig:small-x-pattern-book} shows the naked functions along with the approximations. We're interested in "small" $x$. The left panel shows the functions for $-1 \leq x \leq 1$ and the right panel zooms in for small $x$, taking "small" in an arbitrary but everyday sense of, say, less than 0.01.

```{r small-x-pattern-book, echo=FALSE, fig.show="hold", out.width="50%", fig.cap="Comparing the pattern-book functions exp(x), sin(x), and cos(x) to their simple approximations for small x. The functions are drawn as a broad gray line; the approximations are a thin blue line."}
options(digits=5)
slice_plot(exp(x) ~ x, domain(x=c(-1, 1)), size=3, alpha=0.25) %>%
  slice_plot(1 + x ~ x, color="dodgerblue") %>%
  gf_labs(title = "Exponential")
slice_plot(exp(x) ~ x, domain(x=c(-0.01, 0.01)), size=3, alpha = 0.25) %>%
  slice_plot(1+x ~ x, color = "dodgerblue")

slice_plot(sin(x) ~ x, domain(x=c(-1, 1)), size=3, alpha = 0.25) %>%
  slice_plot(x ~ x, color="dodgerblue") %>%
  gf_labs(title = "Sin")
slice_plot(sin(x) ~ x, domain(x=c(-0.01, 0.01)), size=3, alpha = 0.25) %>%
  slice_plot(x ~ x, color = "dodgerblue")
  
slice_plot(cos(x) ~ x, domain(x=c(-1, 1)), size=3, alpha=0.25) %>%
  slice_plot(1 - x^2 ~ x) %>%
  gf_labs(title = "Cos")
slice_plot(cos(x) ~ x, domain(x=c(-0.01, 0.01)), size=2, alpha = 0.25) %>%
  slice_plot(1 - x^2 ~ x, color = "dodgerblue")
```

Near $x=0$, the graph of the approximation is dead on center of the corresponding pattern-book function.

The approximations will not break down for very small $x$ because none of the approximations involve dividing by $x$. And remember that the approximations are only good when $x$ is small!

Consider the result of differentiating $e^x$. The slope function is

$${\cal D}_x e^x \equiv \frac{e^{x+h} - e^x}{h} = e^x \left[\frac{e^h - 1}{h}\right]$$
Let's examine $\frac{e^h - 1}{h}$ for very small $h$. This is where we will use the approximation for $e^x$, but we'll write it with $h$ as the argument: $e^h \approx 1 + h$. Plugging this in to the bracketed quantity, we have $$\lim_{h \rightarrow 0}\frac{e^h - 1}{h} = \lim_{h\rightarrow 0}\frac{1 + h - 1}{h} = 1$$ Overall, this means $$\partial_x e^x = \lim_{h\rightarrow 0}{\cal D}_x e^x = e^x$$

Now to demonstrate that $\partial_x \sin(x) = \cos(x)$ a fact that is often used in calculus. Only calculus teachers need to know how to perform this demonstration, so the following is just FYI:.

The demonstration builds on a formula for the sine of the sum of two quantities:

$$\sin(x + h) = \sin(x)\cos(h) + \cos(x)\sin(h)$$

With this, we can simplify the slope function of $\sin$:
$${\cal D}_x \sin(x) \equiv \frac{\sin(x+h) - \sin(x)}{h} =\\
\ \\
= \frac{1}{h} \left[\strut\sin(x)\cos(h) + \cos(x)\sin(h) - \sin(x) \right]\\
\ \\
= \frac{1}{h} \left[\strut \sin(x) \left[\cos(h) -1\right] + \cos(x)\sin(h)\right]$$
Plugging in the approximations $\sin(h) = h$ and $\cos(h) = 1 - h^2$ we get
$${\cal D}_x \sin(x) = \frac{1}{h} \left[\strut\sin(x)(- h^2)  +\cos(x)h\right] = \\
\ \\
= \cos(x) - h \sin(x)$$
With the division by $h$ safely removed, we cn now apply the limit:
$$\partial_x \sin(x) = \lim_{h\rightarrow 0} \left[\strut \cos(x) - h \sin(x)\right] = \cos(x) - \sin(x) \left[\lim_{h\rightarrow 0}h\right] = \cos(x)$$

::: {.takenote}
We write the differentiation operator (with respect to input $x$) as $\partial_x$. We wrote the slope-function operator (again, with respect to $x$) as ${\cal D}_x$. 

The slope-function operator ${\cal D}_x$ is only a stepping stone on the path toward the real destination: differentiation. We're defining differentiation as a limit:

$$\partial_x\, f(x) \equiv \lim_{h\rightarrow 0} {\cal D}_x\, f(x)$$
In practice, however, we perform differentiation whenever possible not with the limit definition, but with the *consequences* of the limit definition, such as $\partial_x e^x = e^x$ or $\partial_x \sin(x) = \cos(x)$. Sometimes these consequences are called the ***rules of differentiation***. These rules are important particularly for carrying out differentiation using paper and pencil, but if a computer is available, the rules have been mastered for you by software. Since such software is widely available, we're going to step away from them for a bit. The next chapters present some of the uses for differentiation and contexts in which differentiation is used in constructing new modeling functions. 
:::

```{exercise, name="low-low-rates"}
```
"Exercises/Diff/low-low-rates.Rmd")`

```{exercise, name="singularity-numerics"}
```
"Exercises/Diff/singularity-numerics.Rmd")`

```{exercise, name="NaN-numerics"}
```
"Exercises/Diff/NaN-numerics.Rmd")`

::: {.todo}
In the limits section or an exercise, ask about $x^0$? Is that the same as 1? What about when $x=0$. Zero raised to any positive power is 0. But any positive number raised to 0 is 1. So two ways of thinking about it give different answers.

The computer handles the question with ease:

```{r}
0^0
```
But computer results don't always agree with mathematical results.

In this case, mathematicians looking at the question through the lens of limits, agree with the computer when looking at the function $g(x) \equiv x^0$. But they disagree if the function is $h(x) \equiv 0^x$ or $f(x) \equiv x^x$.  These three functions are not the same when it comes to limits.

Remember that $\lim_{x\rightarrow 0}$ is not about replacing $x$ with zero, it is about a process of examining the result for non-zero $x$ as $x$ approaches zero. In "zero raised to any positive power" we're talking about $0^{\lim_{x\rightarrow 0}}$, which is not the same as $\lim_{x\rightarrow 0} \left[x^x\right]$.  Similarly, $\left[\lim_{x\rightarrow 0}\right]^0$ is different from
:::

# Derivatives of the basic modeling functions

::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-7a-1", "Know the derivatives of the Naked Core Functions")
```
:::

- $\partial_x e^x = e^x$
- $\partial_x x^p = p x^{p-1}$ for $p \neq 0$
- $\partial_x \sin(x) = \cos(x)$ and $\partial_x \cos(x) = \sin(x)$
- $\partial_x \ln(x) = 1/x$
- $\partial_x \text{sigmoid}(x) = \text{hump}(x)$


::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-7a-2", "Apply the basic derivative shortcuts to find the derivative of a function.")
```
:::



We're going to leave $\partial_x \mbox{hump}(x)$ alone for now until we have a more complete understanding of derivatives. But other than that, notice that the derivative of every one of these mathematical functions---exponential, power-law, sinusoid---is also in the set of the basic functions.

::: {.workedexample latex-data=""}
How can we know that these formulas are right? Let's start by comparing them to the functions computed from the ${\cal D}_x$ formula when $h$ is very small.

Consider $\partial_x e^x$, which is claimed to be $e^x$. Figure \@ref{fig:exp-h} plots $e^x$ (in green) as well as a function ARC$(x, h)$ (**A**verage **R**ate of **C**hange) that takes both $x$ and $h$ as inputs: $$\mbox{ARC}(x, h) \equiv {\cal D}_x e^x = \frac{e^{x+h} - e^x}{h} = e^x \frac{e^h - 1}{h}$$

```{r exp-h, echo=FALSE}
colors <- heat.colors(5)
ARC <- makeFun(exp(x)*(exp(h)-1)/h ~ x + h)
factor <- makeFun((exp(h)-1)/h ~ h)
P1 <- slice_plot(exp(x) ~ x, domain(x=c(-2,2)), 
           size=3, color="green", alpha=0.4) %>%
  slice_plot(ARC(x, h=1) ~ x, color=colors[1],
             label_text="h=1", label_x=1) %>%
  slice_plot(ARC(x, h=0.5) ~ x, color=colors[2],
             label_text="h=0.5", label_x=1) %>%
  slice_plot(ARC(x, h=0.1) ~ x, color=colors[3],
             label_text="h=0.1", label_x=0.9) %>%
  slice_plot(ARC(x, h=0.01) ~ x, color=colors[4],
             label_text="h=0.01", label_x=0.8) %>%
  slice_plot(ARC(x, h=0.001) ~ x, color=colors[5],
             label_text="h=0.001", label_x=7)
P1
```

You can see at a glance that $\mbox{ARC}(x, h)$ gets closer and closer to the claimed derivative, $e^x$, as $h$ gets smaller. 

A better view of things comes from recognizing that $$\mbox{ARC}(x, h) = e^x \left[\frac{e^h - 1}{h}\right]$$
Let's look at the quantity in brackets as $h$ gets small. If that goes to 1 for small $h$, then $\mbox{ARC}(x, h)$ goes to $e^x$.

$h$ | $\frac{e^h - 1}{h}$ 
:----|-------------------
1   | `r format(factor(1), digits=15)`
0.1   | `r format(factor(0.1), digits=15)`
0.01   | `r format(factor(0.01), digits=15)`
0.001   | `r format(factor(0.001), digits=15)`
0.0001   | `r format(factor(0.0001), digits=15)`
0.00001   | `r format(factor(0.00001), digits=15)`
0.000001   | `r format(factor(0.000001), digits=15)`
0.0000001   | `r format(factor(0.0000001), digits=15)`
0.00000001   | `r format(factor(0.00000001), digits=15)`
0.000000001   | `r format(factor(0.000000001), digits=15)`
:::

```{exercise, name="finch-trim-kayak"}
```
"Exercises/Diff/finch-trim-kayak.Rmd")`

## Derivatives of linear combinations

## Derivatives of composed functions

Just state the rule

$$\partial_ f(g(x)) = \partial_x f(g(x)) \partial_x g(x)$$
and some examples. Then return to the derivation in an example in the Approximations chapter. 




## Derivatives of the basic modeling function


    
::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-8c", "Master derivatives of basic modeling functions")
```
:::

Just an application of the principles behind composed functions.

* $\partial_x e^{k x} = k\,e^{k x}$
* $\partial_x x^0 = 0$
* $\partial_x x^p = p\, x^{p-1}$. unless $p=0$. 
* $\partial_x \sin(\frac{2\pi}{P} x) = \frac{2\pi}{P}\cos(\frac{2\pi}{P}x)$
* $\partial_x \cos(\frac{2\pi}{P} x) = \frac{2\pi}{P}\sin(\frac{2\pi}{P}x)$
* $\partial_x \mbox{sigmoid}(x, \text{center}, \text{width}) = \frac{1}{\text{center}}\mbox{hump}(x, \text{center}, \text{width})$


NEXT STEP. The linear expansion ...

$f(x + h) \approx f(x) + h \partial_x f(x)$










YOU GOT HERE.

## The $\Delta$ operator

CHANGE THIS TO ${\cal D}$ and introduce it once you have gotten rid of $h$.

Up until now, we used only functions that take numerical quantities as inputs and produce a numerical quantity as an output. It's time now to broaden our perspective a bit.

Imagine a function named $\Delta()$ defined like this:
$$\Delta(f) \equiv \frac{f(x+h) - f(x)}{h}$$
$\Delta()$ takes as input a **function**. You might have guessed this because the definition uses the name $f$ for the input to $\Delta()$, but the name of an input does not matter so long as it is used consistently in the body of the function. Looking at the body, you can that the name $f$ is being used (twice!) in the position of a function's name.

For instance:

- $\Delta(\sin) = \frac{\sin(x+h) - \sin(x)}{h}$
- $\Delta(x^2) = \frac{(x+h)^2 - x^2}{h}$
- $\Delta(a + b x) = \frac{a + b(x + h) - \left(a + b x\right)}{h}$

::: {.why latex-data=""}
Why did you use $=$ in the above statements rather than the $\equiv$ ?

$\equiv$ means "is defined as" or "is the name of." When we wrote $$\Delta(f) \equiv \frac{f(x+h) - f(x)}{h}$$ we were giving a name to a function. But writing $\Delta(a + b x)$ means to apply the already defined function $\Delta$ to an input, that input being the straight-line function $a + b x$. Rather than *defining* what is $\Delta(a + b x)$ we are *deducing* it from objects that have already been defined.
:::

What is the output of $\Delta()$? It takes a *function* as an input and returns ... a function as an output.

We might choose to give a name to the output, for example fred() or betty() or, more helpfully, $\mbox{rate}_{\sin}$ or $\Delta \sin$, but our naming conventions, particularly the use of $x$ indicate that $\frac{\sin(x+h) - \sin(x)}{h}$ is a function.

In calculus, there are a handful of celebrity functions that take a function as input and return a function as output. Notice that the word "function" appeared three times in the previous sentence. To avoid this sort of sleep-inducing repetition, we'll call such functions ***operators***. 

```{exercise, name="SIR-dimensions"}
```
"Exercises/Diff/SIR-dimensions.Rmd")`



## Differentiation


Two central operations in calculus are:

1. Given a function $f(t)$, find the function $\partial_t\,f(t)$ giving the instantaneous rate of change of $f()$. This process of deriving $\partial_t\, (t)$ from $f(t)$ is called ***differentiation***.
2. Given a function $\partial_t\,(t)$, find the $f(t)$ of which $\partial_t\,f(t)$ is the instantaneous rate of change. This process of finding $f()$ given $\partial_t\,f()$ is called ***anti-differentiation***. `r mark(2065)`


Notice that in (1) and (2) above we didn't use the ${\cal D}_t$ notation. It's time to switch away from that. What prompts the change is the nuisance of the constant 0.1 in the definition of the slope function:
$${\cal D}_t f(t) \equiv \frac{f(t+0.1) - f(t)}{0.1}$$

Whereas the slope function ${\cal D}_t f(t)$ gives an approximation to the instantaneous rate of change, $\partial_t f(t)$ refers to the instantaneous rate of change **exactly**. 

We'll come back to the relationship between ${\cal D}_t$ and $\partial_t$ in Chapter \@ref(evanescent-h). It's a subject of particular interest to mathematicians, hence a central part of traditional calculus texts (which are mostly written by mathematicians). For modeling and other applications of calculus, it is something of a side issue. `r mark(2070)`

As an intermediate step on the path between ${\cal D}_t$ and $\partial_t$, let's redefine the slope function to eliminate the 0.1 and replace it with a parameter $h$: $${\cal D}_x f(x) \equiv \frac{f(x+h) - f(x)}{h}$$
This way of writing the slope function will enable us to consider how the slope function changes as $h$ gets smaller and smaller.
