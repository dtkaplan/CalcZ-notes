# Taken out of Diff- series



## Functions and perception

<!--
As you know, a function takes one or more inputs and returns a value as output. The functions we examine in CalcZ take *quantities* as inputs and return a *quantity* as an output.
The algorithm that forms the body of the function describes arithmetic and other calculations that can turn the inputs into the output.

On the other hand, we can also use ***tables*** as functions. With a table, you specify the input, look up that input in one of the colums of the table which brings you to the right row. Then read out from that row the value in another column to be the output. The quantitative operation needed for table lookup is simple comparison. The floor/corridor/door metaphor describes table lookup as well as function evaluation.

In the previous block, we constructed functions to represent the patterns seen in data. In one example, we constructed a function $g(t) = A + B e^{-k t}$ to represent the temperature of water cooling in a mug as a function of time. In another example, we summarized the pattern of rising and falling tides.

It's common sense that data is stored in tables. But we could easily represent any smooth mathematical function, such as our basic modeling functions, as a table look-up problem. Indeed, in the era before computers, many mathematical functions were used in exactly this manner: a printed table in which a person could search for a match to the input and retrieve a value for the output.

::: {.todo}
[Picture of some nice old table.]
:::

In the computer era, we still routinely represent functions this way: data stored in computer files. For instance, an MP3 file is not much more than a sequence of numbers that record a complicated function of time: the air pressure variations of sound. Similarly, digital images record functions of $x$ and $y$ over a limited domain. Given $x$ and $y$ as input, you can look up the output by going to the right pixel.

-->

We humans perceive the world using sight and sound and our other senses. Both sight and sound are highly complex biophysically, but the *process* can be broken down trivially into a relationship: *reality $\longrightarrow$ perception*.

Perception takes place in your mind and brain, extended by sensors such as the retina of the eye and cochlea of the ear. There is considerable scientific understanding of how the retina and cochlea work, but perception itself is still somewhat mysterious and involves the interaction of sensor input with our memory and other cognitive processes.

Still, for both hearing and sight, we can analyze the *reality $\longrightarrow$ perception* process by adding an intermediate layer:

- For sight: *reality $\longrightarrow$ image $\longrightarrow$ perception*
- For hearing: *reality $\longrightarrow$ sound $\longrightarrow$ perception*

We know a tremendous amount about sound and image. For instance, we can reliably and effectively create or synthesize realistic sounds and images. Indeed, we can study the *image $\longrightarrow$ perception* process in isolation. And, as so often happens in science, we study the process by creating a mathematical representation of it.

This starts with the mathematical representation of image and sound. Since this is a calculus course, you won't be surprised when we propose that good mathematical representations are functions:

- Sound: a function of one variable, $t$ for time.
- Image: a function of two variables: the $x$ and $y$ coordinates of an image.

A sound or an image are often represented as multiple functions, for example the left and right channel of stereo sound, or the red, green, and blue planes of a digital image. But a single channel or plane is able to represent sounds or images with considerable fidelity. For simplicity, we'll stick to that: sound(t) and image(x,y).

What about the 3rd dimension? The retina is effectively a two-dimensional sensor. The third dimension comes in through the *reality $\longrightarrow$ image* part of the overall process.

Consider the image in Figure \@ref(fig:sand-1). It is a picture of some indentations in a small area of sand, about two inches wide in the middle of a hiking trail. The dots are individual grains of sand.

```{r sand-1, echo=FALSE, out.width="50%", fig.cap="Sand furrows found on a hiking trail.", fig.align="center"}
knitr::include_graphics("www/sand-furrows.png")
```

Can you see three almost parallel furrows? How about the small crater in the upper left?

You can see the individual grains of sand because they contrast sharply with their neighbors.


You can think of the surface of the sand as a function of $x$ and $y$. It's lower in some places and higher in others. But, in fact, you can't see the height of an individual point in the photograph. In the right light, you wouldn't notice the furrows at all. But the way the picture is lighted, raking sunlight from the left, the *reality $\longrightarrow image* process translates the surface into broad regions of brightness and shadow. In the light regions, the surface slants toward the sun. In the shadows, the surface slants away from the sun.

What you're mainly seeing in the photo is the ***slant*** or ***slope*** of the surface. The raking light has transformed *elevation* as a function of $x$ and $y$ into *slant* as a function of $x$ and $y$ and then encoded the slant as brightness. Ironically, it would be much less effective to present the surface height directly as an image

The moral here is that sometimes the data in a function is not in the right form for us to extract useful information. But by transforming that data to represent contrast or difference or slope, the information can be revealed.

This Block is about transforming functions to show difference and slope. Such transformation, accomplished by mathematics rather than the raking light of the sun, can take a pattern that we're presented with and turn it into another pattern that can tell us what we want to know.

## Slope of an image

```{r}
set.seed(137)
diam <- 8
f <- rfun(~ x+ y)
contour_plot(f(x, y) ~ x + y, domain(x=c(-diam, diam), y=c(-diam,diam)), contours_at = -100, fill_alpha=1, contour_color=NA, n_fill=500, fill_scale=ggplot2::scale_fill_grey())
fx <- D(f(x, y) ~ x)
contour_plot(fx(x, y) ~ x + y, domain(x=c(-diam,diam), y=c(-diam, diam)), contours_at = -100, fill_alpha=1, contour_color=NA, n_fill=500, fill_scale=ggplot2::scale_fill_grey())
```



::: {.todo}
Move this earthquake example to a point where you can do parametric plots.

:::



## Use case: Risk of earthquakes

For software designers, a ***use case*** is a description of how a person will use the software to accomplish a particular goal. It's fair to wonder what are the use cases of differentiation. It's early days in your study of calculus, so some of the use cases are beyond your reach. But knowing about first and second derivatives, argmaxes and maximums, curvature, and linear and quadratic approximations gives you access to one of the most important general patterns in scientific work: the measurement of ***precision***.

As you know, precision refers to how well you know a quantity and is often expressed using the plus-or-minus notation. To illustrate, consider the earthquake risk situation in the Cascadia Subduction Zone that includes western Oregon and Washington states. The last devastating earthquake was on January 26, 1700, a date approximated by local oral tradition and derived from written tsunami records across the Pacific Ocean in Japan.

Geologic features allow approximate dating of previous large earthquakes in the region, specifically one about 700 years previous to the latest and on about 2000 years before that. 
<!-- https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1029/97RG00222 section 6.3 -->

Your task as a modeler is to estimate the probability of another high-magnitude earthquake occuring in the next 100 years.

A standard model for the time interval between consecutive earthquakes is $p(y) \equiv k e^{-ky}$, where $p(t)$ is the probability of an earthquake happening exactly $y$ years after the last. We can use data on previous earthquakes to find an approximate value for $k$. Once we know $k$, we can calculate the probability that the next earthquake will occur between 2025 and 2125 as $e^{-325 k} - e^{-425 k}$ where 325 is the time interval from 1700 to 2025 and 426 is the interval between 1700 and 2125. (You'll see where the formula comes from in Block 3.)

We have information on two complete earthquake cycles, one lasting 2000 years and the other 700. A standard way to estimate the parameter $k$ is called "maximum likelihood." This amounts to assuming a value for $k$ and then, with that value, calculating the likelihood of having seen inter-quake intervals of 2000 and of 700 years. This likelihood is
$$p(2000) \times p(700) = k^2 e^{-k (2000 + 700)}$$ A graph of the likelihood as a function of $k$, using a log vertical scale gives:

```{r}
slice_plot(log(k^2 * exp(-k*2700)) ~ k, domain(k=c(0.0001, 0.003)), npts=500)
```
Making a parametric plot

```{r}
Quakes <- tibble::tibble(
  k = seq(0.0001, 0.0025, length=1000),
  LL = log(k^2 * exp(-k*2700)),
  prob = exp(-325*k) - exp(-425*k)         
)
gf_point(LL ~ k, data=Quakes)
gf_point(LL ~ prob, data = Quakes)
gf_point(prob ~ k, data = Quakes)
```




## Differencing

Our basic tool for showing difference and slope is a remarkably simple operator that takes a function as input. For a function with one input, the operator ${\cal D}()$ is defined as 
$${\cal D}(f) \equiv \frac{f(x + 0.1) - f(x)}{0.1}$$
Notice that the ${\cal D}()$ operator returns a **function**. The output is a linear combination of the input function $f()$ and a shifted version of $f()$.

For a function with two inputs, there are two versions of ${\cal D}$:
$${\cal D}_x(f) \equiv \frac{f(x+0.1, y) - f(x, y)}{0.1}$$

$${\cal D}_y(f) \equiv \frac{f(x, y+0.1) - f(x, y)}{0.1}$$


::: {.workedexample}
Suppose that $f(x)\equiv x^2$. What is the function ${\cal D}(f)$?

$$
{\cal D}(f) \equiv \frac{f(x+0.1) - f(x)}{0.1}\\ =
10\left((x+0.1)^2 - x^2\right)\\
=10 x^2 + 2x + 0.1 - 10x^2\\ = 2 x + 0.1$$
:::

::: {.workedexample}
Suppose that $f(x) \equiv 2 x + y$. Find the function ${\cal D_x(f)}$.

The linear combination will be 
$$\frac{1}{0.1}\left(\left[2 (x + 0.1) + y\right] - \left[2 x + y\right]\right) =\\ \\
20 x + 2 + 10y - \left[20 x - 10y\right] = 2$$
:::

```{r}
f <- makeFun(2 *x * y ~ x + y)
D(f(x, y) ~ x)
```

Figure \@ref(fig:three-rates) shows the fitted rate of change and average rate of change in the interval $10 \leq t \leq 125$ as well as the instantaneous rate of change at $t_0 = 50$. They are different from one another.

```{r three-rates,  echo=FALSE, warning=FALSE, fig.cap="Comparing the average rate of change (red) over the interval $10 \\leq t \\leq 125$, the instantaneous rate of change at $x_0 =25$, and the fitted linear model (orange). The exponential model is shown in gray."}
set.seed(101)
Stans_data <- CoolingWater %>% sample_n(20)
gf_point(temp ~ time, data = Stans_data, color="orange") %>%
  gf_lm(color="orange") %>%
  slice_plot(water(time) ~ time, domain(time=c(0, 200)), color="gray", size=2, alpha = 0.5) %>%
  slice_plot(tangent(time) ~ time, color = "sienna", 
             domain(time=c(0,80)))  %>%
   gf_point(y ~ t, data = Pts, color="orange3", alpha=0.25, size=4, inherit=FALSE) %>%
  gf_line(y ~ t, data = Pts, color="orange3") %>%
  gf_point(25 ~ t, data = Pts, color="dodgerblue", alpha=0.25, size=4) %>%
  gf_point(y ~ t, data=Pt,
           color="sienna", alpha = 0.25, size=4) %>%
  gf_point(25 ~ t, data = Pt, color="tomato", alpha=0.25, size=4) %>%
  gf_segment(25 + y ~ t+t, data=Pt, color="tomato", linetype=2) %>%
  gf_segment(25 + y ~ t+t, data=Pts, color="dodgerblue", linetype=2) %>%
  gf_lims(y=c(25, NA)) %>%
  I()
```



Symbolic differentiation is a **mechanical** process. Some people are good at working like a machine. And some people aren't. It helps to be very organized and to have a really good short-term memory. For decades, computers have been programmed to calculate symbolic differentiation. What's required is the systematic and reliable application of a small set of rules. Computer's excel at this.

A set of about a dozen rules will handle all the formulas we'll encounter in this ook or that are commonly used for modeling. In this chapter, we're going to derive only a handful of the rules---those related to the straight-line function. Then we will introduce, without motivation, the rules for the pattern-book and basic modeling functions. In Chapter \@ref(evanescent-h) we'll motivate the rules for the pattern-book functions. In Chapters XXX we'll develop the three rules that correspond to the three forms of creating new functions out of old: linear combinations (the ***sum*** rule), function multiplication (the ***product rule***) and function composition (the ***chain rule***).

Recall that for a function $f(x)$, the derivative $\partial_x f(x)$ corresponds to the ***instantaneous slope*** of $f(x)$. There is a small set of functions for which the instantaneous slope is the same for all $x$. These are the straight-line function $g(x) \equiv a x + b$ and the constant function $h(x) \equiv b$. 

The rule for the straight-line function is
$$\partial_x \left[a x + b\right] = a$$
The rule for the constant function is 
$$ \partial_x \left[b\right] =0$$ There are two special cases of the straight-line function which are really just a restatement of the above two rules but which are worth emphasizing:
$$\partial_x \left[a x\right] = a \ \ \ \text{and}\ \ \ 
\partial_x \left[x\right] = 1$$

Note that the right-hand side of all these differentiation rules based on the straight-line function **does not involve $x$**. This amounts to saying that the derivative function for the straight-line function does not vary with $x$. Equivalently, you can say that the derivative function for the straight-line function is a constant function.

The ***with-respect-to input***---the input whose name is given as a subscript to $\partial$  is the key to these patterns. Whenever $\partial_x$ is applied to any formula that **does not contain** $x$, the derivative is zero. You see that in $\partial_x\left[ b \right] = 0$. Similarly $\partial_x \left[ b \right]=0$ and even very complicated things not involving $x$ have derivatives that are zero, for instance: 
$$\partial_x \left[\sin\left(2\pi \sqrt{\cos(y^2 + 5 e^z)}\right)\right] = 0$$
For notation, let's refer to any formula that does not involve the with-respect-to input as $$\text{Anything without x dependence:}\ \ \xcancel{\strut\mathbf x}$$

* No $x$ rule: $\ \partial_x \left[a\right] = 0\ \ \text{where}\ \ a = \xcancel{\strut\mathbf x}$

* Simple x rule: $\partial_x \left[x\right] = 1$

* Scaling $x$ rule: $\partial_x \left[ a x\right] = a\ \ \text{where}\ \ a =  \xcancel{\strut\mathbf x}$


```{exercise, name="basic rules"}
A few drill problems on the above three rules.
```

```{exercise, name="other than x"}
A few drill problems on the above three rules but with other variables
```

## Symbolic derivatives of modeling functions

For a human training to be a symbolic differentiator, it's best simply to memorize the patterns for derivatives of the other pattern-book functions. They are:

$f(x)$      | $\partial_x f(x)$
------------|------------------
$e^x$       | $e^x$
$\sin(x)$   | $\cos(x)$
$\ln(x)$    | $1/x$
$x^p$       | $p\, x^{p-1}$
sigmoid, that is, pnorm(x)    | hump, that is, dnorm(x)
hump, that is dnorm(x)        | $-x \text{dnorm}(x)$

Note that some of the pattern-book functions have derivatives which are simply pattern-book functions

Name $f(x)$|           $f(x)$  | $\partial f(x)$ | name $\partial_x f(x)$
:---------:|:----------------:|:----------------:|:------:
exponential| $e^x$              | $e^x$          | exponential
logarithm  | $\ln(x)$           | $1/x$          | power-law
sigmoid    | $\text{pnorm(x)}$ |$\text{dnorm(x)}$| hump


Another couple of pattern-book functions involve division or division by $x$ and multiplication by a scalar. (The second one, $\partial_x \text{hump}(x)$, was not in the original short list)

Name $f(x)$|           $f(x)$  | $\partial f(x)$ | name $\partial_x f(x)$
:---------:|:----------------:|:----------------:|:------:
power-law  |    $x^p$  | $\frac{p}{x} x^p = px^{p-1}$  | power-law
hump       | $\text{dnorm(x)}$  | $- x\, \text{dnorm(x)}$| "hiccough"

Traditionally, the derivative of the sinusoid is presented as a pair of kissing cousins:

Name $f(x)$|           $f(x)$  | $\partial f(x)$ | name $\partial_x f(x)$
:---------:|:----------------:|:----------------:|:------:
sinusoid   | $\sin(x)$  | $\cos(x) = \sin(x+\pi/2)$ | sinusoid
sinusoid   | $\cos(x)$  | $-\sin(x) = \cos(x + \pi/2) =  \sin(x+\pi)$ | sinusoid

::: {.takenote  data-latex=""}
Memorizing the derivatives of the short list of pattern-book functions will make it much easier to follow the rest of the section. Do that now.
:::

Recall that the ***basic modeling functions*** are just the pattern-book functions which have been tailored to match the type of input. The tailoring can be a simple a scaling function, such as $a\, x$ or $k\, x$  or $\frac{2\pi}{P} x$. As well, the tailoring can involve both a scaling and a ***shift***, for instance $a x + b$ or $a (x-x_0)$
Let's give a name to the tailoring function and recognize the several varients:

Variations of tailoring$(x)$ |    | $\partial_x \text{tailoring}(x)$
-----------------------|---------|
$ax$  | scaling | $a$
$kx$  | scaling | $k$
$\frac{2\pi}{P} x$ | scaling | $\frac{2\pi}{P}$
$ax + b$ | scaling and shift | $a$
$a(x-x_0)$ shift and scale   | $a$
$\frac{x - \text{mn}}{\text{sd}}$ | shift and scale | $\frac{1}{\text{sd}}$}


Every basic modeling function is a composition of the corresponding pattern-book function with the tailoring function:

pattern-book    | tailored   | typically written
---------|-----------|------------------
`exp(x)` | `exp(tailoring(x))` | $e^{kx}$
`sin(x)` | `sin(tailoring(x))` | $\sin\left(\frac{2\pi}{P} x\right)$
`pnorm(x)` | `pnorm(tailoring(x))` | `pnorm(x, mn, sd)`
`dnorm(x)` | `dnorm(tailoring(x))` | `dnorm(x, mn, sd)`

To differentiate the basic modeling functions, follow exactly the same rules as for the pattern-book functions, but instead of $x$, write the tailored form of $x$. And then, multiply the whole thing by the derivative with respect to $x$ of the tailoring function.

$$\partial_x \left[ f(\text{tailoring}(x))\right] = [\partial_x]\left(\strut\text{tailoring}(x)\right) \times 
\partial_x \text{tailoring}(x)$$

Some examples:

$f(x)$    |   $\partial_x f(x)$
----------|----------------------
$e^{kx}$  | $k \times e^{kx}$
$\sin\left(\frac{2\pi}{P} x\right)$ | $\frac{2 \pi}{P} \times \cos\left(\frac{2\pi}{P} x\right)$
$\text{pnorm}(x, \text{mn}, \text{sd})$ | $\frac{1}{\text{sd}} \times \text{dnorm}(x, \text{mn}, \text{sd})$


<!--

MOVE TO LATER ON



Come to think of it ... this is where computers shine. So let's start with how to use the computer for differentiation. Then we can talk about the rules that the computer is using.

I'm going to assume that you are comfortable with using `makeFun()` in the R/mosaic software. 

> **To compute derivatives, use `D()` instead of `makeFun()`. Everything else is exactly the same. 

For example, this is a typical use of `makeFun()`:
```{r}
f <- makeFun(a*x^2 + b*x + c ~ x, a=2, b=4, c=-3)
```
We're calling the newly created function `f()` and we can use it in the ordinary ways:
```{r}
f(2)    #evaluate it at a specific input
g <- makeFun(f(x)*sin(x) ~ x) # Build a new function with it
slice_plot(f(x) ~ x, domain(x=c(-3,3))) # Plot it
findZeros(f(x) ~ x)  # Find the zero crossings
```
Same with `D()`:

```{r}
dx_f <- D(a*x^2 + b*x + c ~ x, a=2, b=4, c=-3)
```
We're calling the derivative of `f()` by the name `dx_f()`. We could, as usual, have used any name, but it's nice to have a convention that makes it easier to keep track of things.
```{r}
dx_f(2)    #evaluate it at a specific input
g <- makeFun(dx_f(x)*sin(x) ~ x) # Build a new function with it
slice_plot(dx_f(x) ~ x, domain(x=c(-3,3))) # Plot it
findZeros(dx_f(x) ~ x)  # Find the zero crossings
```
::: {.tip }
As a matter of good computing style ... 

Since we had already defined `f()`, in computing its derivative a good practice is to use `f()` itself rather than copying over the tilde formula for use in `D()`. The function created by `D()` will inherit all the same default parameters used in `f()`.

```{r}
D(f(x) ~ x)
```

Notice: No $h$!

:::

## Symbolic differentiation rules

Each rule consists of a pair of patterns that can be organized into two columns of a table. If the formula matches a pattern on the left, then re-write it into the pattern on the right.

First, we need a way to denote the patterns involved in differentiation and to relate them to the way we write formulas. We already have most of what's needed in the patterns of the basic modeling functions and in the patterns of linear combination, function composition, and function multiplication. 

Let's start with the straight-line function and it's derivative. Here's the first line of our pattern table, which we'll write using $x$ as the ***with-respect-to input***.

pattern name  | Original       |  Derivative w.r.t. $x$
--------------|----------------|-------------
straight-line | $a x + b$      | $a$

There are many forms that are equivalent to the straight-line formula, for instance $3 x + 2$ or $4(x-6)$ or $a(x-x_0)$, $b + x a$, $(4 + 1)x - 7$. Whenever you encounter such a form, re-arrange it into $a x + b$.


* **Example 1**: Re-arrange $7-(4+1)x$ as $a x + b$.    
    
    Use arithmetic reduction to simplify to $7-5x$ then put it in the standard order, getting $a=-5$ and $b=7$. The derivative is therefore $-5$.

* **Example 2**: Re-arrange $3(x-2)$ as $a x + b$.

    Distribute the multiplication by 3 to get $3x - 6$. This has $a=3$ and $b=-6$. There derivative is therefore $3$.

* **Example 3**: Re-arrange $m (x - x_0)$ as $a x + b$.

    Distribute the multiplication by $m$ to get $mx - m x_0$. Recognizing that $m x_0$ does not involve the input $x$, get $a=m$ and $b= -m x_0$. The derivative is therefore $p$.

* **Example 4**: Re-arrange $3 x + 4 - 2 x +1$ as $ax + b$. 

    Bring together the terms involving $x$ and use arithmetic to simplify. Similarly, bring together the terms not involving $x$ and use arithmetic to simplify. 
$$3 x + 4 - 2x + 1 = (3 x - 2 x) + (4 + 1) =  1 x + 5$$ so $a=1$ and $b=5$. The derivative is therefore $1$.

* **Example 5**: Re-arrange $-x$ as $ax + b$. 

    We have to be inventive and recognize that $-x$ has the format $-1 x + 0$. So $a=-1$ and $b=0$. There derivative is therefore $-1$. 

* **Example 6**: Re-arrange $2 x_0$ as $ax + b$. Being inventive in the same way to recognize $a = 0$ and $b=2 x_0$. The derivative is therefore 0.

* **Example 7**: Re-arrange $\frac{2 \pi}{P} (x-x_0)$ as $a x + b$. 

What's potentially confusing here is complicated looking $\frac{2 \pi}{P}$. You can see that this doesn't involve the input $x$ in any way. For the moment, let's replace $\frac{2 \pi}{P}$ with something simpler: $c$. Then the formula is $c(x-x_0)$ which is in the form of Example 3. The derivative of that is simply $c$. And $c$ really stands for $\frac{2 \pi}{P}$ in the original, so the derivative of the original is $\frac{2 \pi}{P}$

Counter Example: Re-arrange $c x^2$ into $a x + b$. There's no way! $c x^2$ is a different kind of pattern. There's not yet a row in the pattern table to tell us how to handle $x^2$.

The straight-line rule is implemented as `axb_rule()`. You give `axb_rule()` the formula (in R notation). The result is the derivative according to that rule. Importantly, if the rule does not apply, then the result is `FALSE`:

```{r eval=FALSE}
sum_rule((a+b)*x + c*x^2)
```



## Re-writing the basic modeling functions

We're going to extend the pattern table to include the basic modeling functions. Recall that every basic modeling function is a composition of a pattern-book function with an $ax + b$ scaling function.

pattern name  | Original             |  Derivative w.r.t. $x$
--------------|----------------------|-------------
straight-line | $a x + b$            | $a$
exponential   | $\exp(a x + b)$      | $a \exp(ax + b)$
sinusoid      | $\sin(a x + b)$      | $a \cos(ax + b)$
power-law     | $\text{pow}(ax+b,p)$ | $a\,p\, \text{pow}(a x + b, p\!\!-\!\!1)$
logarithm     | $\log(a x + b)$      | $a\, \text{pow}(ax + b, -1)$
sigmoid       | $\text{pnorm}(a x + b, mn, sd)$ | $a\, \text{dnorm}(a x + b, mn, sd)$

::: {.why  data-latex=""}

I wrote most of the basic modeling functions in their traditional mathematical notation except for $e^x$ and $x^p$. In the table, I wrote those as $\exp(x)$ and $\text{power}(x, p)$. This is similar in form to $\text{pnorm}(x, mn, sd)$ and $\text{dnorm}(x, mn, sd)$.

The reason to use the $\exp()$ and $\text{pow}()$ notation is to make it easier to see the commonality of the patterns. Each of the rows of the table, except the first, has the form $f(a x + b)$ or $f(a x + b, p)$ or $f(a x + b, mn, sd)$. Only the first argument to $f()$ involves an $x$-expression. If there are additional arguments, they never involve $x$. In other words, $p$, $mn$, and $sd$ are ***constants***.

If you prefer the traditional notation, the relevant rows of the pattern table are:

pattern name  | Original             |  Derivative w.r.t. $x$
--------------|----------------------|-------------
exponential   | $e^{a x + b}$      | $a e^{ax + b}$
power-law     | $(ax+b)^p$ | $p\, a\, (ax + b)^{p-1}$

:::


In Chapters XX, XXX, and XXXX we'll give the re-writing rules for linear combination of functions, function composition, and function multiplication. 


```{exercise}
Drill on these forms
```

```{exercise}
Move this to the chapter on assembling functions. Drill on spotting linear combinations, f(g(x)) is composition, $\sin(x^2)$ is composition, ...
```

## Pattern recognition

 To demonstrate this, and to show why computers can be "taught" to do symbolic differentiation, we'll have you practice with an R function that identifies the two kinds of patterns in our pattern table.

```{r echo=FALSE}

```

The first question when symbolically differentiating a function is whether the formula for the function is of the form $ax + b$. Our function for this is `axb_rule()`. The output of this function will be `FALSE` if the formula is not of that form. But if there is a match, `axb_rule()` will give you the formula for the derivative.

```{r eval=FALSE}
axb_rule(a*x + b)
axb_rule(2*x)
axb_rule(x)
axb_rule(-x)
axb_rule(1/x)
axb_rule(x^2)
```
The function `bmf_rule()` identifies if the formula is one of the basic modeling functions.

```{r eval=FALSE}
bmf_rule(a*x + b)
bmf_rule(sin(x))
bmf_rule(sin(3*x + 2))
bmf_rule(sqrt(x))
bmf_rule(sqrt(a*x + b))
bmf_rule(sqrt(x^2))
bmf_rule(sin(sqrt(x)))
bmf_rule(x^3)
bmf_rule((a*x + b)^4)
bmf_rule((a *x^2)^ 4)
bmf_rule(pnorm(x, mn, sd))
```
```{exercise}
```
Explain why the result is zero and not simply `a`.
```{r eval=FALSE}
bmf_rule(ax + b)
```

-->


# h and derivatives {#h-and-derivatives}

We already established, by numerical experiment, the result of differentiating the pattern-books functions. To summarize,

```{r child="D-naked-functions.Rmd"}
```

A mathematician might prefer to replace the word "established" in the first sentence of this table with a weaker word "motivated" or "proposed." This is entirely fair. Indeed, let's do another experiment that will cause us to wonder just how solid are the conclusions presented in the table above.

Recall that we can easily define the slope function for any $f(x)$, for example the slope function of $\sin(x)$:
```{r}
Dsin <- makeFun((sin(x+h) - sin(x))/h  ~ x)
```
In justifying the entry for $\sin()$ in the table, we plotted `Dsin()` using small `h`, for instance, `h=0.000001`. It would be comforting to continue the experiment with even smaller `h`. Doing so, we discover a problem.


```{r Dsin-small-h, echo=FALSE, fig.cap="Graphing the computer's slope function of $\\sin(x)$ for small enough $h$ produces a result inconsistent with the table for the pattern-book functions. Instead of producing a $\\cos(x)$, we get a ragged function."}
slice_plot(Dsin(t, h=0.000000000000001) ~ t, domain(t=c(-5,2*pi)))
```
Things get even worse for smaller $h$ still, as you can confirm for yourself using a computing sandbox.

It turns out that the reason for this behavior is the way in which computer arithmetic has been engineered. To demonstrate the non-mathematical behavior of computer arithmetic, consider what happens when we add and subtract using 1 and a number that is small compared to 1.

```{r}
options(digits=20)
0.000000000000000000000001
1 + 0.000000000000000000000001
1 - 0.000000000000000000000001
1 + 0.000000000000000000000001 - 1
```

Using technical computing successfully at a professional level requires some understanding of the ways in which computer arithmetic differs from mathematical arithmetic, And the R/mosaic `D()` operator has been constructed with these considerations in mind. But our purpose here is not to push the computer beyond it's arithmetic limits but to demonstrate that the differentiation table for the pattern-book functions is correct.

## The $h$ framework

In the end, all of the work we're going to do with $h$ will have a simple result: confirming the facts presented in the differentiation table. In a court of law, that confirmation could be established by appeal to established authority. For instance, look in any calculus textbook and you'll see the same facts as in the table. (And if you find an exception, you can be sure it's a typographical error!)

For most people, mathematical proof is not much different from appealing to established authority. Not everyone is skilled at following the deductive steps of a proof and almost everyone has been tricked into accepting a step that is not logically valid. And everyone makes mistakes. 

For the benefit of such people, rather than proving the facts in the differentiation table, we're going to reconstruct, hopefully in a fun way, the  framework developed by the mathematics community over roughly two centuries that enabled mathematicians to satisfy themselves that the methods used by the pioneers could be justified beyond doubt.

The basic problem, which everyone always agreed on, is that it's not proper to set $h$ to zero. The dispute is about how to handle $h$ in such a way that it can be held non-zero and yet give results where $h$ has evaporated as if it were never there. 

To illustrate how this can be done, consider the algebra of the slope function for $g(x) \equiv x$:

$${\cal D}_x g(x) \equiv \frac{g(x+h) - g(x)}{h} = \frac{(x+h) - x}{h} = \frac{h}{h} = 1$$ 
The above steps are completely justified for any $h$ whatsoever, so long as $h \neq 0$. So we have a formula ${\cal D}_x\, x = 1$ which will be correct no matter whether $h$ is big or small.



In contrast, consider the slope function of $f(x) \equiv x^2$:
$${\cal D}_x f(x) \equiv \frac{f(x+h) - f(x)}{h} = \frac{(x+h)^2 -x^2}{h} = \\
\ \\
= \frac{x^2 + 2hx + h^2 - x^2}{h} = \frac{2 h x + h^2}{h} = 2 x + h$$
Again, we have a formula which is correct for any $h$ whatsoever, so long as $h$ is not zero. Unfortunately, $h$ is still present in the formula. In differentiating $x^2$ we want to make $h$ go away.


I like to think of $h$ as a kind of *tire iron*, a small tool used to stretch the bead of a bicycle tire in order to pull it over the wheel rim. 

```{r echo=FALSE, out.width="30%", fig.cap="A tire iron in use", fig.align="center"}
knitr::include_graphics("www/tire-iron.png")
```

Once the tire iron has done its job, its removed and you would never know that it was ever there (except that the tire is now successfully mounted on the wheel).

But this is calculus, not bicycle mechanics. How do we know that removing the tire iron isn't damaging the mathematical wheel? 

Still in the spirit of having fun, let's try a more serious metaphor... imagining $h$ is actually a central character in a calculus play. The character $h$ is in the middle of the story but *never appears in the play*, like the missing character Godot in the famous play *[Waiting for Godot](https://en.wikipedia.org/wiki/Waiting_for_Godot#Godot)*. 

We said that $h$ in the slope function ${\cal D}_x x^2 = 2 x + h$, so long as $h$ is small, plays both a central role and has hardly any effect. An economizing director re-writes the play to take $h$ out of it, setting $h=0$ in the formula $2 x + h$: a non-speaking, offstage role.

We've already seen using legitimate algebra that $${\cal D}_x g(x) = 2 x + h$$ Re-writing by replacing $h$ with 0 streamlines the play, turning ${\cal D}_x x^2 = 2x +h$ from a dialog involving both $x$ and $h$ into a monologue with $h$ absent: $$\partial_x x^2 = 2 x$$ Simple.

And yet ... the director gets a letter from the Bit Players Union. 

> *We observe that you have eliminated the role of $h$ in the final production version of $\partial_x g(x)$. This is a violation of Union regulations. Recall that the basis for $\partial_x g(x)$ is the slope function ${\cal D}_x g(x)$. The slope function is defined as a ratio: $${\cal D}_x g(x) \equiv \frac{g(x+h) - g(x)}{h}$$ Eliminating $h$ entirely by replacing her with zero is a **division by zero error** forbidden by Article 3.16§B¶2 of the Unified Laws of Arithmetic. We ask that you comply with this Article by re-instating the role of $h$ in all evaluations of ${\cal D} g(x)$.*

Reading this, the director calls her lawyer. Is there a loophole for removing $h$ without breaking the mathematical prohibition on dividing by zero? 

The loophole involves a bit of legalistic cover, something like a corporation. As you may know, a corporation is a legal structure that makes if feasible for people to invest without being subject to unlimited liability. You bought stock in a company that later accidentally caused a catastrophe? The company will go out of business and your stock will be worthless. But you are legally obliged to fix the damage: you liability is limited. Such companies identify themselves as such with the legal suffix "Inc." and they are beholden to the state in certain ways, such as the requirement to pay taxes on profits.

In calculus, the equivalent of "Inc" is $\lim_{h \rightarrow 0}$. By prepending this to a calculation, you are allowed to carry out arithmetic operations such as dividing by $h$ without concern about liability for dividing by zero. You can perform any algebraic operations so long as they are legitimate when $h \neq 0$. For instance, it is taken as entirely correct to say:

$$\lim_{h\rightarrow 0}\frac{h}{h} = 1$$
Another privilege for the users of $\lim_{h\rightarrow 0}$ is that, at the end of the algebraic derivation, they are entitled to replace $h$ with zero so long as no divide by zero is required. The endpoint of the above is 1, where $h$ doesn't even appear. But consider
$$\lim_{h \rightarrow 0} \frac{(x + h)^2 - x^2}{h} = \\
\ \\
= \lim{h \rightarrow 0} 2 x + h$$

In the last step, an $h$ appears. But we are entitled to take the result of the derivation, $2 x + h$, and replace the $h$ by zero. Doing so doesn't entail any illegitimate operation such as dividing by zero.

At it's most basic, you're entitled to state $$\lim_{h\rightarrow 0} h = 0$$ which, to be honest, looks like nothing more than common sense.

## Approximations to pattern-book functions

Let's take a close look at two of our pattern-book functions: $e^x$ and $\sin(x)$. By "close" I mean very near to $x=0$. You already know that $e^0 = 1$ and $\sin(0) = 0$. But consider "very small" $x$, in the spirit of $\lim_{x\rightarrow 0}$. 

The fundamental approximations are these:

$$e^x \approx 1 + x\\
\ \\
\sin(x) \approx x\\
\ \\
\cos(x) \approx 1 - x^2$$

Figure \@ref(fig:small-x-pattern-book} shows the naked functions along with the approximations. We're interested in "small" $x$. The left panel shows the functions for $-1 \leq x \leq 1$ and the right panel zooms in for small $x$, taking "small" in an arbitrary but everyday sense of, say, less than 0.01.

```{r small-x-pattern-book, echo=FALSE, fig.show="hold", out.width="50%", fig.cap="Comparing the pattern-book functions exp(x), sin(x), and cos(x) to their simple approximations for small x. The functions are drawn as a broad gray line; the approximations are a thin blue line."}
options(digits=5)
slice_plot(exp(x) ~ x, domain(x=c(-1, 1)), size=3, alpha=0.25) %>%
  slice_plot(1 + x ~ x, color="dodgerblue") %>%
  gf_labs(title = "Exponential")
slice_plot(exp(x) ~ x, domain(x=c(-0.01, 0.01)), size=3, alpha = 0.25) %>%
  slice_plot(1+x ~ x, color = "dodgerblue")

slice_plot(sin(x) ~ x, domain(x=c(-1, 1)), size=3, alpha = 0.25) %>%
  slice_plot(x ~ x, color="dodgerblue") %>%
  gf_labs(title = "Sin")
slice_plot(sin(x) ~ x, domain(x=c(-0.01, 0.01)), size=3, alpha = 0.25) %>%
  slice_plot(x ~ x, color = "dodgerblue")
  
slice_plot(cos(x) ~ x, domain(x=c(-1, 1)), size=3, alpha=0.25) %>%
  slice_plot(1 - x^2 ~ x) %>%
  gf_labs(title = "Cos")
slice_plot(cos(x) ~ x, domain(x=c(-0.01, 0.01)), size=2, alpha = 0.25) %>%
  slice_plot(1 - x^2 ~ x, color = "dodgerblue")
```

Near $x=0$, the graph of the approximation is dead on center of the corresponding pattern-book function.

The approximations will not break down for very small $x$ because none of the approximations involve dividing by $x$. And remember that the approximations are only good when $x$ is small!

Consider the result of differentiating $e^x$. The slope function is

$${\cal D}_x e^x \equiv \frac{e^{x+h} - e^x}{h} = e^x \left[\frac{e^h - 1}{h}\right]$$
Let's examine $\frac{e^h - 1}{h}$ for very small $h$. This is where we will use the approximation for $e^x$, but we'll write it with $h$ as the argument: $e^h \approx 1 + h$. Plugging this in to the bracketed quantity, we have $$\lim_{h \rightarrow 0}\frac{e^h - 1}{h} = \lim_{h\rightarrow 0}\frac{1 + h - 1}{h} = 1$$ Overall, this means $$\partial_x e^x = \lim_{h\rightarrow 0}{\cal D}_x e^x = e^x$$

Now to demonstrate that $\partial_x \sin(x) = \cos(x)$ a fact that is often used in calculus. Only calculus teachers need to know how to perform this demonstration, so the following is just FYI:.

The demonstration builds on a formula for the sine of the sum of two quantities:

$$\sin(x + h) = \sin(x)\cos(h) + \cos(x)\sin(h)$$

With this, we can simplify the slope function of $\sin$:
$${\cal D}_x \sin(x) \equiv \frac{\sin(x+h) - \sin(x)}{h} =\\
\ \\
= \frac{1}{h} \left[\strut\sin(x)\cos(h) + \cos(x)\sin(h) - \sin(x) \right]\\
\ \\
= \frac{1}{h} \left[\strut \sin(x) \left[\cos(h) -1\right] + \cos(x)\sin(h)\right]$$
Plugging in the approximations $\sin(h) = h$ and $\cos(h) = 1 - h^2$ we get
$${\cal D}_x \sin(x) = \frac{1}{h} \left[\strut\sin(x)(- h^2)  +\cos(x)h\right] = \\
\ \\
= \cos(x) - h \sin(x)$$
With the division by $h$ safely removed, we cn now apply the limit:
$$\partial_x \sin(x) = \lim_{h\rightarrow 0} \left[\strut \cos(x) - h \sin(x)\right] = \cos(x) - \sin(x) \left[\lim_{h\rightarrow 0}h\right] = \cos(x)$$

::: {.takenote  data-latex=""}
We write the differentiation operator (with respect to input $x$) as $\partial_x$. We wrote the slope-function operator (again, with respect to $x$) as ${\cal D}_x$. 

The slope-function operator ${\cal D}_x$ is only a stepping stone on the path toward the real destination: differentiation. We're defining differentiation as a limit:

$$\partial_x\, f(x) \equiv \lim_{h\rightarrow 0} {\cal D}_x\, f(x)$$
In practice, however, we perform differentiation whenever possible not with the limit definition, but with the *consequences* of the limit definition, such as $\partial_x e^x = e^x$ or $\partial_x \sin(x) = \cos(x)$. Sometimes these consequences are called the ***rules of differentiation***. These rules are important particularly for carrying out differentiation using paper and pencil, but if a computer is available, the rules have been mastered for you by software. Since such software is widely available, we're going to step away from them for a bit. The next chapters present some of the uses for differentiation and contexts in which differentiation is used in constructing new modeling functions. 
:::

```{exercise, name="low-low-rates"}
```
"Exercises/Diff/low-low-rates.Rmd")`

```{exercise, name="singularity-numerics"}
```
"Exercises/Diff/singularity-numerics.Rmd")`

```{exercise, name="NaN-numerics"}
```
"Exercises/Diff/NaN-numerics.Rmd")`

::: {.todo}
In the limits section or an exercise, ask about $x^0$? Is that the same as 1? What about when $x=0$. Zero raised to any positive power is 0. But any positive number raised to 0 is 1. So two ways of thinking about it give different answers.

The computer handles the question with ease:

```{r}
0^0
```
But computer results don't always agree with mathematical results.

In this case, mathematicians looking at the question through the lens of limits, agree with the computer when looking at the function $g(x) \equiv x^0$. But they disagree if the function is $h(x) \equiv 0^x$ or $f(x) \equiv x^x$.  These three functions are not the same when it comes to limits.

Remember that $\lim_{x\rightarrow 0}$ is not about replacing $x$ with zero, it is about a process of examining the result for non-zero $x$ as $x$ approaches zero. In "zero raised to any positive power" we're talking about $0^{\lim_{x\rightarrow 0}}$, which is not the same as $\lim_{x\rightarrow 0} \left[x^x\right]$.  Similarly, $\left[\lim_{x\rightarrow 0}\right]^0$ is different from
:::

# Derivatives of the basic modeling functions

::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-7a-1", "Know the derivatives of the Naked Core Functions")
```
:::

- $\partial_x e^x = e^x$
- $\partial_x x^p = p x^{p-1}$ for $p \neq 0$
- $\partial_x \sin(x) = \cos(x)$ and $\partial_x \cos(x) = \sin(x)$
- $\partial_x \ln(x) = 1/x$
- $\partial_x \text{sigmoid}(x) = \text{hump}(x)$


::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-7a-2", "Apply the basic derivative shortcuts to find the derivative of a function.")
```
:::



We're going to leave $\partial_x \text{hump}(x)$ alone for now until we have a more complete understanding of derivatives. But other than that, notice that the derivative of every one of these mathematical functions---exponential, power-law, sinusoid---is also in the set of the basic functions.

::: {.workedexample data-latex=""}
How can we know that these formulas are right? Let's start by comparing them to the functions computed from the ${\cal D}_x$ formula when $h$ is very small.

Consider $\partial_x e^x$, which is claimed to be $e^x$. Figure \@ref(fig:exp-h_ plots $e^x$ (in green) as well as a function ARC$(x, h)$ (**A**verage **R**ate of **C**hange) that takes both $x$ and $h$ as inputs: $$\text{ARC}(x, h) \equiv {\cal D}_x e^x = \frac{e^{x+h} - e^x}{h} = e^x \frac{e^h - 1}{h}$$

```{r exp-h, echo=FALSE}
colors <- heat.colors(5)
ARC <- makeFun(exp(x)*(exp(h)-1)/h ~ x + h)
factor <- makeFun((exp(h)-1)/h ~ h)
P1 <- slice_plot(exp(x) ~ x, domain(x=c(-2,2)), 
           size=3, color="green", alpha=0.4) %>%
  slice_plot(ARC(x, h=1) ~ x, color=colors[1],
             label_text="h=1", label_x=1) %>%
  slice_plot(ARC(x, h=0.5) ~ x, color=colors[2],
             label_text="h=0.5", label_x=1) %>%
  slice_plot(ARC(x, h=0.1) ~ x, color=colors[3],
             label_text="h=0.1", label_x=0.9) %>%
  slice_plot(ARC(x, h=0.01) ~ x, color=colors[4],
             label_text="h=0.01", label_x=0.8) %>%
  slice_plot(ARC(x, h=0.001) ~ x, color=colors[5],
             label_text="h=0.001", label_x=7)
P1
```

You can see at a glance that $\text{ARC}(x, h)$ gets closer and closer to the claimed derivative, $e^x$, as $h$ gets smaller. 

A better view of things comes from recognizing that $$\text{ARC}(x, h) = e^x \left[\frac{e^h - 1}{h}\right]$$
Let's look at the quantity in brackets as $h$ gets small. If that goes to 1 for small $h$, then $\text{ARC}(x, h)$ goes to $e^x$.

$h$ | $\frac{e^h - 1}{h}$ 
:----|-------------------
1   | `r format(factor(1), digits=15)`
0.1   | `r format(factor(0.1), digits=15)`
0.01   | `r format(factor(0.01), digits=15)`
0.001   | `r format(factor(0.001), digits=15)`
0.0001   | `r format(factor(0.0001), digits=15)`
0.00001   | `r format(factor(0.00001), digits=15)`
0.000001   | `r format(factor(0.000001), digits=15)`
0.0000001   | `r format(factor(0.0000001), digits=15)`
0.00000001   | `r format(factor(0.00000001), digits=15)`
0.000000001   | `r format(factor(0.000000001), digits=15)`
:::

```{exercise, name="finch-trim-kayak"}
```
"Exercises/Diff/finch-trim-kayak.Rmd")`

## Derivatives of linear combinations

## Derivatives of composed functions

Just state the rule

$$\partial_ f(g(x)) = \partial_x f(g(x)) \partial_x g(x)$$
and some examples. Then return to the derivation in an example in the Approximations chapter. 




## Derivatives of the basic modeling function


    
::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Deriv-8c", "Master derivatives of basic modeling functions")
```
:::

Just an application of the principles behind composed functions.

* $\partial_x e^{k x} = k\,e^{k x}$
* $\partial_x x^0 = 0$
* $\partial_x x^p = p\, x^{p-1}$. unless $p=0$. 
* $\partial_x \sin(\frac{2\pi}{P} x) = \frac{2\pi}{P}\cos(\frac{2\pi}{P}x)$
* $\partial_x \cos(\frac{2\pi}{P} x) = \frac{2\pi}{P}\sin(\frac{2\pi}{P}x)$
* $\partial_x \text{sigmoid}(x, \text{center}, \text{width}) = \frac{1}{\text{center}}\text{hump}(x, \text{center}, \text{width})$


NEXT STEP. The linear expansion ...

$f(x + h) \approx f(x) + h \partial_x f(x)$










YOU GOT HERE.

## The $\Delta$ operator

CHANGE THIS TO ${\cal D}$ and introduce it once you have gotten rid of $h$.

Up until now, we used only functions that take numerical quantities as inputs and produce a numerical quantity as an output. It's time now to broaden our perspective a bit.

Imagine a function named $\Delta()$ defined like this:
$$\Delta(f) \equiv \frac{f(x+h) - f(x)}{h}$$
$\Delta()$ takes as input a **function**. You might have guessed this because the definition uses the name $f$ for the input to $\Delta()$, but the name of an input does not matter so long as it is used consistently in the body of the function. Looking at the body, you can that the name $f$ is being used (twice!) in the position of a function's name.

For instance:

- $\Delta(\sin) = \frac{\sin(x+h) - \sin(x)}{h}$
- $\Delta(x^2) = \frac{(x+h)^2 - x^2}{h}$
- $\Delta(a + b x) = \frac{a + b(x + h) - \left(a + b x\right)}{h}$

::: {.why data-latex=""}
Why did you use $=$ in the above statements rather than the $\equiv$ ?

$\equiv$ means "is defined as" or "is the name of." When we wrote $$\Delta(f) \equiv \frac{f(x+h) - f(x)}{h}$$ we were giving a name to a function. But writing $\Delta(a + b x)$ means to apply the already defined function $\Delta$ to an input, that input being the straight-line function $a + b x$. Rather than *defining* what is $\Delta(a + b x)$ we are *deducing* it from objects that have already been defined.
:::

What is the output of $\Delta()$? It takes a *function* as an input and returns ... a function as an output.

We might choose to give a name to the output, for example fred() or betty() or, more helpfully, $\text{rate}_{\sin}$ or $\Delta \sin$, but our naming conventions, particularly the use of $x$ indicate that $\frac{\sin(x+h) - \sin(x)}{h}$ is a function.

In calculus, there are a handful of celebrity functions that take a function as input and return a function as output. Notice that the word "function" appeared three times in the previous sentence. To avoid this sort of sleep-inducing repetition, we'll call such functions ***operators***. 

```{exercise, name="SIR-dimensions"}
```
"Exercises/Diff/SIR-dimensions.Rmd")`



## Differentiation


Two central operations in calculus are:

1. Given a function $f(t)$, find the function $\partial_t\,f(t)$ giving the instantaneous rate of change of $f()$. This process of deriving $\partial_t\, (t)$ from $f(t)$ is called ***differentiation***.
2. Given a function $\partial_t\,(t)$, find the $f(t)$ of which $\partial_t\,f(t)$ is the instantaneous rate of change. This process of finding $f()$ given $\partial_t\,f()$ is called ***anti-differentiation***. `r mark(2065)`


Notice that in (1) and (2) above we didn't use the ${\cal D}_t$ notation. It's time to switch away from that. What prompts the change is the nuisance of the constant 0.1 in the definition of the slope function:
$${\cal D}_t f(t) \equiv \frac{f(t+0.1) - f(t)}{0.1}$$

Whereas the slope function ${\cal D}_t f(t)$ gives an approximation to the instantaneous rate of change, $\partial_t f(t)$ refers to the instantaneous rate of change **exactly**. 

We'll come back to the relationship between ${\cal D}_t$ and $\partial_t$ in Chapter \@ref(evanescent-h). It's a subject of particular interest to mathematicians, hence a central part of traditional calculus texts (which are mostly written by mathematicians). For modeling and other applications of calculus, it is something of a side issue. `r mark(2070)`

As an intermediate step on the path between ${\cal D}_t$ and $\partial_t$, let's redefine the slope function to eliminate the 0.1 and replace it with a parameter $h$: $${\cal D}_t f(t) \equiv \frac{f(t+h) - f(x\t)}{h}$$
This way of writing the slope function will enable us to consider how the slope function changes as $h$ gets smaller and smaller.

## Taylor Polynomials

Remarkably, as $x \rightarrow 0$, the error also goes to zero. 

```{r small-sine2, echo=FALSE, fig.cap="The polynomial $g(x) \\equiv x -x^3 / 6$ is remarkably similar to $\\sin(x)$ near $x=0$."}
slice_plot(sin(x) ~ x, domain(x=c(-1, 1)), size=3, alpha=0.25) %>%
  slice_plot(x ~ x, color="blue") %>%
  slice_plot(x - x^3/6 ~ x, color="magenta")
```


Showing the error grows as $x^5$, finishing up growing at $x^3$.
```{r echo=FALSE, eval=FALSE}
Pts <- tibble(
  logx = seq(-5,5, length=100),
  x = exp(logx),
  E = abs(sin(x) - (x - x^3/6))
)
gf_point(log(E) ~ logx, data = Pts)
lm(log(E) ~ logx, data = Pts %>% filter(logx < 1))
```

The left panel in Figure \@ref(fig:sin-error) shows that the error increases rapidly with $x$. The right panel, using log-log scales, shows that the error is a ***power-law*** function $x$, since the graph is a straight-line on log-log axes. 


To illustrate, consider approximating a sinusoid function with a polynomial. Take a moment now to review Section \@ref(polynomial-basics) and how the shape of a polynomial depends on its order. Then do Exercise 26.05 before reading on.

Now that you have done Exercise 26.05, I'm going to guess what you might have come up with.


Figure \@ref(fig:sin-poly) shows a sinusoid together with polynomials of order 1 (straight-line), 3, 5, 7, and 9.

```{r sin-poly, echo=FALSE, fig.cap="Approximating a sinusoid with polynomials of degree 1, 3, 5, 7, and 9."}
colors <- hcl.colors(5)
f1 <- makeFun(x ~ x)
f3 <- makeFun(x - x^3/6 ~ x)
f5 <- makeFun(x - x^3/6 + x^5/60 ~ x)
f7 <- makeFun(f5(x) + x^7 / 5040 ~ x)
f9 <- makeFun(f7(x) - x^9 / 362880 ~ x)
P <- slice_plot(sin(x) ~ x, domain(x = c(-6,6)), size = 3, alpha = 0.15) 
P %>%  slice_plot(f1(x) ~ x, color=colors[1]) %>%
  gf_lims(y = c(-1.5, 1.5))
P %>%  slice_plot(f3(x) ~ x, color=colors[2]) %>%
  gf_lims(y = c(-1.5, 1.5))
P %>%  slice_plot(f5(x) ~ x, color=colors[3]) %>%
  gf_lims(y = c(-1.5, 1.5))
P %>%  slice_plot(f7(x) ~ x, color=colors[4]) %>%
  gf_lims(y = c(-1.5, 1.5))
P %>%  slice_plot(f9(x) ~ x, color=colors[5]) %>%
  gf_lims(y = c(-1.5, 1.5))

```

For modelers, this advantage is a huge liability


```{r echo=FALSE}
poly_through <- function(npts=11, norder=4, extra=FALSE, seed=165, seed2=seed+111) {
  if (norder >= npts) stop("At least n+1 points are needed to define an n-th-order polynomial. ")
  set.seed(seed2)
  Background <- tibble(x = runif(1000, -3, 3), y = 2 - x^2 + 2*runif(length(x), 0.5, 1.5))
  Pts <- Background %>% sample(size=npts)
  Extra <- Background %>% sample(size=as.numeric(extra))
  mod1 <- lm(y ~ poly(x, norder), data = Pts)
  mod2 <- lm(y ~ poly(x, norder), data = bind_rows(Pts, Extra))
  fmod1 <- makeFun(mod1)
  fmod2 <- makeFun(mod2)
  gf_point(y ~ x, data = Background, alpha = 0.3, size=0.5) %>%
    gf_point(y ~ x, data = Pts, size=3, alpha = 1, shape=20) %>%
    gf_point(y ~ x, data = Extra, size=2, alpha=1, color="blue") %>%
    slice_plot(fmod2(x) ~ x, color = "red", npts=500) %>%
    slice_plot(fmod1(x) ~ x, size = 2, alpha = 0.25, npts=500) %>%
    gf_lims(y=c(-5, 5))
}
poly_through()
```

Taylor polynomial through piecewise functions.



# Approximation near a reference input {#taylor-polynomial}

Back in Chapter \@ref(local-approximations) we considered ***eight
simple shapes*** for functions of one input:


All these simple shapes can be generated with the same function formula
and appropriate values for parameters $a$, $b$, and $c$.

$$g(x) \equiv a_0 + a_1 x + a_2 x^2$$ This chapter examines the
possibilities for extending the formula a bit, to include higher-order
terms, e.g.
$$h(x) \equiv a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4 + \cdots$$

We'll consider two possible applications:

1.  Creating an arithmetically simple approximation to a function whose
    formula is already known. Such approximations are known as ***Taylor
    polynomials***.
2.  Creating a function to capture the patterns in data, as in Chapter
    \@ref(local-approximations). It turns out that this is a dubious
    practice. We discuss the reasons why so that you can know to avoid
    using high-order polynomials to fit data. `r mark(2915)`

## The reference point

Since this is all about ***approximations***, we need to have a way to
specify the neighborhood of the function domain in which the
approximation is intended to be good enough for use. We can use the same
approach that turned the pattern-book functions (e.g., $x$, $x^2$, ...)
into the basic modeling functions: replacing $x$ in the polynomial with
$\line(x)$. But unlike the basic modeling functions, where the useful
form of $\line()$ was usually $ax + b$, here, we'll use just a ***shift
form*** of line, where the slope is 1: `r mark(2920)`

$$\text{shift}(x) \equiv \left[\strut x - x_0\right]$$ The parameter
$x_0$ is called the ***reference point***. For a power-law function,
$$\left[\strut\text{shift}(x)\right]^n =  \left[\strut x - x_0\right]^n$$
the output is always zero when $x=x_0$, which will be a matter of
considerable importance as we go on. Also, note that we're using square
braces $\left[\ \ \right]$ simply to make it completely unambiguous what
is being exponentiated. `r mark(2925)`


------------------------------------------------------------------------

With the reference point $x_0$ we will re-write the approximating
polynomial as
$$h(x) \equiv a_0 + a_1 [x-x_0] + a_2 [x - x_0]^2 + a_3 [x - x_0]^3 + \cdots$$
This format is convenient because in finding the $a_0$, $a_1$, $\ldots$
for approximating a function $f(x)$ in the neighborhood of $x_0$, we
have a way to calculate quickly the value of $a_0$. Note that at
$x=x_0$, all the terms in the polynomial go to zero except the first, so
we know $a_0 = f(x_0)$. `r mark(2930)`

Now consider the derivative of the approximating polynomial. This is
$$\partial_x h(x) = a_1 + 2 \times a_2 [x-x_0] + 3 \times a_3 [x-x_0] + \cdots$$
Again, at $x=x_0$ all the terms except the first go to zero. So if
$h(x)$ is an approximation to $f(x)$ we'll have
$a_1 = \partial_x f(x_0)$.

We can do this as many times as we want. Here's the second derivative
$\partial_{xx} h(x)$:
$$\partial_{xx} h(x) = 2 a_2  + 2 \times 3 \times a_3 [x-x_0] + \cdots$$
and the third $$\partial_{xxx} h(x) =  2 \times 3 \times a_3 + \cdots$$

As before, all the terms in $\partial_{xx} h()$ and $\partial_{xxx} h()$
*except the first* go to zero when $x=x_0$. This implies
$$a_2 = \frac{1}{2} \partial_{xx} f(x_0) \ \ \ \text{and}\ \ \ a_3 = \frac{1}{2\times 3} \partial_{xxx}f(x_0)$$
Just following the pattern, we can guess that
$a_4 = \frac{1}{2 \times 3 \times 4} \partial_{xxxx} f(x_0)$ and, in
general for the n^th^ term
$$a_n = \frac{1}{1\times 2 \times 3 \times \cdots \times n} \partial^n f(x_0)$$
We're writing
$${\huge \partial^n} \ \text{to stand for}\ \ \stackrel{\Huge \partial}{\ } \underbrace{xx...x}_\text{n times}$$

The quantity $1\times 2 \times 3 \times \cdots \times n$ is called a
factorial and written
$$\huge n! =  1\times 2 \times 3 \times \cdots \times n$$

In case you're not already familiar with factorials, note the following:
$$1! = 1\\
2! = 2\\
3! = 6\\
4! = 24\\
5! = 120\\
\text{... and so on}
$$

In R, use the `factorial()` function to calculate $n!$ for instance:

```{r}
factorial(5)
factorial(6)
factorial(7)
factorial(10)
factorial(15)
```

## Approximations around $x^\star$

Starting with just the pattern-book functions (e.g. $e^t$), you have a
small but rich set of mathematical operations that enables you to make a
huge variety of functions to suit a big range of modeling needs:
`r mark(2855)`

-   ***input scaling***, which turns the pattern-book functions into the
    more directly useful basic modeling functions.
-   ***linear combinations*** of functions, e.g. $A + B e^{-kt}$
-   ***compositions*** of functions, e.g. $e^{-kt^2}$ which you can
    recognize as the composition of an exponential with a power-law
    function.
-   ***products*** of functions, e.g.,
    $\sin\left(\frac{2\pi}{P}x\right) e^{-kt}$

Now we want to tame this profusion of possibilities and consider a way
to construct stand-ins for any function, using a universal format that
needs a minimum of information and can be used for many purposes ***in
place of*** the original function. It's helpful to have a name for the
stand-ins that reminds us of whom they are stand-ins for. If the
original function is $f(x)$, we'll write the names of the stand-ins with
a tilde, as in $\widetilde{\,f\ }(x)$. `r mark(2860)`

The stand-in functions are intended to be much simpler than the original
but useable as a substitute for the original. The catch is that the
stand-in is warranteed to be a good substitute only ***within a small
neighborhood*** of the domain of the origin. `r mark(2865)`

The information we need to construct the stand-ins is very limited.
First, we need to specify where the warranteed neighborhood is. We'll
tend to use $x_0$ as identifying the center of that neighborhood. We'll
also need $f(x_0)$, the output of the original function when the input
is $x_0$, and $\partial_x f(x_0)$ and $\partial_{xx} f(x_0)$.
`r mark(2870)`

Here are two universal formats that can be used to construct a stand-in
for *any* function near a particular input $x_0$. Since it's useful to
have a name for the stand-in, we'll use a tilde on top of the original
function name: `r mark(2885)`

-   First-order approximation:
    $\widetilde{f_1}(x) \equiv f(x_0) + \partial_x f(x_0) (x-x_0)$
-   Second-order approximation:
    $\widetilde{f_2}(x) \equiv f(x_0) + \partial_x f(x_0) [x-x_0] + \frac{1}{2} \partial_{xx} f(x_0) [x - x_0]^2$

Notice that the first two terms of $\widetilde{f_2}(x)$ are identical to
$\widetilde{f_1}(x)$, so we could write the second-order approximation
as
$$\widetilde{f_2}(x) \equiv \widetilde{f_1}(x) +\frac{1}{2} \partial_{xx} f(x_0) [x-x_0]^2$$

The first-order approximation $\widetilde{f_1}(x)$ is nothing more than
the straight-line function whose graph is tangent to the graph of $f(x)$
at the input $x=x_0$.

The second-order approximation is a quadratic polynomial. Being
quadratic, its graph is the familiar parabola. The graph of
$\widetilde{f_2}(x)$ is the parabola that is tangent to the graph of
$f(x)$.

::: {.workedexample}
Consider the function $g(x)$ whose graph is shown in Figure
\@ref(fig:ds-g).

```{r ds-g, echo=FALSE, warning=FALSE}
g <- rfun( ~ x, seed=973)
slice_plot(g(x) ~ x, domain(x=c(-3, 3))) %>%
  gf_labs(title="g(x) vs x") %>%
  gf_vline(xintercept= ~ -1, color="dodgerblue", alpha=0.1, size=20) %>%
  gf_vline(xintercept= ~ -1, color="dodgerblue") %>%
  gf_text(-5 ~ -1.1, label="x0 = -1", color="dodgerblue", angle=90) %>%
  gf_theme(theme_minimal)
```

We haven't given you a formula for $g(x)$, but you can see that it isn't
any of the basic modeling functions but something more complicated.
We're going to construct a first-order and second-order approximation to
$g(x)$ in a neighborhood $x_0 = -1$ as marked by the blue shaded area.
`r mark(2890)`

Note that $x_0$ is not an argmin of $g(x)$. You can see that the argmin
is a little to the right of $x_0$.

The "facts" about $g(x)$ that are needed to construct the
approximations, beyond the specification of the location of the
neighborhood $x_0$, are the values $g(x_0)$, $\partial_x g(x_0)$, and
$\partial_{xx} g(x_0)$. These are: `r mark(2895)`

```{r}
x0 <- -1
g(x0)
dx_g <- D(g(x) ~ x)
dxx_g <- D(g(x) ~ x + x)
dx_g(x0)
dxx_g(x0)
```

With these facts, we can construct the first- and second-order
approximations:

```{r}
tilde1_g <- makeFun(-23.992 - 2.3493*(x-x0) ~ x)
tilde2_g <- makeFun(tilde1_g(x) + (7.8077/2) * (x-x0)^2 ~ x)
```

Figure \@ref(fig:ds-g2) shows $\widetilde{g_1}(x)$ and
$\widetilde{g_2}(x)$, zooming in around $x_0 = -1$.

```{r ds-g2, echo=FALSE, warning=FALSE, fig.cap="The first-order (green) and second-order (red) approximations to $g(x)$ near $x_0=-1$."}
g <- rfun( ~ x, seed=973)
slice_plot(tilde1_g(x) ~ x, domain(x=c(-2, 0)),
             color="green", size=2, alpha=0.25) %>%
  slice_plot(tilde2_g(x) ~ x, domain(x=c(-2, 0)),
             color="orange3", size=2, alpha=0.25) %>%

slice_plot(g(x) ~ x, domain(x=c(-2, 0))) %>%
  gf_labs(title="g(x) vs x") %>%
  gf_vline(xintercept= ~ -1, color="dodgerblue", alpha=0.1, size=50) %>%
  gf_vline(xintercept= ~ -1, color="dodgerblue") %>%
  gf_text(-20 ~ -1.05, label="x0 = -1", color="dodgerblue", angle=90) %>%
  gf_theme(theme_minimal())
```

You can see that $\widetilde{g_2}(x)$ is a good approximation to $g(x)$.
In particular, the argmin of $\widetilde{g_2}(x)$ is close to the that
of $g(x)$.

In a previous example, we showed that the argmin of the parabolic
function $a_0 + a_1 x + a_2 x^2$ is $x^\star = -\frac{a_1}{2 a_2}$.
Using that formula, the argmin of $\widetilde{g_2}(x)$ is
-2.3493/(7.8077/2) = -0.602. `r mark(2900)`
:::

## Taylor polynomials

Putting together everything in the previous sections, we arrive at a
remarkable formula for a polynomial to approximate any smooth,
continuous function $f(x)$ in the neighborhood of a selected input
$x_0$. The overall formula is daunting at first glance, but each of the
terms has the same pattern:
$$f(x) \approx f(x_0) + \frac{\partial_x f(x_0)}{1!} [x - x_0]^1
+ \frac{\partial_{xx} f(x_0)}{2!} [x - x_0]^2
+ \frac{\partial_{xxx} f(x_0)}{3!} [x - x_0]^3
+ \ldots
$$ This is the ***Taylor polynomial***. A Taylor polynomial that
terminates with the $[x-x_0]^2$ term is a ***second-order Taylor
polynomial***, one that terminates with the $[x-x_0]^3$ term is a
***third-order Taylor polynomial***. Mathematicians are particularly
interested in the $n$th-order Taylor polynomial where
$n \rightarrow \infty$. `r mark(2935)`

Construction of a Taylor polynomial involves finding the various orders
of derivatives. There are some cases where this is simple, especially if
a felicitous choice of $x_0$ can be made.

Example: The successive derivatives of $\sin(x)$ are $cos(x)$, then
$-\sin(x)$, then $-\cos(x)$, then back to $\sin(x)$ and onward to any
order derivative you like. If we select $x_0=0$, then each of the
derivatives evaluated at $x_0$ will be zero, $-1$, or $1$. The Taylor
polynomial (to 5th order) of $\sin(x)$ is:
$$\sin(x) \approx 0 + \frac{1}{1!}[x] + \frac{0}{2!} [x]^2 - \frac{1}{3!} [x]^3 + \frac{0}{4!} [x]^4 + \frac{1}{5!} [x]^5 = x - \frac{x^3}{3!} + \frac{x^5}{5!}$$

::: {.why  data-latex=""}
Why say "smooth, continuous function" instead of just function when
talking about the kinds of functions Taylor polynomials can approximate?

Keep in mind that each of the terms in the polynomial has the form
$a_n [x-x_0]^n$ for $n=1,2,3, \ldots$. Each of these is a power-law
function and therefore smooth and continuous. So the polynomial---the
sum of the individual terms---will always be smooth and continous. If
$f()$ is not, no promises can be given about the quality of the
approximation. `r mark(2940)`
:::

## Polynomial computer

```{r echo=FALSE}
source("Exercises/Diff/www/polycomp.R")
```

The day's topic is the translation of a continuous function of one
variable, $f(x)$, whatever form it might be, into a polynomial, that is,
a linear combination of power-law functions (with integer exponents):

We've already seen that any continuous function can be approximated by a
straight-line function near any given point. We have already looked
extensively at using low-order polynomials (up to quadratic terms of
potentially multiple variables) as a modeling tool. Now we're going to
look at whether and when an approximation can be improved by adding
higher-order terms such as $x^3$ and so on. And in order to say whether
an approximation can be improved, we have to have some way to measure
the quality of the approximation.

In 1715, Brooke Taylor (1685-1731) introduced a method to find for any
$n$ the "best" approximating polynomial. This amounts to specifying the
polynomial coefficients $a_0$, $a_1$, $a_2$, and so on. Taylor produced
a formula in terms of the derivatives of the function $f()$:

$$a_n = \frac{f^{(n)}(x_0)}{n!}$$ where $f^{(n)}$ means the "n\^th"
derivative. That is, $$f^{(0)}(x_0) = f(x)\left.\right|_{x_0}\\
f^{(1)}(x_0) = \partial_x f(x) \left.\right|_{x_0}\\
f^{(2)}(x_0) = \partial_{xx} f(x) \left.\right|_{x_0}\\
f^{(3)}(x_0) = \partial_{xxx} f(x) \left.\right|_{x_0}\\
\text{... and so on}
$$

```{r pc1-1, echo=FALSE, results="markup"}
askMC(
  "Consider $f(x) \\equiv e^x$ and take $x_0 = 0$. Use Taylor's formula to find the coefficients from $a_0$ to $a_5$. Which choice below is right?",
  "1, 1, 1, 1, 1, 1" = 
    "Remember the factorials in Taylor's formula.",
  "1, 2, 3, 4, 5, 6",
  "1, 1/2, 1/3, 1/4, 1/5, 1/6" = 
    "Remember that, say, 4! = 4 x 3 x 2",
  "+1, 1, 1/2, 1/6, 1/24, 1/120+",
  random_answer_order = FALSE
  
)
```

For Taylor, "best approximation" means that the all the orders of
derivative of $f(x)$ and those of $s(x)$ match exactly at $x=x_0$. This
can be proved simply by differentiating the polynomial with coefficients
$a_n$ given by Taylor's formula. Or, seen another way, Taylor's formula
was invented with this property in mind. Today, in contrast, people are
much more likely to think of "best" as a least-squares or other
statistical approximation.

Taylor's invention was important largely for reasons that are no longer
relevant. There is a handful of facts based on Taylor's formula that are
still useful when working algebraically with sinusoids, exponentials,
and logs. Also still important is the framework for measuring the
quality of the approximation, which is still important when comparing,
say, Euler and other algorithms for the numerical integration of
differential equations.

To understand why Taylor's invention was genuinely important in the 18th
and 19th centuries, it helps to compare the technology for computing of
Taylor's day to today's computing technology. Today, of course, an
ordinary computer can almost instantaneously perform arithmetic to 15
digits precision. From these basic operations, software has been
constructed to compute the values of many functions to a similar level
of precision: `exp(0)`, `sin()` and `cos()`, `log()`, and so on. The
algorithms of these functions today are different from Taylor's, but for
the people developing those algorithms it was important to be able to
compare their new methods to Taylor's method.

In order to facilitate the comparison of Taylor's method with that of
modern computing, it helps to think about Taylor's invention as a
computer, which I'll call the "polynomial computer," but which is also
called "Taylor polynomials." (There is also something called "Taylor
series," which are closely related but mainly of interest in pure
mathematics rather than applied math, modeling, and computing.)

There were no electronic chips implementing the polynomial computer, but
even in Taylor's day you could hire a computer: a skilled person who
could perform the arithmetic calculations of addition, subtraction,
multiplication and division. Think of this as the "hardware" of a
polynomial computer.


$$f(x) \overset{?}{=}  s(x) \equiv a_0 + a_1 (x-x_0) + a_2 (x-x_0)^2 + \cdots + a_n (x-x_0)^n + \cdots $$

There's also software for the polynomial computer. Each program for the
polynomial computer consisted simply of an ordered set of numbers:
$a_0$, $a_1$, $a_2$, $\cdots$, and $a_n$ as well as the value $x_0$.

We can write a simulator of the polynomial computer in R. To program the
simulated computer, pick the value of $x_0$ and the coefficients $a_0$
to $a_n$. Once programmed, the computer is simply a function: You give
it $x$ and it gives you $f(x)$. Let's try it for a very simple
polynomial: $f(x) \equiv 1 + x + x^2$. The first step is to use
`poly_comp()` to define $f()$. Then we can apply $f()$ to any $x$ we
wish.

Note that in $f(x) \equiv 1 - 2*x + x^2$ nowhere does $x_0$ appear. In
other words, $x_0 = 0$. The coefficients are $a_0=1$, $a_1=-2$, and
$a_3 = 1$.

```{r pc1-2, exercise=TRUE, exercise.cap="Demo of poly_comp()",exercise.nlines=6, eval=FALSE}
f <- poly_comp(0, 1, -2, 1) #these are the coefficients for the previous function, you should change them to answer the MC questions
slice_plot(f(x) ~ x, domain(x=c(-1,1)))
```

Using the sandbox above, re-program the polynomial computer with
$x_0 = 0$ and $a_0$ through $a_4$ set to the coefficients you found
earlier that match $e^x$ up to the $a_4 x^4$ term.

Another mathematical question is when and whether the question mark in
$\overset{?}{=}$ can be removed and equality established between $f(x)$
and a corresponding polynomial.

We tend to think of computing as a modern activity. The first electronic
computers were built in the 1940s for decoding and solving ballistics
problems; by 1960s computers were available to mid-sized businesses for
handling accounting, inventory, and payroll; around 1980 micro-computers
with mouse-based interfaces became reasonably affordable to consumers
and the foundations of the internet were in place; about 1990 the World
Wide Web and browsers were starting to emerge; the smart phone appeared
in 2007.

But this modern history is about a certain form of computing:
electronic, stored instruction, Von Neumann architecture computers.
Before that there were mechanical calculators and card tabulators. And
before that ...

This session is about a type of computer that started to emerge around
1700. Since it lacks an official name, we'll call it the "infinity
computer," since it's based on ideas of infinitely long series and
infinitesimally small differences. It's fair to say that the infinity
computer was *discovered* rather than *invented*; it was put together
out of technological components available by 1700 and took form as
mathematicians realized the sorts of problems that could be solved by
it.

A key component of the infinity computer is polynomials. These had been
available for 500 years (with roots going much further back) and much of
the high-school mathematics curriculum is still oriented around them. As
you know, a polynomial is a function built up as a linear combination of
power-law functions:

$$p(x) \equiv a_0 x^0 + a_1 x^1 + a_2 x^2 + a_3 x^3 + \cdots$$
Polynomials are "flexible" and, importantly, a polynomial function can
be evaluated at any $x$ by a series of multiplications and additions,
arithmetic operations that had already been mastered.

Before Newton, polynomials were mostly used to describe shapes and
generally consisted of only the first few terms: linear, quadratics, and
cubics were standard forms. It was only with the advent of the infinity
computer that much thought was given to the possibilities of the
$\cdots$ terms.

The other key component of the infinity computer is the idea of a
derivative function, introduced in the late 1600s. Already by 1700 the
basic apparatus of calculating derivatives was available, e.g. the chain
and product rules, symbolic forms derivatives of some basic modeling
functions such as power laws and sinusoids.

The initiating idea of the infinity computer was sequences of
derivatives evaluated at a single value of $x$. (We'll use $x=0$ but any
point could be used.)

To illustrate, the table shows the first few derivatives of a few of our
basic modeling functions, evaluated at $x=0$

+--------+---------+-----------+---------+-----------+---------------+
| $f()$  | $\      | $         | $\part  | $\p       | $\cdots$      |
|        | partial | \partial_ | ial_{xx | artial_{x |               |
|        | _x f()$ | {xx} f()$ | x} f()$ | xxx} f()$ |               |
+========+=========+===========+=========+===========+===============+
| $\si   | $\cos(x | $-        | $-      | $         | $\cdots$      |
| n(x)\l | )\left. | \sin(x)\l | \cos(x) | \sin(x)\l |               |
| eft.\r | \right| | eft.\righ | \left.\ | eft.\righ |               |
| ight|_ | _0 = 1$ | t|_0 = 0$ | right|_ | t|_0 = 0$ |               |
| 0 = 0$ |         |           | 0 = -1$ |           |               |
+--------+---------+-----------+---------+-----------+---------------+
| $e^x\l | $e^     | $e^x\l    | $e^     | $e^x\l    | $\cdots$      |
| eft.\r | x\left. | eft.\righ | x\left. | eft.\righ |               |
| ight|_ | \right| | t|_0 = 1$ | \right| | t|_0 = 1$ |               |
| 0 = 1$ | _0 = 1$ |           | _0 = 1$ |           |               |
+--------+---------+-----------+---------+-----------+---------------+
| $x^3\l | $3 x^2  | $3        | $3\cd   | $\        | $\cdots$      |
| eft.\r |  \left. | \cdot 2\c | ot 2 \c |  \ \ \ 0$ |               |
| ight|_ | \right| | dot x^1\l | dot 1 \ |           |               |
| 0 = 0$ | _0 = 0$ | eft.\righ | cdot x^ |           |               |
|        |         | t|_0 = 0$ | 0\left. |           |               |
|        |         |           | \right| |           |               |
|        |         |           | _0 = 6$ |           |               |
+--------+---------+-----------+---------+-----------+---------------+

Compare this to the first few derivatives of the polynomial $p(x)$
evaluated at $x=0$:

-   $p(x = 0)\ \ \ \ \ \ \ \ =\ \ \ \  a_0$
-   $\partial_{x} p(x=0)\ \ \ \ \ \ =\ \ \ \  a_1$
-   $\partial_{xx} p(x=0)\ \ \ \ = \ \ \ 2\cdot a_2$
-   $\partial_{xxx} p(x=0) \ \ = \ \ 3\cdot 2 \cdot a_3\ \  = \ \ 3!\, a_3$
-   $\partial_{xxxx} p(x=0) = \ 4\cdot 3 \cdot 2 \cdot 1 \cdot a_4 \ \ = \ \ 4!\, a_4$
-   $\cdots$

Here's a tantalizing possibility! Suppose we custom design a polynomial
by picking the coefficients $a_0, a_1, a_2, \ldots$ in order to match
the derivatives of the function $f(x)$. For instance, the polynomial
designed to have the same derivatives as $\sin(x)$ (at $x=0$) is:

$$p_{\sin}(x) = 0 + \frac{1}{1!} x + \frac{0}{2!} x + \frac{-1}{3!}x^3! + \frac{0}{4!} x^4 + \ldots = x - x^3/3! + \ldots $$

The polynomial that matches the derivatives of $e^x$ at $x=0$ is even
simpler:

$$p_{\exp}(x) = 1 + \frac{1}{1!}x + \frac{1}{2!} x^2 + \frac{1}{3!} 3^2 + \frac{1}{4!} x^4 + \cdots$$

Natural questions to ask are

$$p_{\sin}(x) \overset{?}{=} \sin(x)  \ \ \ \text{or} \ \ \ \ p_{\exp}(x) \overset{?}{=} e^x$$
Imagine that the answer were yes. (That turns out to be the case.)
Evaluating polynomials is easy: just addition and multiplication. So if
we can write a polynomial $p_f(x)$ that matches any (differentiable)
function $f(x)$, several tasks come within our range. For instance:

1.  Evaluate $f(x)$ for some $x$. Just plug in that $x$ to the
    polynomial, turn the arithmetic crank, and the answer appears.
2.  Integrate $f(x)$. As you remember, integration can be algebraically
    hard or even impossible. But integrating the terms of a polynomial,
    $a_n x^n$ is so easy: the answer is $\frac{a_n}{n+1} x^{n+1}$.
3.  Examine carefully questions like
    $\lim_{x\rightarrow 0} \frac{\sin(x)}{x}$ which involve division by
    zero.

Generations of calculus students have been taught to program the
infinity computer. That is, they have been exercises to construct the
polynomial that matches $f(x)$ and to use that to solve problems (1),
(2), and (3).

EXERCISE: Write expansion for $h(x) \equiv \sqrt{x}$ at $x=1$.

