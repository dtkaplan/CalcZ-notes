# Iteration and accumulation

Let's review the concept of an ***Euler step***.


**Problem statement**:

1. You seek to find a function, e.g. $f(t)$ which you do not yet know.
2. You already know $\partial_t f(t)$. (In the following chapters we'll show you some of the many instances of this unlikely-sounding situation, where you don't know $f(t)$ but you do know $\partial_t f(t)$.) This relationship, $\partial_t f(t) \longrightarrow f(t)$ is, of course, revealed by ***anti-differentiation***.

**Setup**:

3. You know or make up a value for $f(t)$ at a particular time of interest $t=t_0$. "Making up" is a perfectly legitimate practice here to get started in creating an an anti-derivative. The reason is hard to explain until you see the whole process of anti-differentiation by the ***Euler method***, so hang on. In the population modeling problem in Chapter \@ref(change-accumulation), we were interested in forecasting the population into the future, so we chose $t_0$ to be the most recent year for which we have information: $t_0 = 2020$. (In polite mathematical conversation, "making up" is pronounced "assuming.")
4. Consider a quantity $h$ which is to be regarded as a little bit of $t$: they have the same dimension. $h$ should be "small," but "small" compared to what? Often, the modeling problem at hand involves a domain of interest. For instance, the population modeling problem in Chapter \@ref(change-accumulation) involved a forecast over the time interval from year 2020 to 2100. Provisionally, take small to mean 1% or 0.1% of the extent of the domain of interest. (In the population modeling problem, we chose $h=1$ year.)

**The Step**

1. Compute the value of the (still unknown) function $f(t)$ at input $t_0 + h$ as
$$f(t_0+h) = f(t_0) + h \times \partial_t f(t)$$

Now you know a little bit more about $f(t)$, namely, its value at $t_0 + h$.

Since the original problem was to figure out $f(t)$ over some domain of interest (starting at $t=t_0$), we are not finished. We have taken but one step of our journey and have many more to go.

To continue on the journey, we take what we just found out, namely $f(t_0 + h)$. We use this as the starting point in a new step. We don't have to make anything up because we already know $f(t_0 + h)$.

The second step brings us to $f(t_0 + 2h)$

$$f(t_0+2h) = f(t_0+h) + h \times \partial_t f(t+h)$$
And the third:
$$f(t_0+3h) = f(t_0+2h) + h \times \partial_t f(t+2h)$$

How long to continue this step-by-step process? The original problem involved some domain of interest, and we selected $h$ as 1% or 0.1% of the extent of this domain. For $h$ as 1%, taking 100 steps will get us to the far end of the domain. For $h$ as 0.1%, we would need to take 1000 steps. 

```{exercise, name="euler-step"}
Exercises like this:
```
<details>
  
Using the Euler method find $\int f(t) dt$ over the interval $t_0=0$ to $t_{end}=1$. The $t$ quantity is in steps of $h=0.01$.

$t$  | $\partial_t f(t)$ | $\int f(t) dt$
-----|-------------------|-----------------
0    | `r round(dnorm(0)   , 3)`   |    0.5
0.01 | `r round(dnorm(0,01), 3)`   |  
0.02 | `r round(dnorm(0,02), 3)`   |  
0.03 | `r round(dnorm(0.03), 3)`   |  

</details>

## Automating Euler

Although the Euler method was invented in the 18th century, it only became a practicable way of accumulating in the 1930s with the advent of analog computing and then in the 1940s when electronic computing was first becoming available. In those days, a primary mission for computers was accumulating the trajectories of artillery shells, a problem that was inaccessible to symbolic anti-differentiation because of the complexities of air resistence and the varying density of air at different altitudes, with different humidity, and so on. 

Nowadays computing is roughly one-billionth the cost that it was the World War II era, and we use computers for just about every task imaginable. But in those early days, the huge costs involved in developing early computers was justified only by the prospect of improved gunnery in war. And those problems were all about accumulation and, more specifically, Euler methods. 

What today seems an esoteric use for computers was in fact the prime motivation for their development.

### Euler as a spreadsheet

Spreadsheets are popular for certain simple kinds of computer programming. The Euler method is a case in point. Figure \@ref(fig:euler-spreadsheet1) shows the formula layer of a Google Sheet implementing the Euler method for $\int \dnorm(t) dt$. In the spreadsheet, `NORMDIST()` is the equivalent of $\dnorm()$.

```{r euler-spreadsheet1, echo=FALSE, fig.cap="The formula layer of a spreadsheet implementing the Euler method for $\int \dnorm(x) dx$. The whole spreadsheet is viewable [here](https://docs.google.com/spreadsheets/d/1e3SwcNLQxoYCUedALia6XxzIlh5kY_dTohFroewbNR8/edit?usp=sharing)"}
knitr::include_graphics("www/Euler-spreadsheet1.png")
```

The first argument to `NORMDIST()` is a reference to a cell in column B which stores the sequence $t_0$, $t_0 + h$, $t_0 + 2h, \ldots$. (The value of $h$ is stored in cell `H2` (not shown).) Column D contains the results of the Euler calculation. It starts at time $t_0=0$ by referencing cell `H4` (not shown) which contains the starting value $f(t_0)$. Each successive cell in that column refers to the one immediately above that, adding in the appropriate value from the `NORMDIST()` column and multiplying it by $h$ (that is, `H2`). 

Only about 10 Euler steps are shown, out of 100 altogether. Typically in viewing a spreadsheet you don't see the formula layer. Instead, the results of the calculation in each cell are presented. (Figure \@ref(fig:euler-spreadsheet2)). 

```{r euler-spreadsheet2, echo=FALSE, fig.cap="Results from the formula layer shown in Figure \@ref(fig:euler-spreadsheet1)."}
knitr::include_graphics("www/Euler-spreadsheet2.png")
```

Plotting column `D` against column `B` gives a graph of $\int \dnorm(t) dt$ versus $t$. Notice that the output of the Euler method is a function stored as a ***table*** rather than as a function formula. And even though $\int \dnorm(t) dt$ has a pretty simple form, namely, $\pnorm(t)$, the spreadsheet makes it look like something more elaborate.

Spreadsheets are rightly criticized for being verbose. This can make it extremely challenging for the human programmer to create a spreadsheet that does what is claimed for it, let alone to demonstrate that the result is correct. 

Worse, it's hard to generalize a spreadsheet to handle any given function rather than the one (here, `NORMDIST()`) hard-coded into the sheet. A usual practice is to copy a spreadsheet and then customize it to use the function whose anti-derivative is sought. This introduces the potential for human error in a way that can be hard to detect down the line.

### Using `antiD()`

The R/mosaic `antiD()` operator uses symbolic differentiation if it can. Otherwise it uses numerical methods in the spirit of Euler. Here is the same anti-differentiation as in the spreadsheet but done with `antiD()`:

```{r}
f <- antiD(dnorm(t) ~ t)
```

It happens that $\dnorm(t)$ is one of those many functions where the anti-derivative cannot be calculated symbolically. (We already know that $\int \dnorm(t) dt = \pnorm(t)$, but this in fact the *definition* of $\pnorm()$, which is only known from numerical calculations like Euler.)

An advantage of using programming systems like R/mosaic is that they provide a way to do something with the result of the calculation. You could of course plot `f()` with `slice_plot()`, but let's show something much more fundamental ... 

> **How do we know that `antiD()` is giving the result it should give?**

Whenever you do a calculation, you should ask a similar question: "How do I know that [*my calculation*] is giving the result it should?" In general, it takes considerable experience to provide an answer to such a question, but in the case of `antiD()`, the approach is simple: If `f()` is really the anti-derivative of `dnorm()`, then differentiating `f()` should give us something that's the same as `dnorm()`.

Here's the actual calculation:
```{r}
fprime <- D(f(t) ~ t)
deviation <- makeFun(dnorm(t) - fprime(t) ~ t)
slice_plot(deviation(t) ~ t, domain(t=c(-5,5)))
```

With numerical methods, there will almost always be some error introduced by round-off in the calculations, so we should never expect the deviation to be zero. You can see from the plot of the deviation (Look carefully at the vertical axis!) that it is smaller than 0.0000000001, which is a tiny, tiny part of the size of the output from $\dnorm()$.

It might seem odd to use 3 lines of R/mosaic commands to confirm an answer that was calculated in one line. That's a pretty good representation of how much effort a professional will put into **testing** the result compared to **getting** the result.

```{exercise}
Try another strategy for confirming that `antiD()` is giving a good answer. Compare `f()` from the `antiD()` calculation to `pnorm()`.
```


### Using `cumsum()`

### Using `iterate()`


### Using a loop




## The accumulated quantity

::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Int-2e", "Determine the units of an anti-derivative given the base function and the variable of integration.")
```
:::


## Graphical construction

::: {.objectives}
```{r echo=FALSE, results="asis"}
state_objective("Int-5a", "Determine dimensions and units utilizing a graph.")
state_objective("Int-5b", "Graph the anti-derivative of a function given the graph of the base function and a single point on the anti-derivative.")
```
:::

## Step by step with Euler

::: {.objectives}
```{r echo=FALSE, results="markup"}
state_objective("Int-2a", "Utilize Eulerâ€™s Method to approximate anti-derivatives when the value of is small.")
```
:::




